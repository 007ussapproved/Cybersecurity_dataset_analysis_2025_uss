{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab6caea-6640-4b50-ae87-81b9bb124a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "import csv\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load OpenAI API Key\n",
    "load_dotenv(\"api_key.env\")\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "# File paths\n",
    "CUSTOM_DATASETS_FILE = \"custom_created_datasets_results.csv\"\n",
    "EXTRACTED_PAPERS_FILE = \"extracted_test_papers_new_NEWEST.jsonl\"\n",
    "OUTPUT_CSV = \"final_results_custom_created.csv\"\n",
    "OUTPUT_JSONL = \"final_results_custom_created.jsonl\"\n",
    "\n",
    "# Load papers\n",
    "def load_papers_from_jsonl(file_path):\n",
    "    papers = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            try:\n",
    "                papers.append(json.loads(line.strip()))\n",
    "            except json.JSONDecodeError:\n",
    "                continue\n",
    "    return papers\n",
    "\n",
    "# Normalize\n",
    "def normalize_title(title):\n",
    "    return re.sub(r'\\s+', ' ', title.lower().strip())\n",
    "\n",
    "def extract_title(json_str):\n",
    "    try:\n",
    "        data = json.loads(json_str)\n",
    "        return data.get(\"title\", \"\").strip()\n",
    "    except:\n",
    "        return json_str.strip()\n",
    "\n",
    "# Load and normalize\n",
    "papers = load_papers_from_jsonl(EXTRACTED_PAPERS_FILE)\n",
    "custom_df = pd.read_csv(CUSTOM_DATASETS_FILE)\n",
    "custom_df[\"title\"] = custom_df[\"title\"].apply(extract_title)\n",
    "custom_titles = set(custom_df[\"title\"].apply(normalize_title))\n",
    "\n",
    "# Match papers\n",
    "matched_papers = []\n",
    "for paper in papers:\n",
    "    norm_title = normalize_title(paper['title'])\n",
    "    if norm_title in custom_titles:\n",
    "        matched_papers.append(paper)\n",
    "\n",
    "print(f\"Matched {len(matched_papers)} papers with custom datasets.\")\n",
    "\n",
    "# Task list\n",
    "tasks = [\n",
    "    \"dataset_creation_reason\",\n",
    "    \"data_collection_method\",\n",
    "    \"scale_of_data\",\n",
    "    \"data_sources\",\n",
    "    \"data_collection_purpose\"\n",
    "]\n",
    "\n",
    "\n",
    "def generate_all_dataset_details_prompt(paper, dataset_row, task):\n",
    "    title = paper['title']\n",
    "    content = paper['content']\n",
    "    dataset_name = dataset_row['dataset_name']\n",
    "    category = dataset_row['category']\n",
    "    subcategory = dataset_row['subcategory']\n",
    "    availability = dataset_row['availability']\n",
    "    labeling_type = dataset_row['labeling_type']\n",
    "    dataset_type = dataset_row['dataset_type']\n",
    "\n",
    "    # Dataset Creation Reason Extraction\n",
    "    if task == \"dataset_creation_reason\":\n",
    "        return f\"\"\"\n",
    "        You are tasked with identifying the **reason for creating the dataset** in the cybersecurity paper titled \"{title}\".\n",
    "        \n",
    "        Dataset details:\n",
    "        - Dataset Name: {dataset_name}\n",
    "        - Category: {category}\n",
    "        - Subcategory: {subcategory}\n",
    "        - Availability: {availability}\n",
    "        - Labeling Type: {labeling_type}\n",
    "        - Dataset Type: {dataset_type}\n",
    "        \n",
    "        Guidelines:\n",
    "        \n",
    "        Important: If the paper has used both publicly available datasets and also created a new dataset (i.e., a custom-created dataset), you should focus \n",
    "        **only on the custom-created dataset** when answering this task.\n",
    "\n",
    "        \\t1. Identify why the dataset was created in the paper. Look for statements such as:\n",
    "        \\t   - No publicly available dataset\n",
    "        \\t   - Outdated datasets\n",
    "        \\t   - Limited coverage\n",
    "        \\t   - Need for labeled data\n",
    "        \\t   - Domain-specific constraints\n",
    "        \\t   - Need for large-scale data\n",
    "        \\t   - To detect a new type of fraud or attack\n",
    "        \\t   - Need for synthetic data\n",
    "        \\t   - Other reasons not mentioned in the list.\n",
    "        \n",
    "        \\t2. Select the most applicable reason(s) for dataset creation from the options:\n",
    "        \\t   **(A) No publicly available dataset**  \n",
    "        \\t   **(B) Outdated datasets**  \n",
    "        \\t   **(C) Limited coverage**  \n",
    "        \\t   **(D) Need for labeled data**  \n",
    "        \\t   **(E) Domain-specific constraints**  \n",
    "        \\t   **(F) Need for large-scale data**  \n",
    "        \\t   **(G) To detect a new type of fraud or attack\n",
    "        \\t   **(H) Need for synthetic data**  \n",
    "        \\t   **(I) Other reason(s)**  \n",
    "\n",
    "        \\t3. These example are from our dataset, please check clearly and return the same output for these papers. Be vigilant, while processing these papers below: \n",
    "        \n",
    "        - Example 1: In the paper \"POISED: Spotting Twitter Spam Off the Beaten Paths\", the authors created a custom dataset using the Twitter API and manually labeled tweet clusters to support their spam detection system. \n",
    "        The dataset was large-scale (1.3M tweets) and required labeling, but it was not shared publicly.\n",
    "        \"POISED: Spotting Twitter Spam Off the Beaten Paths\" (dataset_creation_reason): {{ \"dataset_creation_reason\": [\"D\", \"F\"]}}\n",
    "\n",
    "        - Example 2: In the paper \"Dissecting Click Fraud Autonomy in the Wild\", they created dataset to detect humanoid attacks, which are novel and not captured in prior datasets.\n",
    "        \"Dissecting Click Fraud Autonomy in the Wild\" (dataset_creation_reason): {{ \"dataset_creation_reason\": [\"F\", \"G\"]}}\n",
    "\n",
    "        -Example 3: In the paper \"Deterrence of Intelligent DDoS via Multi-Hop Traffic Divergence\". They collected a 49.8 TB dataset over 24 hours to evaluate EID's effectiveness at large scale and the dataset supports\n",
    "        detecting intelligent, strategic DDoS attacks that evolve via adversarial machine learning,  a novel threat type the authors aim to deter.\n",
    "        \"Deterrence of Intelligent DDoS via Multi-Hop Traffic Divergence\" (dataset_creation_reason): {{\"dataset_creation_reason\": [\"A\", \"F\", \"G\"]}}\n",
    "\n",
    "        - Example 4: In the paper \"Experimenting with Zero-Knowledge Proofs of Training\", the authors generated a synthetic dataset of 262,144 records with 1,024 features (4GB total) to benchmark the cryptographic cost and performance of their zero-knowledge proof of training (zkPoT) protocol, since it's synthetically created so it will lie in **(H)** Category from above given list.\n",
    "        \"Experimenting with Zero-Knowledge Proofs of Training\" (dataset_creation_reason): {{ \"dataset_creation_reason\": [\"H\", \"F\", \"I\"]}}\n",
    "\n",
    "        - Example 5: In the paper \"Scamdog Millionaire: Detecting E-commerce Scams in the Wild\", the authors created a custom ground-truth dataset of 8,944 e-shop domains, consisting of 3,765 confirmed scam domains and 5,179 benign ones. The dataset was assembled using crowdsourced reviews, manual analyst verification, and telemetry data, addressing the lack of large-scale labeled datasets for scam detection. \n",
    "        \"Scamdog Millionaire: Detecting E-commerce Scams in the Wild\" (dataset_creation_reason): {{ \"dataset_creation_reason\": [\"A\", \"D\", \"F\"] }}\n",
    "\n",
    "        - Example 6: In the paper \"Detecting Weak Keys in Manufacturing Certificates: A Case Study\", the authors custom created dataset, which consist of 226 million manufacturing certificates. It's a large scale dataset so it will lie in **(F)** Category from above given list.\n",
    "        \"Detecting Weak Keys in Manufacturing Certificates\" (dataset_creation_reason): {{ \"dataset_creation_reason\": [\"A\", \"F\", \"I\"] }}\n",
    "\n",
    "        - Example 7: In the paper \"Five Years of the Right to be Forgotten\", the authors created a large-scale dataset of over 3.2 million URL delisting requests submitted to Google between 2014 and 2019. The dataset was built to address the lack of publicly available data,\n",
    "        enable large-scale transparency analysis, and support legal and policy research. The authors also manually labeled the data, categorizing requesters, content types, and decisions.\n",
    "        \"Five Years of the Right to be Forgotten\" (dataset_creation_reason): {{\"dataset_creation_reason\": [\"A\", \"D\", \"E\", \"F\", \"I\"] }}\n",
    "\n",
    "\n",
    "        \\t4. Do NOT hallucinate any reasons. Select a reason ONLY IF there is **clear or strongly implied evidence** in the paper text.\n",
    "\n",
    "        \\t5. If no applicable reason is found from (A)–(H), and the paper still clearly mentions another rationale, return:\n",
    "        \n",
    "        {{\n",
    "            \"dataset_creation_reason\": [\"I\"]\n",
    "        }}\n",
    "\n",
    "        \\t6. If no reason is mentioned or implied at all, return:\n",
    "        \n",
    "        {{\n",
    "            \"dataset_creation_reason\": \"not mentioned\"\n",
    "        }}\n",
    "\n",
    "\n",
    "        Here is the paper content:\n",
    "        <Start of Paper Content>\n",
    "        {content}\n",
    "        <End of Paper Content>\n",
    "\n",
    "        Your response must be returned in the following JSON format:\n",
    "        {{\n",
    "            \"dataset_creation_reason\": [\"A\", \"B\"]\n",
    "        }}\n",
    "\n",
    "        Your response:\n",
    "        \"\"\"\n",
    "\n",
    "    # Task 2: Dataset Collection Method Extraction\n",
    "\n",
    "    elif task == \"data_collection_method\":\n",
    "        return f\"\"\"\n",
    "        You are tasked with identifying the **method used to collect the dataset** in the cybersecurity paper titled \"{title}\".\n",
    "\n",
    "        Dataset details:\n",
    "        - Dataset Name: {dataset_name}\n",
    "        - Category: {category}\n",
    "        - Subcategory: {subcategory}\n",
    "        - Availability: {availability}\n",
    "        - Labeling Type: {labeling_type}\n",
    "        - Dataset Type: {dataset_type}\n",
    "\n",
    "        Guidelines:\n",
    "\n",
    "        Important: If the paper has used both publicly available datasets and also created a new dataset (i.e., a custom-created dataset), you should focus \n",
    "        **only on the custom-created dataset** when answering this task.\n",
    "\n",
    "        \\t1. Identify the method used to collect the custom-created dataset, and select ONLY IF there's clear or strongly implied evidence in the paper. Choose from:\n",
    "        \\t   **DC1:** Web Scraping  \n",
    "        \\t   **DC2:** Network Traffic Logs  \n",
    "        \\t   **DC3:** Malware Repositories  \n",
    "        \\t   **DC4:** User-Contributed Data  \n",
    "        \\t   **DC5:** IoT / Embedded System Logs  \n",
    "        \\t   **DC6:** Manual Labeling  \n",
    "        \\t   **DC7:** Darknet / Underground Markets  \n",
    "        \\t   **DC8:** Government / Institutional Sources  \n",
    "        \\t   **DC9:** Simulated / Synthetic Data  \n",
    "        \\t   **DC10:** Enterprise Logs  \n",
    "        \\t   **DC11:** Mobile / App Data  \n",
    "        \\t   **DC12:** Cloud / Hosting Logs  \n",
    "        \\t   **DC13:** API-based Data Collection\n",
    "        \\t   **DC14:** Other\n",
    "        \\t   **DC15:** Binary Static & Dynamic Analysis Logs\n",
    "\n",
    "         \\t\\t. Clarification:\n",
    "         \n",
    "         - IMPORTANT: If the paper says \"crawled\" or \"crawling\", **check if it's used with an API**. \n",
    "         - If it says “used Twitter API to crawl timelines” or “crawled using Play API,” assign **DC13 (API-based Collection)**.\n",
    "         - Only assign **DC1 (Web Scraping)** if crawling is done directly from websites or app store HTML **without** using APIs.\n",
    "\n",
    "        \\t2. Examples for clarity:\n",
    "\n",
    "        - Example 1: In the paper \"POISED: Spotting Twitter Spam Off the Beaten Paths\", the authors used the Twitter API to crawl user timelines. This is API-based collection.\n",
    "        \"POISED: Spotting Twitter Spam Off the Beaten Paths\" (data_collection_method): {{ \"data_collection_method\": [\"DC13\"] }}\n",
    "\n",
    "        - Example 2: In the paper \"Dissecting Click Fraud Autonomy in the Wild\", the authors crawled 10K apps from Google Play (without using an API). This is web scraping.\n",
    "        \"Dissecting Click Fraud Autonomy in the Wild\" (data_collection_method): {{ \"data_collection_method\": [\"DC1\", \"DC11\"] }}\n",
    "\n",
    "        - Example 3: In \"The Effectiveness of Security Interventions on GitHub\", the dataset was collected through GitHub’s REST API.\n",
    "        \"The Effectiveness of Security Interventions on GitHub\" (data_collection_method): {{\"data_collection_method\": [\"DC13\"]}}\n",
    "\n",
    "        - Example 4: In the paper \"AFLGuard: Byzantine-robust Asynchronous Federated Learning\", the authors constructed a synthetic dataset using a multivariate Gaussian distribution to\n",
    "        simulate federated learning scenarios.\n",
    "        \"AFLGuard: Byzantine-robust Asynchronous Federated Learning\" (data_collection_method): {{\"data_collection_method\": [\"DC9\"]}}\n",
    "\n",
    "        \n",
    "        \\t2. Only choose DC8 or DC9 if the paper explicitly mentions **government/institutional datasets** or **synthetic/simulated generation**.\n",
    "\n",
    "        \\t3. Do not hallucinate a method. Only select from the list if the evidence in the paper clearly supports it. If no valid collection method is identifiable, return:\n",
    "\n",
    "        {{\n",
    "            \"data_collection_method\": [\"DC14\"]\n",
    "        }}\n",
    "\n",
    "\n",
    "        Here is the paper content:\n",
    "        <Start of Paper Content>\n",
    "        {content}\n",
    "        <End of Paper Content>\n",
    "\n",
    "        Your response must be returned in the following JSON format:\n",
    "        {{\n",
    "            \"data_collection_method\": [\"DC1\"]\n",
    "        }}\n",
    "\n",
    "        Your response:\n",
    "        \"\"\"\n",
    "\n",
    "     # Task 3: Dataset Creation Purpose Extraction\n",
    "    \n",
    "    elif task == \"data_collection_purpose\":\n",
    "         return f\"\"\"\n",
    "         \n",
    "         You are tasked with identifying the **purpose for which the dataset was used in the study** in the cybersecurity paper titled \"{title}\".\n",
    "\n",
    "         Guidelines:\n",
    "\n",
    "         Important: If the paper has used both publicly available datasets and also created a new dataset (i.e., a custom-created dataset), you should focus **only on the custom-created dataset** when answering this task.\n",
    "\n",
    "         \\t1. Identify the intended research use or experimental goal for the dataset. Look for statements such as:\n",
    "         \\t   - Needed a real-world dataset for intrusion detection\n",
    "         \\t   - Created to study real-world attack/fraud data\n",
    "         \\t   - Needed a labeled dataset for supervised learning\n",
    "         \\t   - Created to test adversarial security models\n",
    "         \\t   - Needed synthetic data for analysis\n",
    "         \\t   - To study message propagation in social networks\n",
    "         \\t   - To evaluate model robustness under adversarial machine learning attacks\n",
    "         \\t   - Training ML models for binary recovery\n",
    "         \\t   - Created to study real-world vulnerabilities in web applications.\n",
    "         \\t   - Created to study real-world vulnerabilities in Android applications.\n",
    "         \\t   - Created to study large-scale trends, user behaviors, or operational patterns in real-world systems\n",
    "         \\t   - Created to study real-world website fingerprinting attacks\n",
    "         \\t   - Created to study real-world vulnerabilities in manufacturing certificates\n",
    "         \\t   - Created to study real-world vulnerabilities in mobile authentication through shoulder surfing.\n",
    "         \\t   - Created to study zero-knowledge proof of training for logistic regression.\n",
    "         \\t   - Created to study real-world vulnerabilities in online collaboration services.\n",
    "         \\t   - Created to study real-world audio transmission behavior of IoT devices\n",
    "         \\t   - Other purposes specified in the paper\n",
    "         \n",
    "         \\t2. Json Output Example:\n",
    "         \n",
    "         **Note: These examples are taken from our output before reading the content, chech below examples, since we already checked them manually so they are correct.**\n",
    "         \n",
    "         - Example 1: In the paper \"POISED: Spotting Twitter Spam Off the Beaten Paths\", the authors created a custom dataset using the Twitter API and manually labeled tweet clusters to support a spam detection model. The dataset was used to study message propagation\n",
    "         across Twitter communities and evaluate adversarial robustness.\n",
    "         'POISED: Spotting Twitter Spam Off the Beaten Paths' (data_collection_purpose): {{ \"data_collection_purpose\": \"Created to study real-world attack data.\" }}\n",
    "         \n",
    "         - Example 2: In the paper \"Debin: Predicting Debug Information in Stripped Binaries\", the authors collected non-stripped ELF binaries to train machine learning models for binary recovery tasks.\n",
    "         'Debin: Predicting Debug Information in Stripped Binaries' (data_collection_purpose): {{ \"data_collection_purpose\": \"Training ML models for binary recovery.\" }}\n",
    "         \n",
    "         - Example 3: In the paper \"Experimenting with Zero-Knowledge Proofs of Training\", the authors have created a 'Synthetic dataset', which was generated for benchmarking/training purposes to test zero-knowledge proof of training protocol.\n",
    "         'Experimenting with Zero-Knowledge Proofs of Training' (data_collection_purpose): {{ \"data_collection_purpose\": \"Needed synthetic data for analysis\" }}\n",
    "         \n",
    "         \\t3. Do not hallucinate a purpose. Only select a purpose if it is **clearly stated or strongly implied** in the paper.\n",
    "         If no valid purpose is identifiable, return:\n",
    "         {{\n",
    "            \"data_collection_purpose\": \"not mentioned\"\n",
    "         }}\n",
    "         If the purpose does not match any of the listed options, return:\n",
    "        {{\n",
    "            \"data_collection_purpose\": \"Other purposes specified in the paper\"\n",
    "        }}\n",
    "        \n",
    "        Here is the paper content:\n",
    "        <Start of Paper Content>\n",
    "        {content}\n",
    "        <End of Paper Content>\n",
    "        \n",
    "        Your response must be returned in the following JSON format:\n",
    "        {{\n",
    "            \"data_collection_purpose\": \"Created to study real-world attack data.\"\n",
    "        }}\n",
    "\n",
    "        Your response:\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "    # Task 4: Scale of Data Extraction\n",
    "    elif task == \"scale_of_data\":\n",
    "        return f\"\"\"\n",
    "        You are tasked with identifying the **scale of the dataset** in the cybersecurity paper titled \"{title}\".\n",
    "        \n",
    "        Guidelines:\n",
    "        \\t1. Identify the scale of the dataset. Look for numbers, such as:\n",
    "        \\t   - Number of images, logs, or samples collected\n",
    "        \\t   - For example: \"50,000 logs collected\", \"10,000 images captured\"\n",
    "\n",
    "        \\t2. Below are some examples from our dataset, make sure to give same output of below mentioned papers, as they were manually verified. Be very careful\n",
    "\n",
    "        \n",
    "        Json Output Examples:\n",
    "        \n",
    "        - Example 1: In the paper \"Latent Typing Biometrics in Online Collaboration Services\", the authors collected approximately 10 typing instances per user, each containing 1,000–1,500 characters, across a total of 74 participants.\n",
    "\n",
    "        \"Latent Typing Biometrics in Online Collaboration Services\" (scale_of_data): {{ \"scale_of_data\": \"74 users × 10 samples each (1,000–1,500 characters per instance)\" }}\n",
    "\n",
    "        \n",
    "        Here is the paper content:\n",
    "        <Start of Paper Content>\n",
    "        {content}\n",
    "        <End of Paper Content>\n",
    "\n",
    "        Your response must be returned in the following JSON format:\n",
    "        {{\n",
    "            \"scale_of_data\": \"50,000 logs collected\"\n",
    "        }}\n",
    "\n",
    "        Your response:\n",
    "        \"\"\"\n",
    "\n",
    "    # Task 5: Data Sources Extraction\n",
    "    elif task == \"data_sources\":\n",
    "        return f\"\"\"\n",
    "        You are tasked with identifying the **sources from which the data was collected** in the cybersecurity paper titled \"{title}\".\n",
    "        \n",
    "        Guidelines:\n",
    "        \n",
    "        Important: If the paper has used both publicly available datasets and also created a new dataset (i.e., a custom-created dataset), you should focus \n",
    "        **only on the custom-created dataset** when answering this task.\n",
    "        \n",
    "        \\t1. Identify the sources from which the data was collected. Common sources include:\n",
    "        \\t   - Network logs\n",
    "        \\t   - GooglePlay\n",
    "        \\t   - Social media\n",
    "        \\t   - Malware repositories\n",
    "        \\t   - IoT devices\n",
    "        \\t   - University datasets\n",
    "        \\t   - Public datasets\n",
    "        \\t   - Alexa Top Sites\n",
    "        \\t   - Reddit\n",
    "        \\t   - Wikipedia\n",
    "        \\t   - Simulated statistical model\n",
    "        \\t   - Other sources mentioned in the paper\n",
    "        \n",
    "\n",
    "        \\t2. Below are some examples from our dataset, make sure to give same output of below mentioned papers, as they were manually verified. Be very careful\n",
    "        \n",
    "        Json Output Examples: \n",
    "        \n",
    "        **Note: These examples are taken from our output before reading the content, chech below examples, since we already checked them manually so they are correct.**\n",
    "        \n",
    "        - Example 1: In the paper \"Dissecting Click Fraud Autonomy in the Wild\", the data source was Google Play and Huawei AppGallery.\n",
    "\n",
    "        \"Dissecting Click Fraud Autonomy in the Wild\" (data_sources): {{ \"data_sources\": [\"Google Play\", \"Huawei AppGallery\"] }}\n",
    "\n",
    "        - Example 2: In the paper \"AFLGuard: Byzantine-robust Asynchronous Federated Learning\", the synthetic dataset was entirely generated by the authors using simulated statistical models (i.e., multivariate Gaussians with random parameters).\n",
    "\n",
    "        \"FLGuard: Byzantine-robust Asynchronous Federated Learning\" (data_sources): {{ \"data_sources\": [\"Simulated statistical model\"] }}\n",
    "\n",
    "        - Example 3: In the paper \"Debin: Predicting Debug Information in Stripped Binaries\", the dataset was collected from Linux debug symbol packages (e.g., coreutils, dpkg, gcc) available in public Linux distributions.\n",
    "\n",
    "        \"Debin: Predicting Debug Information in Stripped Binaries\" (data_sources): {{ \"data_sources\": [\"Linux debug symbol packages\"] }}\n",
    "\n",
    "\n",
    "\n",
    "         \\t3. Do not hallucinate or guess the data sources. Select a source **only if it is explicitly stated or strongly implied** in the paper.\n",
    "\n",
    "         \\t4. If no valid source is found, return:\n",
    "         \n",
    "         {{\n",
    "            \"data_sources\": \"not mentioned\"\n",
    "         }}\n",
    "\n",
    "         \\t5. If a data source is mentioned but it doesn’t match any of the above categories, return:\n",
    "         {{\n",
    "            \"data_sources\": [\"Other sources mentioned in the paper\"]\n",
    "         }}\n",
    "\n",
    "\n",
    "        Here is the paper content:\n",
    "        <Start of Paper Content>\n",
    "        {content}\n",
    "        <End of Paper Content>\n",
    "\n",
    "        Your response must be returned in the following JSON format:\n",
    "        {{\n",
    "            \"data_sources\": [\"Network logs\", \"University dataset\"]\n",
    "        }}\n",
    "\n",
    "        Your response:\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Invalid task\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e7b205-7899-4350-bd76-7fe5c1b43247",
   "metadata": {},
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c8d562c-e1ef-4f65-84b5-745010cbf9a8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def process_custom_dataset_tasks(papers, custom_df, tasks):\n",
    "    results = []\n",
    "\n",
    "    for i, paper in enumerate(papers):\n",
    "        norm_title = normalize_title(paper['title'])\n",
    "        matched_datasets = custom_df[custom_df['title'].apply(normalize_title) == norm_title]\n",
    "\n",
    "        for _, dataset_row in matched_datasets.iterrows():\n",
    "            result_entry = {\n",
    "                \"title\": paper['title'],\n",
    "                \"dataset_name\": dataset_row['dataset_name']\n",
    "            }\n",
    "\n",
    "            for task in tasks:\n",
    "                prompt = generate_all_dataset_details_prompt(paper, dataset_row, task)\n",
    "                print(f\" Processing {paper['title']} / {dataset_row['dataset_name']} [{task}]\")\n",
    "\n",
    "                try:\n",
    "                    response = client.chat.completions.create(\n",
    "                        model=\"gpt-4o-mini\",\n",
    "                        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                        temperature=0.2,\n",
    "                        max_tokens=1500\n",
    "                    )\n",
    "                    content = response.choices[0].message.content.strip()\n",
    "                    print(\" Response:\", content)\n",
    "                    try:\n",
    "                        parsed_json = json.loads(content)\n",
    "                        result_entry[task] = parsed_json.get(task, content)\n",
    "                    except:\n",
    "                        result_entry[task] = content\n",
    "                except Exception as e:\n",
    "                    print(f\" Error with {paper['title']} [{task}]:\", e)\n",
    "                    result_entry[task] = f\"error: {str(e)}\"\n",
    "\n",
    "            results.append(result_entry)\n",
    "\n",
    "    return results\n",
    "\n",
    "# Save\n",
    "\n",
    "def save_results(results, csv_path, jsonl_path):\n",
    "    with open(csv_path, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=[\"title\", \"dataset_name\"] + tasks)\n",
    "        writer.writeheader()\n",
    "        writer.writerows(results)\n",
    "\n",
    "    with open(jsonl_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for row in results:\n",
    "            f.write(json.dumps(row) + \"\\n\")\n",
    "\n",
    "# Run full pipeline\n",
    "results = process_custom_dataset_tasks(matched_papers[:20], custom_df, tasks)\n",
    "save_results(results, OUTPUT_CSV, OUTPUT_JSONL)\n",
    "\n",
    "print(f\" Saved {len(results)} records to {OUTPUT_CSV} and {OUTPUT_JSONL}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cba3b36-4261-47ac-86bc-4e74b1f892d0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def process_custom_dataset_tasks(papers, custom_df, tasks):\n",
    "    results = []\n",
    "\n",
    "    for i, paper in enumerate(papers):\n",
    "        norm_title = normalize_title(paper['title'])\n",
    "        matched_datasets = custom_df[custom_df['title'].apply(normalize_title) == norm_title]\n",
    "\n",
    "        for _, dataset_row in matched_datasets.iterrows():\n",
    "            result_entry = {\n",
    "                \"title\": paper['title'],\n",
    "                \"dataset_name\": dataset_row['dataset_name']\n",
    "            }\n",
    "\n",
    "            for task in tasks:\n",
    "                prompt = generate_all_dataset_details_prompt(paper, dataset_row, task)\n",
    "                print(f\" Processing {paper['title']} / {dataset_row['dataset_name']} [{task}]\")\n",
    "\n",
    "                try:\n",
    "                    response = client.chat.completions.create(\n",
    "                        model=\"gpt-4o-mini\",\n",
    "                        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                        temperature=0.2,\n",
    "                        max_tokens=1500\n",
    "                    )\n",
    "                    content = response.choices[0].message.content.strip()\n",
    "                    print(\" Response:\", content)\n",
    "                    try:\n",
    "                        parsed_json = json.loads(content)\n",
    "                        result_entry[task] = parsed_json.get(task, content)\n",
    "                    except:\n",
    "                        result_entry[task] = content\n",
    "                except Exception as e:\n",
    "                    print(f\" Error with {paper['title']} [{task}]:\", e)\n",
    "                    result_entry[task] = f\"error: {str(e)}\"\n",
    "\n",
    "            results.append(result_entry)\n",
    "\n",
    "    return results\n",
    "\n",
    "# Save\n",
    "\n",
    "def save_results(results, csv_path, jsonl_path):\n",
    "    with open(csv_path, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=[\"title\", \"dataset_name\"] + tasks)\n",
    "        writer.writeheader()\n",
    "        writer.writerows(results)\n",
    "\n",
    "    with open(jsonl_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for row in results:\n",
    "            f.write(json.dumps(row) + \"\\n\")\n",
    "\n",
    "# Run full pipeline\n",
    "results = process_custom_dataset_tasks(matched_papers, custom_df, tasks)\n",
    "save_results(results, OUTPUT_CSV, OUTPUT_JSONL)\n",
    "\n",
    "print(f\" Saved {len(results)} records to {OUTPUT_CSV} and {OUTPUT_JSONL}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "473638cd-0bdf-46db-81d3-89931ee23220",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "dataset_creation_reason\n",
      "A: 47\n",
      "F: 32\n",
      "D: 26\n",
      "G: 21\n",
      "E: 19\n",
      "C: 12\n",
      "I: 6\n",
      "H: 2\n",
      "Saved counts → raw_counts\\dataset_creation_reason_counts.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rkarim3\\AppData\\Local\\Temp\\ipykernel_29376\\2031396522.py:118: FutureWarning: \n",
      "\n",
      "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `y` variable to `hue` and set `legend=False` for the same effect.\n",
      "\n",
      "  sns.barplot(x=list(counts), y=list(labels), palette=\"Blues_d\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved plot   → raw_counts\\dataset_creation_reason_top_labels.png\n",
      "\n",
      "data_collection_method\n",
      "DC13: 8\n",
      "DC1: 7\n",
      "DC15: 7\n",
      "DC8: 7\n",
      "DC5: 6\n",
      "DC2: 5\n",
      "DC10: 5\n",
      "DC6: 5\n",
      "DC4: 5\n",
      "DC3: 4\n",
      "Saved counts → raw_counts\\data_collection_method_counts.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rkarim3\\AppData\\Local\\Temp\\ipykernel_29376\\2031396522.py:118: FutureWarning: \n",
      "\n",
      "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `y` variable to `hue` and set `legend=False` for the same effect.\n",
      "\n",
      "  sns.barplot(x=list(counts), y=list(labels), palette=\"Blues_d\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved plot   → raw_counts\\data_collection_method_top_labels.png\n",
      "\n",
      "data_collection_purpose\n",
      "Created to study real-world attack data.: 22\n",
      "Created to study real-world audio transmission behavior of IoT devices.: 5\n",
      "Training ML models for binary recovery.: 3\n",
      "Created to study real-world vulnerabilities in web applications: 2\n",
      "Created to study real-world vulnerabilities in e-commerce scams.: 2\n",
      "Created to study real-world vulnerabilities in Android applications.: 2\n",
      "Created to study real-world attack/fraud data: 2\n",
      "Created to study real-world website fingerprinting attacks.: 2\n",
      "Other purposes specified in the paper: 2\n",
      "Created to study real-world vulnerabilities in access control policies.: 2\n",
      "Saved counts → raw_counts\\data_collection_purpose_counts.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rkarim3\\AppData\\Local\\Temp\\ipykernel_29376\\2031396522.py:118: FutureWarning: \n",
      "\n",
      "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `y` variable to `hue` and set `legend=False` for the same effect.\n",
      "\n",
      "  sns.barplot(x=list(counts), y=list(labels), palette=\"Blues_d\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved plot   → raw_counts\\data_collection_purpose_top_labels.png\n",
      "\n",
      "data_sources\n",
      "Network logs: 6\n",
      "Iot Devices: 6\n",
      "Access logs from real-world systems: 4\n",
      "Ransomware binaries: 4\n",
      "Victim telemetry: 4\n",
      "Synthetic victims: 4\n",
      "University dataset: 3\n",
      "not mentioned: 3\n",
      "Google Play: 2\n",
      "Simulated statistical model: 2\n",
      "Saved counts → raw_counts\\data_sources_counts.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rkarim3\\AppData\\Local\\Temp\\ipykernel_29376\\2031396522.py:118: FutureWarning: \n",
      "\n",
      "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `y` variable to `hue` and set `legend=False` for the same effect.\n",
      "\n",
      "  sns.barplot(x=list(counts), y=list(labels), palette=\"Blues_d\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved plot   → raw_counts\\data_sources_top_labels.png\n",
      "\n",
      "Saved combined counts → raw_counts\\all_fields_counts.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "import ast\n",
    "import re\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "\n",
    "IN_FILE = \"final_results_custom_created.csv\"\n",
    "OUT_DIR = Path(\"raw_counts\")\n",
    "OUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Load the CSV\n",
    "df = pd.read_csv(IN_FILE, encoding=\"cp1252\", on_bad_lines=\"skip\")\n",
    "\n",
    "# Fields you want to analyze\n",
    "fields = [\n",
    "    \"dataset_creation_reason\",\n",
    "    \"data_collection_method\",\n",
    "    \"data_collection_purpose\",\n",
    "    \"data_sources\",\n",
    "]\n",
    "\n",
    "codefence_start = re.compile(r\"^\\s*```(?:json)?\\s*$\", re.IGNORECASE)\n",
    "codefence_end = re.compile(r\"^\\s*```\\s*$\")\n",
    "\n",
    "def strip_code_fences(s: str) -> str:\n",
    "    if s is None or (isinstance(s, float) and pd.isna(s)):\n",
    "        return \"\"\n",
    "    lines = str(s).splitlines()\n",
    "    # remove leading ```json or ```\n",
    "    if lines and codefence_start.match(lines[0]):\n",
    "        lines = lines[1:]\n",
    "    # remove trailing ```\n",
    "    if lines and codefence_end.match(lines[-1]):\n",
    "        lines = lines[:-1]\n",
    "    s2 = \"\\n\".join(lines).strip()\n",
    "    s2 = s2.replace('\"\"', '\"')  # collapse doubled quotes from CSV\n",
    "    return s2\n",
    "\n",
    "# Normalize and safely parse messy entries\n",
    "def safe_parse(value, key):\n",
    "    if pd.isna(value):\n",
    "        return []\n",
    "    value = strip_code_fences(value).strip().strip(\"`\").strip('\"').strip(\"'\")\n",
    "\n",
    "    if value == \"\":\n",
    "        return []\n",
    "\n",
    "    try:\n",
    "        parsed = json.loads(value)\n",
    "        if isinstance(parsed, dict):\n",
    "            if key in parsed:\n",
    "                val = parsed.get(key)\n",
    "            else:\n",
    "                # fall back to first value if present\n",
    "                val = next(iter(parsed.values())) if parsed else []\n",
    "            if isinstance(val, list):\n",
    "                return val\n",
    "            if isinstance(val, str):\n",
    "                return [val]\n",
    "            return [val]\n",
    "        elif isinstance(parsed, list):\n",
    "            return parsed\n",
    "        elif isinstance(parsed, str):\n",
    "            return [parsed]\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # Try as Python literal (e.g., ['A', 'B'])\n",
    "    try:\n",
    "        parsed = ast.literal_eval(value)\n",
    "        if isinstance(parsed, list):\n",
    "            return parsed\n",
    "        elif isinstance(parsed, str):\n",
    "            return [parsed]\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # Fallback: comma-separated string\n",
    "    if \",\" in value:\n",
    "        return [v.strip() for v in value.split(\",\") if v.strip()]\n",
    "    return [value]\n",
    "\n",
    "field_counts = {}\n",
    "for field in fields:\n",
    "    counter = Counter()\n",
    "    # If the column is missing, skip gracefully\n",
    "    if field not in df.columns:\n",
    "        field_counts[field] = counter\n",
    "        continue\n",
    "    for val in df[field]:\n",
    "        items = safe_parse(val, field)\n",
    "        for item in items:\n",
    "            norm = str(item).strip()\n",
    "            if norm:\n",
    "                counter[norm] += 1\n",
    "    field_counts[field] = counter\n",
    "\n",
    "# Plot + Save results\n",
    "def plot_and_save_counts(field, counter, top_n=20):\n",
    "    items = counter.most_common(top_n)\n",
    "    if not items:\n",
    "        print(f\"[{field}] No data found.\")\n",
    "        return\n",
    "\n",
    "    labels, counts = zip(*items)\n",
    "    # Save counts to CSV\n",
    "    out_csv = OUT_DIR / f\"{field}_counts.csv\"\n",
    "    pd.DataFrame({\"Label\": labels, \"Frequency\": counts}).to_csv(out_csv, index=False)\n",
    "    print(f\"Saved counts → {out_csv}\")\n",
    "\n",
    "    # Plot to PNG\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(x=list(counts), y=list(labels), palette=\"Blues_d\")\n",
    "    plt.title(f\"Top {top_n} Labels in {field}\", fontsize=14, fontweight=\"bold\")\n",
    "    plt.xlabel(\"Frequency\")\n",
    "    plt.ylabel(\"Label\")\n",
    "    plt.tight_layout()\n",
    "    out_png = OUT_DIR / f\"{field}_top_labels.png\"\n",
    "    plt.savefig(out_png, dpi=300)\n",
    "    plt.close()\n",
    "    print(f\"Saved plot   → {out_png}\")\n",
    "\n",
    "# Process each field\n",
    "for field in fields:\n",
    "    print(f\"\\n{field}\")\n",
    "    top10 = field_counts[field].most_common(10)\n",
    "    for label, count in top10:\n",
    "        print(f\"{label}: {count}\")\n",
    "    plot_and_save_counts(field, field_counts[field], top_n=20)\n",
    "\n",
    "# Also save a combined summary CSV (optional)\n",
    "summary_rows = []\n",
    "for field, counter in field_counts.items():\n",
    "    for label, freq in counter.items():\n",
    "        summary_rows.append({\"field\": field, \"label\": label, \"frequency\": freq})\n",
    "\n",
    "if summary_rows:\n",
    "    combined = pd.DataFrame(summary_rows).sort_values([\"field\", \"frequency\"], ascending=[True, False])\n",
    "    combined_path = OUT_DIR / \"all_fields_counts.csv\"\n",
    "    combined.to_csv(combined_path, index=False)\n",
    "    print(f\"\\nSaved combined counts → {combined_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
