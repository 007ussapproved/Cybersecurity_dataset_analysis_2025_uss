{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00efaafe-7768-45f5-bc55-ad9b99f8514e",
   "metadata": {},
   "source": [
    "import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5fbdcc5-eabc-43f9-8e43-390c9620aaff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import json\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "\n",
    "\n",
    "load_dotenv(\"api_key.env\")\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3917bc8-0bc5-4256-a3d9-a193b02efd56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-existing list of datasets\n",
    "existing_datasets = [\n",
    "    \n",
    "    {\"unique_id\": 1, \"dataset_name\": \"NSL-KDD\", \"contributors\": \"Ghulam Mohi-ud-din\", \n",
    "     \"doi\": \"https://dx.doi.org/10.21227/425a-3e55\", \n",
    "     \"url\": None},\n",
    "    \n",
    "    {\"unique_id\": 2, \"dataset_name\": \"UNSW-NB15\", \"contributors\": \"Moustafa, Nour, and Jill Slay\", \n",
    "     \"doi\": None, \n",
    "     \"url\": \"https://research.unsw.edu.au/projects/unsw-nb15-dataset\"},\n",
    "    \n",
    "    {\"unique_id\": 3, \"dataset_name\": \"CICIDS2017\", \"contributors\": \"Iman Sharafaldin, Arash Habibi Lashkari, and Ali A. Ghorbani\", \n",
    "     \"doi\": None, \n",
    "     \"url\": \"https://www.unb.ca/cic/datasets/ids-2017.html\"},\n",
    "    \n",
    "    {\"unique_id\": 4, \"dataset_name\": \"BoT-IoT dataset\", \"contributors\": \"Nickolaos Koroniotis, Nour Moustafa, Elena Sitnikova, Benjamin Turnbull\",  \n",
    "     \"doi\": None, \n",
    "     \"url\": \"https://research.unsw.edu.au/projects/bot-iot-dataset\"},\n",
    "     {\"unique_id\": 5, \"dataset_name\": \"Drebin\", \"contributors\": \"Daniel Arp, Michael Spreitzenbarth, Malte Hubner , Hugo Gascon, and Konrad Rieck\",  \n",
    "     \"doi\": None, \n",
    "     \"url\": \"https://www.ndss-symposium.org/wp-content/uploads/2017/09/11_3_1.pdf\"}\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa19aabe-1ddf-48a6-bcc5-9bcd3a9729e1",
   "metadata": {},
   "source": [
    "PROMPTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8437706e-a923-4361-82aa-3d0e7cbedf16",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def generate_system_prompt(paper, task, dataset_names):\n",
    "    title = paper['title']\n",
    "    content = paper['content']\n",
    "\n",
    "    # Extract title first and then reuse in other tasks\n",
    "    if task == \"title\":\n",
    "        return f\"\"\"\n",
    "        You are tasked with extracting the title of the provided cybersecurity paper.\n",
    "        \n",
    "        Guidelines and Rules:\n",
    "        \\t1. The title is often at the top of the first page.\n",
    "        \\t2. Extract the title in its entirety.\n",
    "        \n",
    "        Here is the paper content:\n",
    "        <Start of Paper Content>\n",
    "        {content}\n",
    "        <End of Paper Content>\n",
    "        \n",
    "        Your response must be returned in the following JSON format:\n",
    "        {{\n",
    "            \"title\": \"Title of the paper here\"\n",
    "        }}\n",
    "\n",
    "        Your response: \"\"\"\n",
    "\n",
    "    elif task == \"authors_name\":\n",
    "        return f\"\"\"\n",
    "        You are tasked with extracting the authors' names from the provided cybersecurity paper.\n",
    "        \n",
    "        Guidelines and Rules:\n",
    "        \\t1. The authors' names are usually listed directly below the title.\n",
    "        \\t2. Extract all the authors, separated by commas.\n",
    "        \n",
    "        Here is the paper content:\n",
    "        <Start of Paper Content>\n",
    "        {content}\n",
    "        <End of Paper Content>\n",
    "        \n",
    "        Your response must be returned in the following JSON format:\n",
    "        {{\n",
    "            \"authors\": \"Comma-separated list of authors' names here\"\n",
    "        }}\n",
    "\n",
    "        Your response: \"\"\"\n",
    "\n",
    "    elif task == \"conference_name\":\n",
    "        return f\"\"\"\n",
    "        You are tasked with extracting the conference name where the paper was presented.\n",
    "        \n",
    "        Guidelines and Rules:\n",
    "        \\t1. The conference name is usually found at the top or bottom of the first page.\n",
    "        \\t2. Use the short form (USS, NDSS, ACSAC, SP, CCS) if applicable.\n",
    "        \n",
    "        Here is the paper content:\n",
    "        <Start of Paper Content>\n",
    "        {content}\n",
    "        <End of Paper Content>\n",
    "        \n",
    "        Your response must be returned in the following JSON format:\n",
    "        {{\n",
    "            \"conference\": \"Short form of conference name (USS, NDSS, ACSAC, SP, CCS)\"\n",
    "        }}\n",
    "\n",
    "        Your response: \"\"\"\n",
    "\n",
    "    elif task == \"published_year\":\n",
    "        return f\"\"\"\n",
    "        You are tasked with extracting the year of publication from the provided cybersecurity paper.\n",
    "        \n",
    "        Guidelines and Rules:\n",
    "        \\t1. The year of publication is usually found near the conference name or at the bottom of the first page.\n",
    "        \n",
    "        Here is the paper content:\n",
    "        <Start of Paper Content>\n",
    "        {content}\n",
    "        <End of Paper Content>\n",
    "        \n",
    "        Your response must be returned in the following JSON format:\n",
    "        {{\n",
    "            \"year\": \"Year of publication here\"\n",
    "        }}\n",
    "        \n",
    "        Your response: \"\"\"\n",
    "\n",
    "    elif task == \"school_institution\":\n",
    "        return f\"\"\"\n",
    "        You are tasked with extracting the school or institution name(s) associated with the authors of the provided cybersecurity paper.\n",
    "        \n",
    "        Guidelines and Rules:\n",
    "        \\t1. The institution or school is often listed near the authors' names, either directly below or in the footer of the first page.\n",
    "        \\t2. Extract all institutions mentioned, separated by commas if there are multiple.\n",
    "        \n",
    "        Here is the paper content:\n",
    "        <Start of Paper Content>\n",
    "        {content}\n",
    "        <End of Paper Content>\n",
    "        \n",
    "        Your response must be returned in the following JSON format:\n",
    "        {{\n",
    "            \"school\": \"Comma-separated list of schools/institutions\"\n",
    "        }}\n",
    "        \n",
    "        Your response: \"\"\"\n",
    "        \n",
    "\n",
    "    elif task == \"dataset_name\":\n",
    "        datasets = [\n",
    "            {\n",
    "                \"unique_id\": \"null\",  # Set to null if not found in existing datasets\n",
    "                \"dataset_name\": \"Name of the first dataset you find\",\n",
    "                \"contributors\": \"Comma separated string of contributors names for the first dataset you find\",\n",
    "                \"doi\": \"DOI for the first dataset you find. If not available, this should be an empty string.\",\n",
    "                \"url\": \"URL link to the first dataset you find. If not available, this should be an empty string.\",\n",
    "            },\n",
    "            {\n",
    "                \"unique_id\": \"null\",  # Set to null if not found in existing datasets\n",
    "                \"dataset_name\": \"Name of the second dataset you find\",\n",
    "                \"contributors\": \"Comma separated string of contributors names for the second dataset you find\",\n",
    "                \"doi\": \"DOI for the second dataset you find. If not available, this should be an empty string.\",\n",
    "                \"url\": \"URL link to the second dataset you find. If not available, this should be an empty string.\",\n",
    "            }\n",
    "        ]\n",
    "        datasets_json = json.dumps({\"datasets\": datasets}, indent=4)\n",
    "        \n",
    "        return f\"\"\"\n",
    "        You are tasked with identifying and extracting datasets from the cybersecurity paper titled \"{title}\".\n",
    "        \n",
    "        Guidelines and Rules:\n",
    "        **STRICTLY FOLLOW ALL GUIDELINES**\n",
    "        \\t1.**Definition of a Dataset**:\n",
    "        \\t- A dataset is a named collection of data used for experiments, evaluation, training, testing, or comparison.\n",
    "        \\t- Examples: CICIDS2017, UNSW-NB15, CAIDA, Alexa top 1 million, HDFS, etc. In one paper they can use as much dataset they want, \n",
    "        for instance; if they have used 10 dataset so return all 10 dataset in output.\n",
    "        \\t- Custom-created datasets by the authors also count if explicitly mentioned as such.\n",
    "        \\t- Datasets can be mentioned explicitly by name (e.g., \"We use UNSW-NB15\") or implicitly (e.g., \"we use the dataset from [25]\" \n",
    "        if reference [25] clearly points to a dataset).\n",
    "\n",
    "        \\t2. **Be Comprehensive & Systematic:**\n",
    "        \\t- Carefully read the entire paper content (including references and methodology sections).\n",
    "        \\t- Identify every dataset mentioned, not just the first few. If you find 10 datasets, list all 10.\n",
    "        \\t- If the same dataset is mentioned multiple times under slightly different names (e.g., \"HDFS dataset\", \"HDFS logs\"), consider them \n",
    "        as referring to the same dataset.\n",
    "        \n",
    "        \\t3. **Real Example from a Paper**:\n",
    "        Consider the ACM CCS paper \"Recompose Event Sequences vs. Predict Next Events: A Novel Anomaly Detection Approach for Discrete Event Logs\"\n",
    "        as an example:\n",
    "        \\t- Introduction: \"DabLog achieves 97.18% and 80.25% F1 scores in evaluation upon HDFS system logs and UNSW-NB15 traffic logs...\"\n",
    "        \\t- Motivation section: \"Both methods were evaluated upon the same HDFS dataset [38, 39]...\"\n",
    "        \\t- Evaluation section: \"We evaluate DabLog with two datasets: UNSW-NB15 traffic logs [29] and HDFS console logs [39]...\"\n",
    "        \n",
    "        From these mentions, they clearly identify two datasets:\n",
    "        \\t- \"HDFS\"\n",
    "        \\t- \"UNSW-NB15\"\n",
    "        \n",
    "        In such a scenario, both \"HDFS\" and \"UNSW-NB15\" must be returned.\n",
    "        \n",
    "        \\t4. **Consider Reference-Based Mentions**:\n",
    "        \\t- If the paper references a dataset indirectly, for example, \"the same HDFS dataset [38, 39],\" then check references. If these references are known sources \n",
    "        for the HDFS dataset, include it.\n",
    "        \\t- For Example: in the paper \"DoubleX: Statically Detecting Vulnerable Data Flows\" author(s) have clearly mentioned \n",
    "        \"To evaluate DoubleX false negatives, we consider the dataset of vulnerable extensions released by Somé with EmPoWeb. His paper [72] provides a list of extension \n",
    "        IDs and corresponding vulnerabilities. Of the 171 Chrome extensions he reported as vulnerable in 2019, 82 still existed on March 16, 2021.\" So which mean they \n",
    "        used this **Chrome extensions dataset** for DoubleX evaluation.\n",
    "        \\t5. **No Guessing or Inferring**:\n",
    "        \n",
    "        \\t- Do not guess or infer a dataset if it's not explicitly mentioned.\n",
    "        \\t- Attacks, vulnerabilities, software tools, protocols, or platforms are not datasets.\n",
    "        \\t- If after thoroughly reviewing the paper and references you find no dataset mentioned, return 'null':\n",
    "        \n",
    "         {{\n",
    "           \"datasets\": null\n",
    "         }}\n",
    "\n",
    "        \\t6.**Do not confuse datasets with other elements**:\n",
    "        \\t- Vulnerability Codes: These are vulnerability codes, so be vigilant. Examples include \"CVE-2019-14815\", \"CVE-2016-4997\", and \"CVE-2017-9074\". Be vigilant with this information, For Example: In the paper \"Automated Bug Hunting With Data-Driven Symbolic RootCause Analysis\" authors haven't used any dataset, instead CVE (Common Vulnerabilities and Exposures) is used as part of the analysis, particularly focusing on specific vulnerabilities and their contexts. However, CVE is not treated as a \"dataset\" in the conventional sense, as it serves more as a standardized catalog for identifying known security vulnerabilities. So return this paper's output as 'null' .\n",
    "        \\t\\t- Again CVE are not 'datasets', A CVE (Common Vulnerabilities and Exposures) is a publicly disclosed cybersecurity vulnerability or exposure in a software or hardware system. Each CVE is assigned a unique identifier (CVE ID) and is documented in a centralized database to help organizations track, assess, and address security flaws.\n",
    "        \\t- Attacks: These are attack techniques, not datasets. Examples include \"SQL injection\", \"DDoS\", and \"Phishing\".\n",
    "        \\t- Bugs: These represent software flaws or defects. Examples include \"software flaws\" and \"defects\".\n",
    "        \\t- Kernel Modules: These are components of the OS, not datasets. Examples include \"ipv6.ko\" and \"nf_tables.ko\".\n",
    "        \\t- Network Protocols: These are communication protocols. Examples include \"TCP\", \"UDP\", \"IPv4\", and \"IPv6\".\n",
    "        \\t- Software Libraries or Packages: These are tools or resources, not datasets. Examples include \"libc.so\" and \"openssl\".\n",
    "        \\t- Standalone Applications and Benchmark Suites: SPEC CPU2006, NGINX, and PostgreSQL are not software libraries or packages.\n",
    "        \\t\\t- SPEC CPU2006 is a benchmark suite used to evaluate CPU and memory performance across standardized tasks, primarily for research and testing purposes. Fo example in the paper \"VIP: Safeguard Value Invariant Property for Thwarting Critical Memory Corruption Attacks\", no dataset is used , which mean you will return \"null\" output, don't consider **SPEC CPU2006** as a dataset.\n",
    "        \\t\\t- NGINX is a web server application commonly used to handle HTTP requests, serve static content, and balance load across servers.\n",
    "        \\t\\t- PostgreSQL is a standalone database management system (DBMS) that manages data storage, retrieval, and complex querying.\n",
    "        \\t\\t- ObliviSync is a secure file synchronization and backup system based on write-only ORAM techniques. It evaluates performance using realistic file size distributions without relying on traditional datasets.\n",
    "        \\t\\t\\t- Example:The paper \"ObliviSync: Practical Oblivious File Backup and Synchronization\" evaluates a system for secure file synchronization and backup but does not rely on traditional datasets. So technically, they haven't used any dataset, so it should return 'null'.\n",
    "        \\t- Permissioned Distributed Ledger Platform: Corda is a distributed ledger platform developed by R3 for businesses, focusing on privacy, efficiency, and regulatory compliance. Unlike public blockchains, Corda uses a permissioned network, ensuring that only authorized parties can participate and view transactions. Corda achieves privacy through point-to-point communication and a unique notary system that prevents double-spending without broadcasting transactions. Its modular design supports smart contracts and can be tailored to different industries, making it suitable for applications in finance, healthcare, supply chain, and more. \n",
    "        \\t- Artifact: sometimes authors release their own artifact and shared it, don't confuse it with dataset.\n",
    "        \\t- Raspberry Pi: Raspberry Pi is a small, affordable computer, often used for educational purposes, DIY projects, and experiments in computing, robotics, and IoT, not a dataset. For example in the paper \"Indistinguishability Prevents Scheduler Side Channels in Real-Time Systems\" no dataset is used , which mean you will return \"null\" output, don't consider **Raspberry Pi** as a dataset.\n",
    "        \n",
    "        \\t7. For each dataset, identify:\n",
    "        \\t- **Name** of the dataset.\n",
    "        \\t- **Contributors** (authors or creators).\n",
    "        \\t- **DOI** The DOI of the dataset (if available)\n",
    "        \\t- **URL** The URL link of the dataset (if available)\n",
    "        \\t\\t- Look for DOIs and URLs in the reference section, especially for **custom-created but public** datasets. In these cases, the contributors are usually the authors of the paper, and they often explicitly mention sharing links to platforms like GitHub or other repositories. Be sure to check for these, but do not include any random GitHub or other links—only include links where the authors explicitly state that they have shared their datasets. Remain vigilant in confirming this information. \n",
    "        \\t\\t- If no dataset is found, return:\n",
    "         \n",
    "         {{\n",
    "           \"datasets\": null\n",
    "         }}\n",
    "\n",
    "        \n",
    "        - **Null Cases** Examples (**Check these example thoroughly before returning the ouput for same paper(s) mentioned below**):\n",
    "\n",
    "        Example 1: The paper \"Indistinguishability Prevents Scheduler Side Channels in Real-Time Systems\" is not a \n",
    "        dataset-related paper, which mean the haven't used any dataset, so return the output as 'null'. And other tasks like \n",
    "        **dataset_analysis_combined** , **dataset_categories** and **dataset_usage** will be 'null' too.\n",
    "        Json Output:\n",
    "        \n",
    "        {{\n",
    "           \"datasets\": null\n",
    "         }}\n",
    "    \n",
    "        Example 2: In the paper \"ZKCPlus: Optimized Fair-exchange Protocol Supporting Practical and Flexible Data Exchange\", which mean the haven't used any dataset, so return the output as 'null'. And other tasks like **dataset_analysis_combined** , **dataset_categories** and **dataset_usage** will be 'null' too.\n",
    "        \n",
    "        Json Output:\n",
    "          \n",
    "        {{\n",
    "           \"datasets\": null\n",
    "         }}\n",
    "\n",
    "         Example 3: In the paper \"DPGen: Automated Program Synthesis for Differential Privacy\" , which mean the haven't used any dataset, so return the output as 'null'. And other tasks like **dataset_analysis_combined** , **dataset_categories** and **dataset_usage** will be 'null' too.\n",
    "         \n",
    "         Json Output:\n",
    "           \n",
    "         {{\n",
    "           \"datasets\": null\n",
    "         }}\n",
    "         \n",
    "         Example 4: In the paper \"A Security Framework for Distributed Ledgers\" , which mean the haven't used any dataset, so return the output as 'null'. And other tasks like **dataset_analysis_combined** , **dataset_categories** and **dataset_usage** will be 'null' too.\n",
    "         \n",
    "         Json Output:\n",
    "           \n",
    "         {{\n",
    "           \"datasets\": null\n",
    "         }}\n",
    "\n",
    "         \n",
    "         - Other Examples:\n",
    "         \n",
    "         Example 1: In the paper \"WristPrint: Characterizing User Re-identification Risks from Wrist-worn Accelerometry Data\", the author used two **public** datasets like \"mORAL\" and \"WISDM\".\n",
    " {{\n",
    "        \"datasets\": [\n",
    "            {{\n",
    "                \"unique_id\": \"null\",\n",
    "                \"dataset_name\": \"mORAL\",\n",
    "                \"contributors\": \"Sayma Akther, Nazir Saleheen, Shahin Alan Samiei, Vivek Shetty, Emre Ertin, Santosh Kumar\",\n",
    "                \"doi\": \"\",\n",
    "                \"url\": \"\"\n",
    "            }},\n",
    "            {{\n",
    "                \"unique_id\": \"null\",\n",
    "                \"dataset_name\": \"WISDM\",\n",
    "                \"contributors\": \"Gary M Weiss\",\n",
    "                \"doi\": \"\",\n",
    "                \"url\": \"https://www.uci.edu/ml/datasets/wisdm+smartphone+and+smartwatch+activity+and+biometrics+dataset\"\n",
    "            }}\n",
    "        ]\n",
    "    }}\n",
    "\n",
    "           Example 4: In the \"A Lightweight IoT Cryptojacking Detection Mechanism in Heterogeneous Smart Home Networks\"  a **public **dataset like \"Network traffic for machine learning classification\" or \"Benign dataset\" and **Custom-created datasets but public** dataset are used.\n",
    "    {{\n",
    "        \"datasets\": [\n",
    "            {{\n",
    "                \"unique_id\": \"null\",\n",
    "                \"dataset_name\": \"Iot cryptojacking\",\n",
    "                \"contributors\": \"Ege Tekiner, Abbas Acar, A. Selcuk Uluagac,\n",
    "                \"doi\": \"\",\n",
    "                \"url\": \"https://github.com/cslfiu/IoTCryptojacking\"\n",
    "            }},\n",
    "            {{\n",
    "                \"unique_id\": \"null\",\n",
    "                \"dataset_name\": \"Benign Dataset\",\n",
    "                \"contributors\": \"Víctor Labayen Guembe, Eduardo Magaña, Daniel Morató, Mikel Izal\",\n",
    "                \"doi\": \"10.17632/5pmnkshffm.1\",\n",
    "                \"url\": \"https://data.mendeley.com/datasets/5pmnkshffm/1\"\n",
    "            }}\n",
    "        ]\n",
    "    }}\n",
    "        \n",
    "        Here is the list of existing datasets:\n",
    "        <Existing dataset(s) start>\n",
    "        {existing_datasets}\n",
    "        <Existing dataset(s) stop>\n",
    "\n",
    "        Here is the paper:\n",
    "        <Start of Paper Content>\n",
    "        {content}\n",
    "        <End of Paper Content>\n",
    "\n",
    "        Your output must be returned in valid JSON format:\n",
    "\n",
    "        {datasets_json}\n",
    "    \n",
    "        Your response: \"\"\"\n",
    "\n",
    "    \n",
    "    elif task == \"dataset_analysis_combined\":\n",
    "        \n",
    "        return f\"\"\"\n",
    "\n",
    "        You are tasked with identifying the **availability**, **labeling_type**, and **dataset_type** for each dataset extracted in the **dataset_name** task for the cybersecurity paper titled \"{title}\".\n",
    "\n",
    "        ### **Guidelines and Rules:**\n",
    "\n",
    "        \\t1. For each dataset, you will identify the following:\n",
    "        \\t- **availability**: Whether the dataset is 'public', 'proprietary', 'restricted', 'Custom-created datasets, not shared', or 'Custom-created datasets but public'.\n",
    "        \\t\\t- **Public** are freely available for download (e.g., datasets hosted on websites like Kaggle, GitHub, or institutional repositories. \n",
    "        These datasets existed before the research and were not curated specifically by the authors.\n",
    "        - Example: in the paper \"Black-box Adversarial Attacks on Commercial Speech Platforms \n",
    "        with Minimal Information\", they used are publicaly available datasets. The output should look like this:\n",
    "        Json Output:\n",
    "\n",
    "         {{\n",
    "        \"dataset_analysis_combined\": [\n",
    "            {{\n",
    "                \"dataset_name\": \"Common Voice\",\n",
    "                \"availability\": \"public\",\n",
    "                \"labeling_type\": \"labeled\",\n",
    "                \"dataset_type\": \"real-world\"\n",
    "            }},\n",
    "                \"dataset_name\": \"Song\",\n",
    "                \"availability\": \"public\",\n",
    "                \"labeling_type\": \"labeled\",\n",
    "                \"dataset_type\": \"real-world\"\n",
    "            }},\n",
    "                \"dataset_name\": \"LibriSpeech\",\n",
    "                \"availability\": \"public\",\n",
    "                \"labeling_type\": \"labeled\",\n",
    "                \"dataset_type\": \"real-world\"\n",
    "            }},\n",
    "            }},\n",
    "                \"dataset_name\": \"Voxceleb dataset\",\n",
    "                \"availability\": \"public\",\n",
    "                \"labeling_type\": \"labeled\",\n",
    "                \"dataset_type\": \" real-world\"\n",
    "            }}\n",
    "        ]\n",
    "    }} \n",
    "\n",
    "         - Example: in the paper \"AHEAD: Adaptive Hierarchical Decomposition for Range Query under Local Differential Privacy\", they used are usually publicaly available datasets. The output should look like this:\n",
    "\n",
    "         Json Output:\n",
    "\n",
    "         {{\n",
    "        \"dataset_analysis_combined\": [\n",
    "            {{\n",
    "                \"dataset_name\": \"Salaries\",\n",
    "                \"availability\": \"public\",\n",
    "                \"labeling_type\": \"labeled\",\n",
    "                \"dataset_type\": \"real-world\"\n",
    "            }},\n",
    "                \"dataset_name\": \"blackfriday\",\n",
    "                \"availability\": \"public\",\n",
    "                \"labeling_type\": \"labeled\",\n",
    "                \"dataset_type\": \"real-world\"\n",
    "            }},\n",
    "                \"dataset_name\": \"Loan\",\n",
    "                \"availability\": \"public\",\n",
    "                \"labeling_type\": \"labeled\",\n",
    "                \"dataset_type\": \"real-world\"\n",
    "            }},\n",
    "            }},\n",
    "                \"dataset_name\": \"Financial\",\n",
    "                \"availability\": \"public\",\n",
    "                \"labeling_type\": \"labeled\",\n",
    "                \"dataset_type\": \"synthetic\"\n",
    "            }}\n",
    "        ]\n",
    "    }} \n",
    "         \n",
    "        \n",
    "        \\t\\t- **Proprietary** are those owned outright by an organization and are typically not accessible to the public, there is no route given to access them. \n",
    "        For instance, many internal company records, commercial databases, or market research datasets are considered proprietary because the owning entity.\n",
    "        \\t\\t- **Restricted** are accessible only under specific conditions (e.g., requiring permission, collaboration, DUA, or  licensing). \n",
    "        - Example 1: The HCUP dataset available at [https://hcup-us.ahrq.gov/tech_assist/centdist.jsp] is classified as a restricted dataset. \n",
    "        Although the data is derived from real-world healthcare information, access is granted only under specific conditions, such as requiring permission\n",
    "        through a data use agreement, ensuring that the sensitive nature of the data is properly managed.\n",
    "        \\t\\t- **Custom-created datasets, not shared** are generated specifically for the research project and are not shared publicly. For example the custom created dataset in the paper **(Un)informed Consent: Studying GDPR Consent Notices in the Field** is not shared so return it as **Custom-created datasets, not shared**.\n",
    "        \\t\\t- **Custom-created datasets but public** are custom datasets created by the authors but shared publicly.\n",
    "        \\t\\t- **Custom-created datasets, but restricted** are Custom-Restricted datasets created by authors but shared with access restrictions.\n",
    "         -Note: Sometimes, authors who create custom datasets may explicitly mention in their paper or dataset documentation \n",
    "        (URL/DOI or citation details) that, due to the large size of the dataset, they are unable to share it online but can provide access upon request\n",
    "        (don't make random guess). For example, in the paper \"Towards Precise Reporting of Cryptographic Misuses\", the authors mentioned in their GitHub link: \n",
    "        'Our original datasets consist of a data set of **3,489 open-source Android apps obtained from F-Droid**, and a data set of **1,437 firmwares** \n",
    "        collected from 6 vendors. Due to the large size of the two datasets (APK dataset: 49 GB, firmware dataset: 21 GB), it is difficult to share them online. \n",
    "        If you are interested in obtaining the original **F-Droid**dataset and **firmware** dataset, please contact us.'. \n",
    "        \n",
    "        -Json output:\n",
    "\n",
    "        {{\n",
    "        \"dataset_analysis_combined\": [\n",
    "            {{\n",
    "                \"dataset_name\": \"3489 open-source Android apps\",\n",
    "                \"availability\": \"Custom-created datasets, but restricted\",\n",
    "                \"labeling_type\": \"labeled\",\n",
    "                \"dataset_type\": \"realistic\"\n",
    "            }},\n",
    "                \"dataset_name\": \"1,437 firmware dataset\",\n",
    "                \"availability\": \"Custom-created datasets, but restricted\",\n",
    "                \"labeling_type\": not mentioned\",\n",
    "                \"dataset_type\": \"realistic\"\n",
    "            }},\n",
    "        ]\n",
    "    }}\n",
    "        \n",
    "        - Example 1: if authors download data (e.g., APK files or malware samples) from platforms like VirusTotal, then apply filtering, labeling, or feature extraction to create a tailored dataset, the resulting dataset is custom-created. While the original source (e.g. VirusTotal) can be cited, the curated dataset is distinct from the original collection and should be classified as 'Custom-created datasets, not shared' or 'Custom-created datasets but public', depending on whether the authors shared it publicly. Like in the paper **EIGER: Automated IOC Generation for Accurate and Interpretable Endpoint Malware Detection** they have collected 162K Malware Samples from VirusTotal and Benign public sources of free Windows software but didnt shared their dataset so return it as **Custom-created datasets, not shared**. Identify this correctly, the output should look like this:\n",
    "        -Json Output:\n",
    "\n",
    "        {{\n",
    "        \"dataset_analysis_combined\": [\n",
    "            {{\n",
    "                \"dataset_name\": \"162K Malware Samples from VirusTotal\",\n",
    "                \"availability\": \"Custom-created datasets, not shared\",\n",
    "                \"labeling_type\": \"labeled\",\n",
    "                \"dataset_type\": \"realistic\"\n",
    "            }},\n",
    "            {{\n",
    "                \"dataset_name\": \"Benign public sources of free Windows software\",\n",
    "                \"availability\": \"Custom-created datasets, not shared\",\n",
    "                \"labeling_type\": \"labeled\",\n",
    "                \"dataset_type\": \"realistic\"\n",
    "            }},\n",
    "            {{\n",
    "                \"dataset_name\": \"Hybrid Analysis Dataset\",\n",
    "                \"availability\": \"Custom-created datasets, not shared\",\n",
    "                \"labeling_type\": \"labeled\",\n",
    "                \"dataset_type\": \"realistic\"\n",
    "            }}\n",
    "        ]\n",
    "    }}\n",
    "        \n",
    "        - Example 2: in the paper \"C3PO: Large-Scale Study of Covert Monitoring of C&C Servers via Over-Permissioned Protocol Infiltration\" where they collected **200,000 malware samples** over 15 year, identify this dataset as **Custom-created datasets, not shared**, since author(s) didnt mentioned sharing this dataset with the community. Identify it correctly, the output should look like this:\n",
    "        -Json Output:\n",
    "\n",
    "        {{\n",
    "        \"dataset_analysis_combined\": [\n",
    "            {{\n",
    "                \"dataset_name\": \"200k Malware Dataset\",\n",
    "                \"availability\": \"Custom-created datasets, not shared\",\n",
    "                \"labeling_type\": \"labeled\",\n",
    "                \"dataset_type\": \"realistic\"\n",
    "            }}\n",
    "        ]\n",
    "    }}\n",
    "        \n",
    "        - Example 3: in the paper \"Deterrence of Intelligent DDoS via Multi-Hop Traffic Divergence\", the author's collected **49.8 TB real dataset from a department at Tsinghua campus network**, identify this and return it as **Custom-created datasets, not shared**. Identify it correctly, The output should look like this:.\n",
    "        \n",
    "        -Json Output:\n",
    "\n",
    "        {{\n",
    "        \"dataset_analysis_combined\": [\n",
    "            {{\n",
    "                \"dataset_name\": \"Tsinghua Network Traffic Dataset\",\n",
    "                \"availability\": \"Custom-created datasets, not shared\",\n",
    "                \"labeling_type\": \"Not Mentioned\",\n",
    "                \"dataset_type\": \"realistic\n",
    "                \n",
    "                \"\n",
    "            }}\n",
    "        ]\n",
    "    }}\n",
    "        - Example 4: In the paper \"High Fidelity Data Reduction for Big Data Security Dependency Analyses\", the dataset was collected from a real enterprise environment for one month, which makes it a custom-created dataset. However, the authors didn't mention sharing it, so return it as **Custom-created datasets, not shared**, \n",
    "        and the **labeling_type** wasn't mentioned either, so return it as **Not Mentioned**. Identify it correctly, The output should look like this:\n",
    "        \n",
    "        -Json Output:\n",
    "\n",
    "        {{\n",
    "        \"dataset_analysis_combined\": [\n",
    "            {{\n",
    "                \"dataset_name\": \"Enterprise Security Dependency Dataset\",\n",
    "                \"availability\": \"Custom-created datasets, not shared\",\n",
    "                \"labeling_type\": \"Not Mentioned\",\n",
    "                \"dataset_type\": \"real-world\"\n",
    "            }},\n",
    "        ]\n",
    "    }}\n",
    "        \n",
    "        - Example 5: in the paper \"This Sneaky Piggy Went to the Android Ad Market: Misusing Mobile Sensors for Stealthy Data Exfiltration\" the datasets used are collected from **4.5K of the most popular apps**, **Two typing datasets** and **one typing datasets** all are **Custom-created datasets, not shared**..\n",
    "         -Json Output:\n",
    "\n",
    "        {{\n",
    "        \"dataset_analysis_combined\": [\n",
    "            {{\n",
    "                \"dataset_name\": \"4.5K Popular Apps Dataset\",\n",
    "                \"availability\": \"Custom-created datasets, not shared\",\n",
    "                \"labeling_type\": \"labeled\",\n",
    "                \"dataset_type\": \"real-world\"\n",
    "            }},\n",
    "                \"dataset_name\": \"Two Typing Datasets\",\n",
    "                \"availability\": \"Custom-created datasets, not shared\",\n",
    "                \"labeling_type\": \"labeled\",\n",
    "                \"dataset_type\": \"synthetic\"\n",
    "            }},\n",
    "                \"dataset_name\": \"One Typing Dataset,\n",
    "                \"availability\": \"Custom-created datasets, not shared\",\n",
    "                \"labeling_type\": \"labeled\",\n",
    "                \"dataset_type\": \"synthetic\"\n",
    "            }},\n",
    "        ]\n",
    "    }}\n",
    "        -  Example 6: in the paper \"BAPM: Block Attention Profiling Model for Multi-tab Website Fingerprinting Attacks on Tor\" has created and used following datasets, The output should look like this:\n",
    "        - Json Ouput:\n",
    "        {{\n",
    "        \"dataset_analysis_combined\": [\n",
    "            {{\n",
    "                \"dataset_name\": \"Close World Multi-Tab Dataset\",\n",
    "                \"availability\": \"Custom-created datasets, not shared\",\n",
    "                \"labeling_type\": \"labeled\",\n",
    "                \"dataset_type\": \"synthetic\"\n",
    "            }},\n",
    "                \"dataset_name\": \"Open World Multi-Tab Dataset\",\n",
    "                \"availability\": \"Custom-created datasets, not shared\",\n",
    "                \"labeling_type\": \"labeled\",\n",
    "                \"dataset_type\": \"synthetic\"\n",
    "            }},\n",
    "                \"dataset_name\": \"Three-Tab Dataset\",\n",
    "                \"availability\": \"Custom-created datasets, not shared\",\n",
    "                \"labeling_type\": \"labeled\",\n",
    "                \"dataset_type\": \"synthetic\"\n",
    "            }},\n",
    "            }},\n",
    "                \"dataset_name\": \"real world dataset\",\n",
    "                \"availability\": \"Custom-created datasets but public\",\n",
    "                \"labeling_type\": \"labeled\",\n",
    "                \"dataset_type\": \"real-world\"\n",
    "            }}\n",
    "        ]\n",
    "    }}\n",
    "    \n",
    "         - Example 7: in the paper \"PDiff: Semantic-based Patch Presence Testing for Downstream Kernels\" the datasets used are both customer-created but one is **Custom-created datasets, not shared** and another is **Custom-created datasets but public**. The output should look like this:\n",
    "         \n",
    "         -Json Output:\n",
    "\n",
    "        {{\n",
    "        \"dataset_analysis_combined\": [\n",
    "            {{\n",
    "                \"dataset_name\": \"CVE dataset\",\n",
    "                \"availability\": \"Custom-created datasets, not shared\",\n",
    "                \"labeling_type\": \"Not mentioned\",\n",
    "                \"dataset_type\": \"real-world\"\n",
    "            }},\n",
    "                \"dataset_name\": \"Kernel Image dataset,\n",
    "                \"availability\": \"Custom-created datasets but public\",\n",
    "                \"labeling_type\": \"labeled\",\n",
    "                \"dataset_type\": \"real-world\"\n",
    "            }},\n",
    "        ]\n",
    "    }}\n",
    "      \n",
    "        \\t-.**labeling_type**: Determine the labeling status of the dataset.\n",
    "        \\t\\t- **labeled**:  A dataset is considered labeled if the paper or the dataset’s official documentation (accessed via DOI, URL, or citation details) explicitly \n",
    "        states that data points have labels or categories.  \n",
    "        \\t\\t\\t- Example conditions:\n",
    "        - The paper says “We manually labeled the dataset.”\n",
    "        - The dataset’s website or documentation includes label files or describes classes/categories for each data point.\n",
    "        - If labeling is confirmed from external sources (DOI/URL/citation), specify how it was identified, e.g., \"labeled (via citation details)\" \n",
    "        or \"labeled (found via URL)\".\n",
    "        - For image datasets, if classes, annotations, or bounding boxes are mentioned, consider them as labeled.\n",
    "        \\t\\t- **unlabeled**: A dataset is considered unlabeled only if the paper or dataset documentation **explicitly states** that it has no labels or is unlabeled.  \n",
    "        \\t\\t\\t- For example, if the paper says, “The dataset is completely unlabeled,” or “We have no ground-truth labels,” then mark it as **unlabeled**.\n",
    "        \\t\\t\\t- If discovered via an external source (DOI/URL/citation) that explicitly says it’s unlabeled, note that as \"unlabeled (found via URL)\" or similar.\n",
    "        \\t\\t-**hybrid**: A dataset is considered hybrid if it explicitly contains both labeled and unlabeled data.  \n",
    "        \\t\\t\\t- For instance, if the paper says, “The dataset includes 10,000 labeled samples and 100,000 unlabeled samples,” return **hybrid**.\n",
    "        \\t\\t\\t- If the dataset’s documentation (DOI/URL/citation) mentions both labeled and unlabeled subsets, also mark it as **hybrid**.\n",
    "        \\t\\t- **re-labeled**: If the paper explicitly states that they took an existing dataset and re-annotated or re-labeled it for their study, return **re-labeled**.\n",
    "        \\t\\t\\t- For example, if it says, “We took the UNSW-NB15 dataset and re-labeled the events according to our criteria,” return **re-labeled**.\n",
    "        \\t\\t- **not mentioned**: If after thoroughly checking the paper, its references, and any accessible DOI/URL information, you cannot find any mention of labeling \n",
    "        status (no explicit mention of labeling, unlabeled status, hybrid, or re-labeling), return **not mentioned**.  \n",
    "        \\t\\t\\t- Use **not mentioned** if:\n",
    "        - The paper never states anything about labeling.\n",
    "        - The dataset’s official sources (DOI/URL) do not mention labeling.\n",
    "        - No external citation details clarify the labeling status.\n",
    "\n",
    "        -**Consistent Example Using HDFS**:\n",
    "        Suppose the paper mentions the HDFS dataset and references [38,39] for its original introduction:\n",
    "        \\t\\t\\t- The paper itself doesn’t state whether HDFS is labeled or unlabeled.  \n",
    "        \\t\\t\\t- The instructions say you can use citation details (i.e., papers [38,39]) to learn about the dataset’s labeling. \n",
    "        \\t\\t\\t- After checking the referenced papers (assuming you have \"web access\" through the citation details-i.e., you can infer what the source papers are known for):\n",
    "        - If the HDFS dataset source paper (Xu et al., SOSP ’09) mentions that the dataset consists of system logs classified by event types or that it is \n",
    "        commonly known that HDFS data is often annotated with specific event types, you can conclude it is **labeled** (e.g., \"labeled (via citation details)\"\n",
    "        if found in the referenced paper).\n",
    "        - If the source says explicitly it’s unlabeled logs (just raw logs without event types) and you confirm it from citation details, return \n",
    "        **unlabeled (via citation details)**.\n",
    "        - If you find both labeled and unlabeled samples mentioned in the original dataset source, return **hybrid**.\n",
    "        - If the paper or the reference does not clarify labeling at all and the dataset’s official documentation (if available) is not accessible, return **not mentioned**.\n",
    "        - If the paper says “We re-labeled the HDFS dataset to fit our classification scheme,” return **re-labeled**.\n",
    "        \\t- **dataset_type**: Determine the type of dataset.\n",
    "        \\t\\t- **Real-world**: The dataset is directly collected from a real-world system or environment without significant preprocessing. Examples include raw network traffic logs or unaltered user interaction data or a complete packet capture (PCAP) file from a corporate network during a normal workday.\n",
    "        \\t\\t- **realistic**: Data simulating real-world scenarios, but collected in controlled or lab environments to mimic actual conditions. This may involve preprocessing or specific configurations. Example: A network traffic dataset collected from real systems but heavily anonymized or filtered for privacy or anonymized DNS logs or cleaned financial transaction data.\n",
    "        \\t\\t- **synthetic**: The dataset is completely generated using simulations, models, or algorithms without \n",
    "        any direct data from real-world systems. Examples include simulated attack traffic or algorithmically generated \n",
    "        synthetic images, such as the **SYMTCP**.\n",
    "        \\t\\t\\t- Example: Datasets generated through symbolic execution (e.g., in the SYMTCP project) \n",
    "        are considered **synthetic** used in the paper \"SYMTCP: Eluding Stateful Deep Packet Inspection \n",
    "        with Automated Discrepancy Discovery\", The output should look like this:\n",
    "        - Json Output:\n",
    "        {{\n",
    "        \"dataset_analysis_combined\": [\n",
    "            {{\n",
    "                \"dataset_name\": \"SYMTCP dataset\",\n",
    "                \"availability\": \"Custom-created datasets but public\",\n",
    "                \"labeling_type\":\"labeled\",\n",
    "                \"dataset_type\": \"synthetic\"\n",
    "            }},\n",
    "        ]\n",
    "    }}\n",
    "        \\t\\t\\t- In the paper \"Preparing Network Intrusion Detection Deep Learning Models with Minimal Data Using Adversarial Domain Adaptation\", they have used two benchmark datasets; one is **hybrid** and another is **synthetic**.\n",
    "        - Another json Example:\n",
    "\n",
    "        {{\n",
    "        \"dataset_analysis_combined\": [\n",
    "            {{\n",
    "                \"dataset_name\": \"UNSW-NB15\",\n",
    "                \"availability\": \"public\",\n",
    "                \"labeling_type\":\"labeled\",\n",
    "                \"dataset_type\": \"hybrid\"\n",
    "            }},\n",
    "            {{\n",
    "                \"dataset_name\": \"NSL-KDD\",\n",
    "                \"availability\": \"public\",\n",
    "                \"labeling_type\": \"labeled\",\n",
    "                \"dataset_type\": \"synthetic\"\n",
    "            }},\n",
    "        ]\n",
    "    }}\n",
    "            \n",
    "        \n",
    "        \n",
    "        \\t- **hybrid**:  The dataset combines both real-world and synthetic elements. For example, \"UNSW-NB15\" dataset which is considered a \"hybrid dataset\", encompassing both real-world and synthetic elements.\n",
    "        \\t\\t- For example: in the paper \"Filtering DDoS Attacks from Unlabeled Network Traffic Data Using Online Deep Learning\", they have used two datasets \"CICIDS2017\" and \"CAIDA2007\". Be vigilant with \"CICIDS2017\" dataset whenever you found it in any paper make sure to return it's **dataset_type** as **realistic**. The output should look like this:\n",
    "        -Json Output:\n",
    "\n",
    "        {{\n",
    "        \"dataset_analysis_combined\": [\n",
    "            {{\n",
    "                \"dataset_name\": \"CICIDS2017\",\n",
    "                \"availability\": \"public\",\n",
    "                \"labeling_type\":\"labeled\",\n",
    "                \"dataset_type\": \"realistic\"\n",
    "            }},\n",
    "            {{\n",
    "                \"dataset_name\": \"CAIDA UCSD DDoS Attack 2007\",\n",
    "                \"availability\": \"public\",\n",
    "                \"labeling_type\": \"unlabeled\",\n",
    "                \"dataset_type\": \"realistic\"\n",
    "            }},\n",
    "        ]\n",
    "    }}\n",
    "        \n",
    "        \\t2. Ensure that the dataset names match the ones extracted from the **dataset_name** task.\n",
    "        \\t3. If no dataset is found in the **dataset_name** task, leave **dataset_analysis_combined** task 'null'. Be vigilant.\n",
    "        \n",
    "        **Null cases Examples**\n",
    "        Example 1 (using the same example used in **dataset_name** task): In the paper \"ZKCPlus: Optimized Fair-exchange Protocol Supporting Practical and Flexible Data Exchange\", which mean the haven't used any dataset, so return the output as 'null'. And other tasks like **dataset_analysis_combined** , **dataset_categories** and **dataset_usage** will be 'null' too.\n",
    "        \n",
    "        - Json Output:\n",
    "\n",
    "        {{\n",
    "\n",
    "        \"dataset_analysis_combined\": null\n",
    "    }}\n",
    "\n",
    "        Example 2 (using the same example used in **dataset_name** task): The paper \"Indistinguishability Prevents Scheduler Side Channels in Real-Time Systems\" is not a dataset-related paper, which mean the haven't used any dataset, so return the output as 'null'. And other tasks like **dataset_analysis_combined** , **dataset_categories** and **dataset_usage** will be 'null' too.\n",
    "\n",
    "        - Json Output:\n",
    "\n",
    "        {{\n",
    "\n",
    "        \"dataset_analysis_combined\": null\n",
    "    }}\n",
    "    \n",
    "        Here are the datasets extracted earlier:\n",
    "        {dataset_names}\n",
    "\n",
    "        <Start of Paper Content>\n",
    "        {content}\n",
    "        <End of Paper Content>\n",
    "\n",
    "        Your output must be returned only in valid JSON format using the following structure:\n",
    "         \n",
    "         {{\n",
    "        \"dataset_analysis_combined\": [\n",
    "            {{\n",
    "                \"dataset_name\": \"Exact dataset name from the **dataset_name** task\",\n",
    "                \"availability\": \"Extracted availability status (**public**, **proprietary**, **restricted**, **Custom-created datasets, not shared**, **Custom-created datasets but public**, or **Custom-created datasets, but restricted**)\",\n",
    "                \"labeling_type\": \"Extracted labeling type (**labeled**, **unlabeled**, **hybrid**, **Re-labeled**, or **not-mentioned**)\",\n",
    "                \"dataset_type\": \"Extracted dataset type (**real-world**,**realistic**, **synthetic**, **hybrid**)\"\n",
    "            }}\n",
    "        ]\n",
    "    }}\n",
    "\n",
    "    Output Examples:\n",
    "\n",
    "    **CAREFULLY CHECK THESE PAPER BEFORE RETURNING BACK THE OUTPUT**\n",
    "    \n",
    "\n",
    "        - Example 1: For the paper \"Differentially Private Publishing of High-dimensional Data\".\n",
    "        -Json Output:\n",
    "\n",
    "         {{\n",
    "        \"dataset_analysis_combined\": [\n",
    "            {{\n",
    "                \"dataset_name\": \"Netflix\",\n",
    "                \"availability\": \"public\",\n",
    "                \"labeling_type\":\"labeled\",\n",
    "                \"dataset_type\": \"real-world\"\n",
    "            }},\n",
    "            {{\n",
    "                \"dataset_name\": \"Transaction\",\n",
    "                \"availability\": \"public\",\n",
    "                \"labeling_type\":\"labeled\",\n",
    "                \"dataset_type\": \"real-world\"\n",
    "            }},\n",
    "            {{\n",
    "                \"dataset_name\": \"Movielens\",\n",
    "                \"availability\": \"public\",\n",
    "                \"labeling_type\":\"labeled\",\n",
    "                \"dataset_type\": \"real-world\"\n",
    "            }},\n",
    "            {{\n",
    "                \"dataset_name\": \"Document\",\n",
    "                \"availability\": \"public\",\n",
    "                \"labeling_type\":\"unlabeled (found via URL)\",\n",
    "                \"dataset_type\": \"real-world\"\n",
    "            }},\n",
    "            {{\n",
    "                \"dataset_name\": \"AOL\",\n",
    "                \"availability\": \"public\",\n",
    "                \"labeling_type\":\"unlabeled (found via URL)\",\n",
    "                \"dataset_type\": \"real-world\"\n",
    "            }},\n",
    "            {{\n",
    "                \"dataset_name\": \"Kosarak\",\n",
    "                \"availability\": \"public\",\n",
    "                \"labeling_type\": \"unlabeled (found via URL)\",\n",
    "                \"dataset_type\": \"real-world\"\n",
    "            }},\n",
    "          ]\n",
    "        }}\n",
    "        - Example 2: For the paper \"Recompose Event Sequences vs. Predict Next Events: \n",
    "        A Novel Anomaly Detection Approach for Discrete Event Logs\", be aware with **HDFS dataset**.\n",
    "        -Json Output:\n",
    "\n",
    "             {{\n",
    "        \"dataset_analysis_combined\": [\n",
    "            {{\n",
    "                \"dataset_name\": \"UNSW-NB15\",\n",
    "                \"availability\": \"public\",\n",
    "                \"labeling_type\":\"labeled\",\n",
    "                \"dataset_type\": \"hybrid\"\n",
    "            }},\n",
    "            {{\n",
    "                \"dataset_name\": \"HDFS dataset\",\n",
    "                \"availability\": \"public\",\n",
    "                \"labeling_type\": \"labeled (via citation details)\",\n",
    "                \"dataset_type\": \"realistic\"\n",
    "            }},\n",
    "           ]\n",
    "         }}\n",
    "            - Example 3: \"Model Extraction Attacks on Graph Neural Networks: Taxonomy and Realisation\".\n",
    "            - Json Output:\n",
    "\n",
    "             {{\n",
    "        \"dataset_analysis_combined\": [\n",
    "            {{\n",
    "                \"dataset_name\": \"Cora\",\n",
    "                \"availability\": \"public\",\n",
    "                \"labeling_type\":\"labeled\",\n",
    "                \"dataset_type\": \"real-world\"\n",
    "            }},\n",
    "            {{\n",
    "                \"dataset_name\": \"Pubmed\",\n",
    "                \"availability\": \"public\",\n",
    "                \"labeling_type\": \"labeled\",\n",
    "                \"dataset_type\": \"real-world\"\n",
    "            }},\n",
    "            {{\n",
    "                \"dataset_name\": \"Citeseer\",\n",
    "                \"availability\": \"public\",\n",
    "                \"labeling_type\": \"labeled\",\n",
    "                \"dataset_type\": \"real-world\"\n",
    "            }}\n",
    "           ]\n",
    "        }}\n",
    "        - Example 4: For the paper \"Continuous Release of Data Streams under both Centralized and Local Differential Privacy\", the output should look like:\n",
    "        -Json Output:\n",
    "             {{\n",
    "        \"dataset_analysis_combined\": [\n",
    "            {{\n",
    "                \"dataset_name\": \"DNS\",\n",
    "                \"availability\": \"public\",\n",
    "                \"labeling_type\":\"unlabeled\",\n",
    "                \"dataset_type\": \"real-world\"\n",
    "            }},\n",
    "            {{\n",
    "                \"dataset_name\": \"Fare\",\n",
    "                \"availability\": \"restricted\",\n",
    "                \"labeling_type\":\"unlabeled\",\n",
    "                \"dataset_type\": \"real-world\"\n",
    "            }},\n",
    "            {{\n",
    "                \"dataset_name\": \"Kosarak\",\n",
    "                \"availability\": \"public\",\n",
    "                \"labeling_type\":\"unlabeled\",\n",
    "                \"dataset_type\": \"real-world\"\n",
    "            }},\n",
    "            {{\n",
    "                \"dataset_name\": \"POS\",\n",
    "                \"availability\": \"public\",\n",
    "                \"labeling_type\": \"unlabeled\",\n",
    "                \"dataset_type\": \"real-world\"\n",
    "            }},\n",
    "           ]\n",
    "        }}\n",
    "\n",
    "        - Example 5: As an AI assitant, you have web access so if a dataset is cited is from another work, note its title, contributors, and source publication details for other tasks like **labeling_type**, **availability** or **dataset_type** to extract details by web searching from these citations. For Example:\n",
    "        - \"Vassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur. 2015. Librispeech: an ASR corpus based on public domain audiobooks. In Proc. of ICASSP.\"\n",
    "        - Example: in the paper \"MineSweeper: An In-depth Look into Drive-by Cryptocurrency Mining and Its Defense\" the author custom-created only one dataset and the stated \"We ran 50 Docker containers in parallel for one week mid-March 2018 to collect data from Alexa’s Top 1 Million websites (as of February 28, 2018)\". \n",
    "        Also the shared this dataset with the community **https://github.com/vusec/minesweeper**. The output should look like this:\n",
    "        \n",
    "        Json Output:\n",
    "         {{\n",
    "        \"dataset_analysis_combined\": [\n",
    "            {{\n",
    "                \"dataset_name\": \"collected dataset(minesweeper)\",\n",
    "                \"availability\": \"Custom-created datasets but public\",\n",
    "                \"labeling_type\": \"labeled(via URL)\",\n",
    "                \"dataset_type\": \"real-world\"\n",
    "            }},\n",
    "        ]\n",
    "    }}\n",
    "        - Example (c) in the paper \"Secure Multi-party Computation of Differentially Private Heavy Hitters\", two datasets are used, the output should look like this:\n",
    "        Json Output:\n",
    "        \n",
    "        {{\n",
    "        \"dataset_analysis_combined\": [\n",
    "            {{\n",
    "                \"dataset_name\": \"Zipf distribution\",\n",
    "                \"availability\": \"public\",\n",
    "                \"labeling_type\": \"labeled\",\n",
    "                \"dataset_type\": \"synthetic\"\n",
    "        \n",
    "            }},\n",
    "                \"dataset_name\": \"Online retail dataset\",\n",
    "                \"availability\": \"public\",\n",
    "                \"labeling_type\": \"unlabeled\",\n",
    "                \"dataset_type\": \"real-world\"\n",
    "            }}\n",
    "           ]\n",
    "        }} \n",
    "\n",
    "         Your response: \"\"\"\n",
    "\n",
    "    \n",
    "    elif task == \"dataset_categories\":\n",
    "        \n",
    "        return f\"\"\"\n",
    "\n",
    "        You are tasked with identifying the specific categories and subcategories of datasets extracted from the **dataset_name** task used in the cybersecurity paper titled \"{title}\".\n",
    "\n",
    "        Clarification:\n",
    "        The **dataset_categories** refers specifically to **what the dataset consists of** or contains, not how it is used in the research. \n",
    "        Focus on the dataset's inherent characteristics and contents.\n",
    "        Note: These categories are derived from the taxonomy outlined in the USENIX paper \"Cybersecurity Research Datasets: Taxonomy and Empirical Analysis\" by \n",
    "        Zheng et al., which provides a structured framework for categorizing cybersecurity datasets. Additionally, a new category for multimedia \n",
    "        data has been added based on evolving research needs.\n",
    "\n",
    "        Guidelines and Rules:\n",
    "\n",
    "        \\t1. By **dataset_categories**, we mean identifying whether a dataset belongs to the following major categories and their subcategories:\n",
    "\n",
    "        **Major Categories and Subcategories**:\n",
    "\n",
    "        \\t\\t- **Attacker-Related**:\n",
    "        \\t\\t  1. **Attacks**: Datasets containing information about malicious actions performed to harm systems (e.g., CICIDS2017, Kitsune etc).\n",
    "        \\t\\t  2. **Vulnerabilities**: Datasets capturing weaknesses in systems or software that attackers can exploit (e.g., CVE databases or Open Source \n",
    "        Vulnerability Database as a dataset).\n",
    "        \\t\\t  3. **Exploits**: Technical methods or tools used to execute attacks, such as exploit scripts or frameworks.\n",
    "        \\t\\t  4. **Cybercrime Infrastructures**: Datasets capturing illegal operations and tools, such as botnets, marketplaces, or malware delivery.\n",
    "        \\t\\t  5. **Malware**: is a curated collection of data samples that contain malicious software (malware) or artifacts derived from it.\n",
    "        Raw binaries or executables (e.g., .exe, .apk, .elf files), or Network traffic generated by malware (PCAP files, DNS queries, C2 communications) or etc.\n",
    "\n",
    "        \\t\\t- **Defender Artifacts**:\n",
    "        \\t\\t  1. **Alerts**: Logs or outputs from defensive systems like intrusion detection systems or firewalls.\n",
    "        \\t\\t  2. **Configurations**: Information on setup and configurations of defense systems (e.g., SSL certificate settings).\n",
    "\n",
    "        \\t\\t- **User & Organizational Characteristics**:\n",
    "        \\t\\t  1. **User Activities**: Data on the behavior of users or organizations (e.g., social media activity, browsing logs).\n",
    "        \\t\\t  2. **User Attitudes**: Survey data capturing opinions or sentiments on cybersecurity topics.\n",
    "        \\t\\t  3. **User Attributes**: Characteristics of users or organizations (e.g., demographic profiles or organizational metadata).\n",
    "\n",
    "        \\t\\t- **Macro-Level Internet Characteristics**:\n",
    "        \\t\\t  1. **Applications**: Data on Internet services or products (e.g., website rankings, mobile apps).\n",
    "        \\t\\t  2. **Network Traces**: Packet-level traffic data or network activity logs.\n",
    "        \\t\\t  3. **Topology**: Information on the structure of the Internet, such as AS relationships or routing paths.\n",
    "        \\t\\t  4. **Benchmarks**: contain information about measurements of Internet performance, such as upload/download speed or end-to-end \n",
    "        network reliability. For example in the paper \"Tackling bufferbloat in 3G/4G networks\"  Jiang and Wang constructed a dataset that measured \n",
    "        3G/4G network performance in the US and Korea.\n",
    "        \\t\\t  5. **Adverse Events**: Data on disruptions or outages, like failures caused by misconfigurations.\n",
    "\n",
    "        \\t\\t- **Visual and Multimedia Data** (New Category):\n",
    "        \\t\\t  1. **Image Datasets**: Datasets containing static visual data for tasks like classification, recognition, or detection (e.g., CIFAR-10, MNIST).\n",
    "        \\t\\t  2. **Video Datasets**: Datasets containing dynamic visual data for tasks like motion tracking or behavior analysis (e.g., UCF101, Kinetics).\n",
    "        \\t\\t  3. **Audio Datasets**: Datasets containing audio data (e.g., SpeechCommand, LibriSpeech)..\n",
    "        \\t\\t  4. **Multimodal Datasets**:  Datasets combining different types of data (e.g., images and text or audio and visual) for tasks like cross-modal \n",
    "        retrieval (e.g., Voxceleb).\n",
    "        \\t\\t  5. **Synthetic Media Datasets**: Artificially generated, any form of media datasets.\n",
    "        \\t\\t\\t - For Example: **Typing Motion Dataset (Two-Handed & One-Handed Typing)** used in the paper 'This Sneaky Piggy Went to the Android Ad Market:\n",
    "        Misusing Mobile Sensors for Stealthy Data Exfiltration'  which is artificially generated and falls under this sub-category.\n",
    "\n",
    "        \\t\\t- **Others-catchall** (New Category):\n",
    "        \\t\\t 1.**others**: If no **dataset** fall under above given category return **others**, that's are new catch-all\n",
    "        category.\n",
    "    \n",
    "\n",
    "        \\t2. **Examples for Clarity**:\n",
    "        \\t\\t- A dataset like **Netflix Ratings** used in privacy studies should be categorized under **User & Organizational Characteristics** -> **User Activities**.\n",
    "        \\t\\t- A dataset like **CAIDA AS Relationships**, which captures Internet topology data, should be categorized under **Macro-Level Internet Characteristics** -> **Topology**.\n",
    "        \\t\\t- A dataset like **CIFAR-10**, used for image classification, should be categorized under **Visual and Multimedia Data** -> **Image Datasets**.\n",
    "\n",
    "        \\t3. **Do Not Confuse with Domain**:\n",
    "        \\t\\t- **Domain** refers to the high-level research area (e.g., IoT, malware analysis).\n",
    "        \\t\\t- **Dataset Categories** focus exclusively on the dataset's inherent characteristics (e.g., attacks, vulnerabilities, defender artifacts).\n",
    "\n",
    "        \\t5. **Null Cases**:\n",
    "        \\t\\t. If no dataset found in **dataset_name** task, leave **dataset_categories** as null.\n",
    "\n",
    "        Here are the datasets extracted earlier:\n",
    "        \n",
    "        {dataset_names}\n",
    "\n",
    "        ### Output Structure:\n",
    "\n",
    "        The output must strictly follow this JSON structure:\n",
    "              \n",
    "      {{\n",
    "      \n",
    "      \"dataset_categories\": [\n",
    "            {{\n",
    "                \"dataset_name\": \"Kitsune\",\n",
    "                \"category\": \"attacker_related\",\n",
    "                \"subcategory\": \"attacks\",\n",
    "                \"attacker_related_items\":[\n",
    "                    {{\"name\": \"Fuzzing\"}},\n",
    "                    {{\"name\": \"ARP MitM\"}},\n",
    "                    {{\"name\": \"SSDP Flood\"}},\n",
    "                    {{\"name\": \"SYN DoS\"}},\n",
    "                    {{\"name\": \"Mirai Botnet\"}}\n",
    "                    ]\n",
    "                }},\n",
    "                \n",
    "                {{\n",
    "                \"dataset_name\": \"CIFAR-10\",\n",
    "                \"category\": \"visual_and_multimedia_data\",\n",
    "                \"subcategory\": \"image_datasets\",\n",
    "                \"visual_data_items\": [\n",
    "                    {{\"name\": \"Image Classification\"}}\n",
    "                    ]\n",
    "                }}\n",
    "              ]\n",
    "           }}\n",
    "\n",
    "    ### Output Example:\n",
    "    - For the **Kitsune** dataset, which contains nine attacks such as:\n",
    "        1. OS Scan\n",
    "        2. Fuzzing\n",
    "        3. Video Injection\n",
    "        4. ARP MitM\n",
    "        5. Active Wiretap\n",
    "        6. SSDP Flood\n",
    "        7. SYN DoS\n",
    "        8. SSL Renegotiation\n",
    "        9. Mirai Botnet\n",
    "\n",
    "      If the paper only utilizes attacks 2, 4, 6, 7, and 9, the output should list only those attacks.\n",
    "      \n",
    "      <Start of Paper Content>\n",
    "      {content}\n",
    "      <End of Paper Content>\n",
    "      \n",
    "      Your response: \"\"\"\n",
    "\n",
    "        \n",
    "    \n",
    "    \n",
    "    else:\n",
    "        raise ValueError(\"Invalid task\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31716877-8f32-4323-936d-2fca36146cdc",
   "metadata": {},
   "source": [
    "PROCESS PAPER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170c087b-61af-4404-84e8-a6e44349e706",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "client = OpenAI(\n",
    "    # This is the default and can be omitted\n",
    "    api_key=os.environ.get(\"OPENAI_API_KEY\")\n",
    ")\n",
    "\n",
    "# Function to process papers for multiple tasks using the new OpenAI API\n",
    "def process_papers_for_tasks(papers, tasks):\n",
    "    task_results = {}\n",
    "\n",
    "    for i, paper in enumerate(papers): \n",
    "        paper_title = paper['title']\n",
    "        print(f\"Processing paper: {paper_title}\")\n",
    "        \n",
    "        task_results[paper_title] = {}\n",
    "        \n",
    "        # Process each paper individually for each task\n",
    "        for task in tasks:\n",
    "            # user_prompt = f\"Process this cybersecurity paper for {task}: {paper}\\n\"                \n",
    "            user_prompt = generate_system_prompt(paper, task, task_results[paper_title].get('dataset_name', None))\n",
    "            # Call OpenAI API using the updated Completion method\n",
    "            try:\n",
    "                response = client.chat.completions.create(\n",
    "                    model='gpt-4o-mini',  # Adjust model if needed\n",
    "                     messages=[\n",
    "                            {\n",
    "                                \"role\": \"user\",\n",
    "                                \"content\": user_prompt,\n",
    "                            }\n",
    "                        ],\n",
    "                    # prompt=user_prompt,  # Use 'prompt' instead of 'messages'\n",
    "                    temperature=0.2, \n",
    "                    max_tokens=5000  # Adjust max tokens based on your response length needs\n",
    "                )\n",
    "                # print(response)\n",
    "                response_text = response.choices[0].message.content            \n",
    "                print(response_text)\n",
    "                task_results[paper_title][task] = response_text\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {task} for paper {i+1}: {e}\")\n",
    "                task_results[paper_title][task] = \"error\" + str(e)\n",
    "\n",
    "    return task_results\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "864d0714-d819-478b-9045-0240a8c912b8",
   "metadata": {},
   "source": [
    "**Final RUN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b0259d2-377b-46d6-9b20-50b3d9638872",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_papers = papers[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5eafb75-4eb3-4932-bce5-721aa59a1fab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define the tasks you are going to process\n",
    "tasks = [ \n",
    "    \"title\", \n",
    "    \"authors_name\", \n",
    "    \"conference_name\", \n",
    "    \"published_year\", \n",
    "    \"school_institution\", \n",
    "    \"dataset_name\", \n",
    "    \"dataset_analysis_combined\", \n",
    "    \"dataset_categories\"\n",
    "]\n",
    "# Process the selected 100 papers for all tasks\n",
    "all_results = process_papers_for_tasks(test_papers, tasks)\n",
    "\n",
    "# Print the final results for each task\n",
    "# for task, results in all_results.items():\n",
    "#     print(f\"Results for {task}:\")\n",
    "#     for result in results:\n",
    "#         print(result)\n",
    "\n",
    "# print(\"Processing complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce290d4-5a5e-4a9c-ab6a-982636ecea65",
   "metadata": {},
   "source": [
    "CSV OUTPUT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d5f803-e210-40dd-8ef7-11b9e9735444",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "# Define the tasks as headers\n",
    "tasks = [ \n",
    "    \"title\", \n",
    "    \"authors_name\", \n",
    "    \"conference_name\", \n",
    "    \"published_year\", \n",
    "    \"school_institution\", \n",
    "    \"dataset_name\", \n",
    "    \"dataset_analysis_combined\", \n",
    "    \"dataset_categories\"\n",
    "]\n",
    "\n",
    "# File path for the CSV output\n",
    "output_csv_path = 'results_test.csv'\n",
    "\n",
    "# Open a new CSV file\n",
    "with open(output_csv_path, 'w', newline='', encoding='utf-8') as file:\n",
    "    csv_writer = csv.writer(file)\n",
    "    # Write the header\n",
    "    csv_writer.writerow(tasks)\n",
    "\n",
    "    # Iterate over each paper and its corresponding results\n",
    "    for _, results in all_results.items():  # Remove the paper_title reference\n",
    "        # Prepare a row based on task results\n",
    "        row = []\n",
    "        # Append each task result to the row in the order of the tasks list\n",
    "        for task in tasks:\n",
    "            task_result = results.get(task, \"No result\")\n",
    "            if isinstance(task_result, dict) or isinstance(task_result, list):\n",
    "                task_result = json.dumps(task_result)  # Convert dict or list to string for CSV\n",
    "            row.append(task_result)\n",
    "        # Write the completed row to the CSV file\n",
    "        csv_writer.writerow(row)\n",
    "\n",
    "print(f\"Results have been written to {output_csv_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc70e8f8-a397-429d-a6ae-6b728db90f89",
   "metadata": {},
   "source": [
    "\"Accuracy Evaluation: 95% Confidence Interval\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f774fd8-5f1c-4b3d-8275-dda8ab60f2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.proportion import proportion_confint\n",
    "\n",
    "# Inputs for TP, FP, FN, TN\n",
    "TP = 76\n",
    "FP = 1\n",
    "FN = 1\n",
    "TN = 23\n",
    "\n",
    "# Total predictions (n) and correct predictions (x)\n",
    "n = TP + FP + FN + TN  # Total number of predictions\n",
    "x = TP + TN            # Correct predictions\n",
    "\n",
    "# Calculate 95% Clopper-Pearson Confidence Interval\n",
    "ci_low, ci_high = proportion_confint(count=x, nobs=n, alpha=0.05, method='beta')\n",
    "\n",
    "# Print the results\n",
    "print(f\"95% Confidence Interval for Accuracy: ({ci_low:.4f}, {ci_high:.4f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab709a1d-7d0c-437e-a0b6-45cb7bb8e876",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
