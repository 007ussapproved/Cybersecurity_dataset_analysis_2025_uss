{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6333102d-d53f-4cf1-a2f8-788ab6d952c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import json\n",
    "import PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57dd816c-13bb-4077-9f28-e276fd81942f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import PyPDF2\n",
    "\n",
    "# Function to extract PDFs from a folder\n",
    "def extract_pdfs_from_folder(pdf_folder):\n",
    "    pdf_files = []\n",
    "    for file_name in os.listdir(pdf_folder):\n",
    "        if file_name.endswith('.pdf'):  \n",
    "            pdf_files.append(os.path.join(pdf_folder, file_name))\n",
    "    return pdf_files\n",
    "\n",
    "# Function to extract the title from the PDF (first few lines of the first page)\n",
    "def extract_title_from_pdf(pdf_text):\n",
    "    # Split the text into lines\n",
    "    lines = pdf_text.splitlines()\n",
    "    \n",
    "    # Filtering out empty lines and return the first non-empty line as the title\n",
    "    for line in lines:\n",
    "        if line.strip():  # If the line is not empty\n",
    "            return line.strip()\n",
    "    return \"Unknown Title\"  # Fallback if no title found\n",
    "\n",
    "# Function to extract text from a PDF\n",
    "def extract_text_from_pdf(pdf_file):\n",
    "    try:\n",
    "        pdf_reader = PyPDF2.PdfReader(pdf_file)\n",
    "        pdf_text = \"\"\n",
    "        for page_num in range(len(pdf_reader.pages)):\n",
    "            page_text = pdf_reader.pages[page_num].extract_text()\n",
    "            if page_text:\n",
    "                pdf_text += page_text\n",
    "        return pdf_text\n",
    "    except KeyError as e:\n",
    "        print(f\"Error processing {pdf_file}: {e}\")\n",
    "        return None  # Return None if there's an issue with the PDF\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error processing {pdf_file}: {e}\")\n",
    "        return None  # Handle any other exceptions\n",
    "\n",
    "# Path to the folder containing PDF files\n",
    "pdf_folder = r\"/path/to/pdf/folder\"\n",
    "\n",
    "\n",
    "# Load PDF files and extract content\n",
    "papers = []\n",
    "pdf_files = extract_pdfs_from_folder(pdf_folder)\n",
    "for pdf_file in pdf_files:\n",
    "    text = extract_text_from_pdf(pdf_file)\n",
    "    if text:\n",
    "        title = extract_title_from_pdf(text)  # Try to extract title from the content\n",
    "        papers.append({\"title\": title, \"content\": text})\n",
    "        print(f\"Extracted text and title from: {pdf_file}\")\n",
    "    else:\n",
    "        print(f\"Failed to extract text from: {pdf_file}\")\n",
    "\n",
    "# Save extracted text to a JSONL file\n",
    "with open('extracted_test_papers_new_NEWEST.jsonl', 'w', encoding='utf-8') as f:\n",
    "    for paper in papers:\n",
    "        # Clean up surrogate pairs\n",
    "        safe_text = json.dumps(paper, ensure_ascii=False).encode('utf-8', 'ignore').decode('utf-8')\n",
    "        f.write(safe_text + \"\\n\")\n",
    "\n",
    "print(\"Extracted text from all PDFs has been saved\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "986237c8-9e20-40ba-bfe8-c2b1457529ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install tiktoken\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b2ae80-57c1-4032-b767-4309598b7007",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def load_papers_from_jsonl(file_path):\n",
    "    papers = []\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            for line in file:\n",
    "                try:\n",
    "                    paper = json.loads(line.strip())\n",
    "                    papers.append(paper)\n",
    "                except json.JSONDecodeError:\n",
    "                    print(f\"Skipping a line due to JSON decoding error.\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"The file {file_path} was not found.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred while reading the file: {e}\")\n",
    "\n",
    "    print(f\"Loaded papers from {file_path}.\")\n",
    "    return papers\n",
    "\n",
    "jsonl_file_path = \"extracted_test_papers_new_NEWEST.jsonl\"\n",
    "\n",
    "# Load papers\n",
    "papers = load_papers_from_jsonl(jsonl_file_path)\n",
    "\n",
    "# Display loaded papers (Optional)\n",
    "for i, paper in enumerate(papers[:5]):  # Limit display to the first 5 papers for readability\n",
    "    print(f\"Paper {i+1}:\")\n",
    "    print(f\"Title: {paper.get('title', 'No title provided')}\")\n",
    "    print(\"Content:\")\n",
    "    print(paper.get('content', 'No content provided')[:500])  # Print first 500 characters of content\n",
    "    print(\"-\" * 50)  # Separator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4edcd7eb-5fc2-4143-b31b-0f38a6047953",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "def load_papers_from_jsonl(file_path):\n",
    "    papers = []\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            for line in file:\n",
    "                papers.append(json.loads(line.strip()))\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading or processing file: {e}\")\n",
    "    return papers\n",
    "\n",
    "def calculate_total_tokens(papers):\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")  # Loading GPT-2 tokenizer\n",
    "    total_tokens = 0\n",
    "    for paper in papers:\n",
    "        content = paper.get('content', '')\n",
    "        tokens = tokenizer(content, add_special_tokens=False)['input_ids']  # Tokenize without adding special tokens\n",
    "        total_tokens += len(tokens)\n",
    "        print(f\"Paper: {paper['title']} - Tokens: {len(tokens)}\")\n",
    "    return total_tokens\n",
    "\n",
    "# Specify the path to your JSONL file\n",
    "jsonl_file_path = \"extracted_test_papers_new_NEWEST.jsonl\"\n",
    "\n",
    "# Load papers\n",
    "papers = load_papers_from_jsonl(jsonl_file_path)\n",
    "\n",
    "# Calculate total tokens\n",
    "total_tokens = calculate_total_tokens(papers)\n",
    "print(f\"Total number of tokens in the dataset: {total_tokens}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56922dc5-92c9-486a-a3e1-816873b32c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import json\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "\n",
    "\n",
    "load_dotenv(\"api_key.env\")\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "acm_ccs_structure = [\n",
    "    {\n",
    "        \"high_level_domain\": \"General and reference\",\n",
    "        \"subdomains\": [\n",
    "            {\n",
    "                \"subdomain\": \"Document types\",\n",
    "                \"examples\": [\n",
    "                    \"Surveys and overviews\", \"Reference works\", \"General conference proceedings\",\n",
    "                    \"Biographies\", \"General literature\", \"Computing standards, RFCs and guidelines\"\n",
    "                ]\n",
    "            },\n",
    "            {\n",
    "                \"subdomain\": \"Cross-computing tools and techniques\",\n",
    "                \"examples\": [\n",
    "                    \"Reliability\", \"Empirical studies\", \"Measurement\", \"Metrics\",\n",
    "                    \"Evaluation\", \"Experimentation\", \"Estimation\", \"Design\",\n",
    "                    \"Performance\", \"Validation\", \"Verification\"\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"high_level_domain\": \"Computer systems organization\",\n",
    "        \"subdomains\": [\n",
    "            {\"subdomain\": \"Architectures\", \"examples\": [\"Serial architectures\", \"Parallel architectures\", \"Distributed architectures\", \"Other architectures\"]},\n",
    "            {\"subdomain\": \"Embedded and cyber-physical systems\", \"examples\": [\"Sensor networks\", \"Robotics\", \"Sensors and actuators\", \"System on a chip\", \"Embedded systems\"]},\n",
    "            {\"subdomain\": \"Real-time systems\", \"examples\": [\"Real-time operating systems\", \"Real-time languages\", \"Real-time system specification\", \"Real-time system architecture\"]},\n",
    "            {\"subdomain\": \"Dependable and fault-tolerant systems and networks\", \"examples\": [\"Reliability\", \"Availability\", \"Maintainability and maintenance\", \"Processors and memory architectures\", \"Secondary storage organization\", \"Redundancy\", \"Fault-tolerant network topologies\"]}\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"high_level_domain\": \"Networks\",\n",
    "        \"subdomains\": [\n",
    "            {\"subdomain\": \"Network architectures\", \"examples\": [\"Network design principles\", \"Programming interfaces\"]},\n",
    "            {\"subdomain\": \"Network protocols\", \"examples\": [\"Network protocol design\", \"Protocol correctness\", \"Link-layer protocols\", \"Network layer protocols\", \"Transport protocols\", \"Session protocols\", \"Presentation protocols\", \"Application layer protocols\", \"OAM protocols\", \"Cross-layer protocols\", \"Network File System (NFS) protocol\"]},\n",
    "            {\"subdomain\": \"Network components\", \"examples\": [\"Intermediate nodes\", \"Physical links\", \"Middle boxes / network appliances\", \"End nodes\", \"Wireless access points, base stations and infrastructure\", \"Logical nodes\"]},\n",
    "            {\"subdomain\": \"Network algorithms\", \"examples\": [\"Data path algorithms\", \"Control path algorithms\", \"Network economics\"]},\n",
    "            {\"subdomain\": \"Network performance evaluation\", \"examples\": [\"Network performance modeling\", \"Network simulations\", \"Network experimentation\", \"Network performance analysis\", \"Network measurement\"]},\n",
    "            {\"subdomain\": \"Network properties\", \"examples\": [\"Network security\", \"Network range\", \"Network structure\", \"Network dynamics\", \"Network reliability\", \"Network mobility\", \"Network manageability\", \"Network privacy and anonymity\"]},\n",
    "            {\"subdomain\": \"Network services\", \"examples\": [\"Naming and addressing\", \"Cloud computing\", \"Location based services\", \"Programmable networks\", \"In-network processing\", \"Network management\", \"Network monitoring\"]},\n",
    "            {\"subdomain\": \"Network types\", \"examples\": [\"Network on chip\", \"Home networks\", \"Storage area networks\", \"Data center networks\", \"Wired access networks\", \"Cyber-physical networks\", \"Mobile networks\", \"Overlay and other logical network structures\", \"Wireless access networks\", \"Ad hoc networks\", \"Public Internet\", \"Packet-switching networks\"]}\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"high_level_domain\": \"Software and its engineering\",\n",
    "        \"subdomains\": [\n",
    "            {\"subdomain\": \"Software organization and properties\", \"examples\": [\"Contextual software domains\", \"Software system structures\", \"Software functional properties\", \"Extra-functional properties\"]},\n",
    "            {\"subdomain\": \"Software notations and tools\", \"examples\": [\"General programming languages\", \"Formal language definitions\", \"Compilers\", \"Context specific languages\", \"System description languages\", \"Development frameworks and environments\", \"Software configuration management and version control systems\", \"Software libraries and repositories\", \"Software maintenance tools\"]},\n",
    "            {\"subdomain\": \"Software creation and management\", \"examples\": [\"Designing software\", \"Software development process management\", \"Software development techniques\", \"Software verification and validation\", \"Software post-development issues\", \"Collaboration in software development\", \"Search-based software engineering\"]}\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"high_level_domain\": \"Information systems\",\n",
    "        \"subdomains\": [\n",
    "            {\"subdomain\": \"Data management systems\", \"examples\": [\"Database design and models\", \"Data structures\", \"Database management system engines\", \"Query languages\", \"Database administration\", \"Information integration\", \"Middleware for databases\"]},\n",
    "            {\"subdomain\": \"Information storage systems\", \"examples\": [\"Information storage technologies\", \"Record storage systems\", \"Storage replication\", \"Storage architectures\", \"Storage management\"]},\n",
    "            {\"subdomain\": \"Information systems applications\", \"examples\": [\"Enterprise information systems\", \"Collaborative and social computing systems and tools\", \"Spatial-temporal systems\", \"Decision support systems\", \"Mobile information processing systems\", \"Process control systems\", \"Multimedia information systems\", \"Data mining\", \"Digital libraries and archives\", \"Computational advertising\", \"Computing platforms\"]},\n",
    "            {\"subdomain\": \"World Wide Web\", \"examples\": [\"Web searching and information discovery\", \"Online advertising\", \"Web mining\", \"Web applications\", \"Web interfaces\", \"Web services\", \"Web data description languages\"]},\n",
    "            {\"subdomain\": \"Information retrieval\", \"examples\": [\"Document representation\", \"Information retrieval query processing\", \"Users and interactive retrieval\", \"Retrieval models and ranking\", \"Retrieval tasks and goals\", \"Evaluation of retrieval results\", \"Search engine architectures and scalability\", \"Specialized information retrieval\"]}\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"high_level_domain\": \"Security and privacy\",\n",
    "        \"subdomains\": [\n",
    "            {\"subdomain\": \"Cryptography\", \"examples\": [\"Key management\", \"Public key (asymmetric) techniques\", \"Symmetric cryptography and hash functions\", \"Cryptanalysis and other attacks\", \n",
    "            \"Information-theoretic techniques\", \"Mathematical foundations of cryptography\"]},\n",
    "            {\"subdomain\": \"Formal methods and theory of security\", \"examples\": [\"Trust frameworks\", \"Security requirements\", \"Formal security models\", \"Logic and verification\"]},\n",
    "            {\"subdomain\": \"Security services\", \"examples\": [\"Authentication\", \"Access control\", \"Pseudonymity, anonymity and untraceability\", \"Privacy-preserving protocols\", \n",
    "            \"Digital rights management\", \"Authorization\"]},\n",
    "            {\"subdomain\": \"Intrusion/anomaly detection and malware mitigation\", \"examples\": [\"Malware and its mitigation\", \"Intrusion detection systems\", \"Social engineering attacks\"]},\n",
    "            {\"subdomain\": \"Security in hardware\", \"examples\": [\"Tamper-proof and tamper-resistant designs\", \"Embedded systems security\", \"Hardware security implementation\", \n",
    "            \"Hardware attacks and countermeasures\", \"Hardware reverse engineering\"]},\n",
    "            {\"subdomain\": \"Systems security\", \"examples\": [\"Operating systems security\", \"Browser security\", \"Distributed systems security\", \"Information flow control\", \n",
    "            \"Denial-of-service attacks\", \"Firewalls\", \"Vulnerability management\", \"File system security\"]},\n",
    "            {\"subdomain\": \"Network security\", \"examples\": [\"Security protocols\", \"Web protocol security\", \"Mobile and wireless security\", \"Denial-of-service attacks\", \"Firewalls\"]},\n",
    "            {\"subdomain\": \"Database and storage security\", \"examples\": [\"Data anonymization and sanitization\", \"Management and querying of encrypted data\", \n",
    "            \"Information accountability and usage control\", \"Database activity monitoring\"]},\n",
    "            {\"subdomain\": \"Software and application security\", \"examples\": [\"Software security engineering\", \"Web application security\", \"Social network security and privacy\",\n",
    "            \"Domain-specific security and privacy architectures\", \"Software reverse engineering\"]},\n",
    "            {\"subdomain\": \"Human and societal aspects of security and privacy\", \"examples\": [\"Economics of security and privacy\", \"Social aspects of security and privacy\",\n",
    "            \"Privacy protections\", \"Usability in security and privacy\"]}\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"high_level_domain\": \"Human-centered computing\",\n",
    "        \"subdomains\": [\n",
    "            {\"subdomain\": \"Human computer interaction (HCI)\", \"examples\": [\"HCI design and evaluation methods\", \"Interaction paradigms\", \"Interaction devices\", \"HCI theory, concepts and models\", \"Interaction techniques\", \"Interactive systems and tools\", \"Empirical studies in HCI\"]},\n",
    "            {\"subdomain\": \"Interaction design\", \"examples\": [\"Interaction design process and methods\", \"Interaction design theory, concepts and paradigms\", \"Empirical studies in interaction design\", \"Systems and tools for interaction design\"]},\n",
    "            {\"subdomain\": \"Collaborative and social computing\", \"examples\": [\"Collaborative and social computing theory, concepts and paradigms\", \"Collaborative and social computing design and evaluation methods\", \"Collaborative and social computing systems and tools\", \"Empirical studies in collaborative and social computing\", \"Collaborative and social computing devices\"]},\n",
    "            {\"subdomain\": \"Ubiquitous and mobile computing\", \"examples\": [\"Ubiquitous and mobile computing theory, concepts and paradigms\", \"Ubiquitous and mobile computing systems and tools\", \"Ubiquitous and mobile devices\", \"Ubiquitous and mobile computing design and evaluation methods\", \"Empirical studies in ubiquitous and mobile computing\"]},\n",
    "            {\"subdomain\": \"Visualization\", \"examples\": [\"Visualization techniques\", \"Visualization application domains\", \"Visualization systems and tools\", \"Visualization theory, concepts and paradigms\", \"Empirical studies in visualization\", \"Visualization design and evaluation methods\"]},\n",
    "            {\"subdomain\": \"Accessibility\", \"examples\": [\"Accessibility theory, concepts and paradigms\", \"Empirical studies in accessibility\", \"Accessibility design and evaluation methods\", \"Accessibility technologies\", \"Accessibility systems and tools\"]}\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"high_level_domain\": \"Computing methodologies\",\n",
    "        \"subdomains\": [\n",
    "            {\"subdomain\": \"Symbolic and algebraic manipulation\", \"examples\": [\"Symbolic and algebraic algorithms\", \"Computer algebra systems\", \"Representation of mathematical objects\"]},\n",
    "            {\"subdomain\": \"Parallel computing methodologies\", \"examples\": [\"Parallel algorithms\", \"Parallel programming languages\"]},\n",
    "            {\"subdomain\": \"Artificial intelligence\", \"examples\": [\"Natural language processing\", \"Knowledge representation and reasoning\", \"Planning and scheduling\", \"Search methodologies\", \"Control methods\", \"Philosophical/theoretical foundations of artificial intelligence\", \"Distributed artificial intelligence\", \"Computer vision\"]},\n",
    "            {\"subdomain\": \"Machine learning\", \"examples\": [\"Learning paradigms\", \"Learning settings\", \"Machine learning approaches\", \"Machine learning algorithms\", \"Cross-validation\"]},\n",
    "            {\"subdomain\": \"Modeling and simulation\", \"examples\": [\"Model development and analysis\", \"Simulation theory\", \"Simulation types and techniques\", \"Simulation support systems\", \"Simulation evaluation\"]},\n",
    "            {\"subdomain\": \"Computer graphics\", \"examples\": [\"Animation\", \"Rendering\", \"Image manipulation\", \"Graphics systems and interfaces\", \"Image compression\", \"Shape modeling\"]},\n",
    "            {\"subdomain\": \"Distributed computing methodologies\", \"examples\": [\"Distributed algorithms\", \"Distributed programming languages\"]},\n",
    "            {\"subdomain\": \"Concurrent computing methodologies\", \"examples\": [\"Concurrent programming languages\", \"Concurrent algorithms\"]}\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"high_level_domain\": \"Social and professional topics\",\n",
    "        \"subdomains\": [\n",
    "            {\"subdomain\": \"Professional topics\", \"examples\": [\"Computing industry\", \"Management of computing and information systems\", \"History of computing\", \"Computing education\", \"Computing and business\", \"Computing profession\"]},\n",
    "            {\"subdomain\": \"Computing / technology policy\", \"examples\": [\"Intellectual property\", \"Privacy policies\", \"Censorship\", \"Surveillance\", \"Commerce policy\", \"Network access control\", \"Computer crime\", \"Government technology policy\", \"Medical information policy\"]},\n",
    "            {\"subdomain\": \"User characteristics\", \"examples\": [\"Race and ethnicity\", \"Religious orientation\", \"Gender\", \"Sexual orientation\", \"People with disabilities\", \"Geographic characteristics\", \"Cultural characteristics\", \"Age\"]}\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "\n",
    "acm_json = json.dumps(acm_ccs_structure, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f506b7-1007-452b-ac30-a1f32539a2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_sub_subdomain_map(acm_structure):\n",
    "    mapping = {}\n",
    "    for domain in acm_structure:\n",
    "        high_domain = domain[\"high_level_domain\"]\n",
    "        for sub in domain[\"subdomains\"]:\n",
    "            subdomain = sub[\"subdomain\"]\n",
    "            for example in sub.get(\"examples\", []):\n",
    "                mapping[example.lower()] = {\"high_level_domain\": high_domain, \"subdomain\": subdomain}\n",
    "    return mapping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "611ff520-0c80-40d3-9387-c3ddd436cdac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def generate_system_prompt(paper, task, dataset_names):\n",
    "    title = paper['title']\n",
    "    content = paper['content']\n",
    "\n",
    "    # Extract title first and then reuse in other tasks\n",
    "    if task == \"title\":\n",
    "        return f\"\"\"\n",
    "        You are tasked with extracting the title of the provided cybersecurity paper.\n",
    "        \n",
    "        Guidelines and Rules:\n",
    "        \\t1. The title is often at the top of the first page.\n",
    "        \\t2. Extract the title in its entirety.\n",
    "        \n",
    "        Here is the paper content:\n",
    "        <Start of Paper Content>\n",
    "        {content}\n",
    "        <End of Paper Content>\n",
    "        \n",
    "        Your response must be returned in the following JSON format:\n",
    "        {{\n",
    "            \"title\": \"Title of the paper here\"\n",
    "        }}\n",
    "\n",
    "        Your response: \"\"\"\n",
    "\n",
    "    elif task == \"authors_name\":\n",
    "        return f\"\"\"\n",
    "        You are tasked with extracting the authors' names from the provided cybersecurity paper.\n",
    "        \n",
    "        Guidelines and Rules:\n",
    "        \\t1. The authors' names are usually listed directly below the title.\n",
    "        \\t2. Extract all the authors, separated by commas.\n",
    "        \n",
    "        Here is the paper content:\n",
    "        <Start of Paper Content>\n",
    "        {content}\n",
    "        <End of Paper Content>\n",
    "        \n",
    "        Your response must be returned in the following JSON format:\n",
    "        {{\n",
    "            \"authors\": \"Comma-separated list of authors' names here\"\n",
    "        }}\n",
    "\n",
    "        Your response: \"\"\"\n",
    "\n",
    "    elif task == \"conference_name\":\n",
    "        return f\"\"\"\n",
    "        You are tasked with extracting the conference name where the paper was presented.\n",
    "        \n",
    "        Guidelines and Rules:\n",
    "        \\t1. The conference name is usually found at the top or bottom of the first page.\n",
    "        \\t2. Use the short form (USS, NDSS, ACSAC, SP, CCS) if applicable.\n",
    "        \n",
    "        Here is the paper content:\n",
    "        <Start of Paper Content>\n",
    "        {content}\n",
    "        <End of Paper Content>\n",
    "        \n",
    "        Your response must be returned in the following JSON format:\n",
    "        {{\n",
    "            \"conference\": \"Short form of conference name (USS, NDSS, ACSAC, SP, CCS)\"\n",
    "        }}\n",
    "\n",
    "        Your response: \"\"\"\n",
    "\n",
    "    elif task == \"published_year\":\n",
    "        return f\"\"\"\n",
    "        You are tasked with extracting the year of publication from the provided cybersecurity paper.\n",
    "        \n",
    "        Guidelines and Rules:\n",
    "        \\t1. The year of publication is usually found near the conference name or at the bottom of the first page.\n",
    "        \n",
    "        Here is the paper content:\n",
    "        <Start of Paper Content>\n",
    "        {content}\n",
    "        <End of Paper Content>\n",
    "        \n",
    "        Your response must be returned in the following JSON format:\n",
    "        {{\n",
    "            \"year\": \"Year of publication here\"\n",
    "        }}\n",
    "        \n",
    "        Your response: \"\"\"\n",
    "\n",
    "    elif task == \"school_institution\":\n",
    "        return f\"\"\"\n",
    "        You are tasked with extracting the school or institution name(s) associated with the authors of the provided cybersecurity paper.\n",
    "        \n",
    "        Guidelines and Rules:\n",
    "        \\t1. The institution or school is often listed near the authors' names, either directly below or in the footer of the first page.\n",
    "        \\t2. Extract all institutions mentioned, separated by commas if there are multiple.\n",
    "        \n",
    "        Here is the paper content:\n",
    "        <Start of Paper Content>\n",
    "        {content}\n",
    "        <End of Paper Content>\n",
    "        \n",
    "        Your response must be returned in the following JSON format:\n",
    "        {{\n",
    "            \"school\": \"Comma-separated list of schools/institutions\"\n",
    "        }}\n",
    "        \n",
    "        Your response: \"\"\"\n",
    "        \n",
    "    elif task == \"domain\":\n",
    "        acm_json = json.dumps(acm_ccs_structure, indent=4)\n",
    "\n",
    "        subdomain_map = {}\n",
    "        for domain in acm_ccs_structure:\n",
    "            for sub in domain[\"subdomains\"]:\n",
    "                for example in sub.get(\"examples\", []):\n",
    "                    subdomain_map[example] = {\n",
    "                        \"high_level_domain\": domain[\"high_level_domain\"],\n",
    "                        \"subdomain\": sub[\"subdomain\"]\n",
    "                    }\n",
    "        flat_map_json = json.dumps(subdomain_map, indent=4)\n",
    "\n",
    "        return f\"\"\"\n",
    "        You are tasked with identifying the **ACM Computing Classification System (CCS)** research domains for the following cybersecurity paper titled: \\\"{title}\\\".\n",
    "\n",
    "        Your job is to return the correct **\\\"high_level_domain\\\"** and its corresponding **\\\"subdomain\\\"**, based on the paper’s content and the ACM CCS structure\n",
    "        provided below.\n",
    "        Guideline and Rules:\n",
    "\n",
    "        t1. **Strictly follow the ACM CCS structure** below.\n",
    "        \\t- For each domain, you MUST use:\n",
    "        \\t- **high_level_domain**: One of the official ACM categories (e.g., \\\"Security and privacy\\\").\n",
    "        \\t- **subdomain**: Choose ONLY from the subdomains under the high-level domain as defined in the ACM structure.\n",
    "        t2.  Never invent sub-subdomains. The ACM CCS structure you are given ends at the subdomain level.\n",
    "        \\t- Even if the paper mentions a sub-subdomain (like \\\"Usability in security and privacy\\\"), your job is to **map it to the correct subdomain**\n",
    "        from the ACM structure.\n",
    "        \\t- Use the mapping guide provided below under <SUB_SUBDOMAIN_MAPPING> to assist with this mapping task.\n",
    "        \\t3. You are allowed to return **multiple (high_level_domain, subdomain)** pairs if the paper covers more than one domain.\n",
    "        \\t4. Special Rule:\n",
    "        \\t- If the conference is **NDSS** **Usenix** or **IEEE S&P**, always set **high_level_domain** as \\\"Security and privacy\\\" (if relevant) \n",
    "        and identify the correct **subdomain** based on content. \n",
    "        \\t- For papers from **NDSS** determine the **subdomain** by reading the paper content, typically falling under \"Network security\" or \"Systems security\"\n",
    "        depending on the focus. \n",
    "\n",
    "        Json Output Examples:\n",
    "        Example #1 : For the paper titled **\"(Un)informed Consent: Studying GDPR Consent Notices in the Field\"** with the following CCS concepts:\n",
    "\n",
    "        • Security and privacy → Usability in security and privacy\n",
    "        • Human-centered computing → Empirical studies in interaction design\n",
    "        • Social and professional topics → Government technology policy\n",
    "\n",
    "        You should respond with:\n",
    "\n",
    "        ```json\n",
    "        [\n",
    "          {{\n",
    "            \"high_level_domain\": \"Security and privacy\",\n",
    "            \"subdomain\": \"Human and societal aspects of security and privacy\"\n",
    "          }},\n",
    "        {{\n",
    "            \"high_level_domain\": \"Human-centered computing\",\n",
    "            \"subdomain\": \"Interaction design\"\n",
    "          }},\n",
    "          {{\n",
    "          \"high_level_domain\": \"Social and professional topics\",\n",
    "            \"subdomain\": \"Computing / technology policy\"\n",
    "          }}\n",
    "        ]\n",
    "        ```\n",
    "\n",
    "        Here is the full ACM CCS structure:\n",
    "        <ACM_CCS_START>\n",
    "        {acm_json}\n",
    "        <ACM_CCS_END>\n",
    "\n",
    "        Sub-subdomain mapping to subdomains:\n",
    "        <SUB_SUBDOMAIN_MAPPING>\n",
    "        {flat_map_json}\n",
    "        <SUB_SUBDOMAIN_MAPPING_END>\n",
    "\n",
    "        Start of Paper Content:\n",
    "        {content}\n",
    "        End of Paper Content:\n",
    "\n",
    "        Your response:\"\"\"\n",
    "\n",
    "    elif task == \"dataset_name\":\n",
    "        datasets = [\n",
    "            {\n",
    "                \"unique_id\": \"null\",  # Set to null if not found in existing datasets\n",
    "                \"dataset_name\": \"Name of the first dataset you find\",\n",
    "                \"contributors\": \"Comma separated string of contributors names for the first dataset you find\",\n",
    "                \"doi\": \"DOI for the first dataset you find. If not available, this should be an empty string.\",\n",
    "                \"url\": \"URL link to the first dataset you find. If not available, this should be an empty string.\",\n",
    "            },\n",
    "            {\n",
    "                \"unique_id\": \"null\",  # Set to null if not found in existing datasets\n",
    "                \"dataset_name\": \"Name of the second dataset you find\",\n",
    "                \"contributors\": \"Comma separated string of contributors names for the second dataset you find\",\n",
    "                \"doi\": \"DOI for the second dataset you find. If not available, this should be an empty string.\",\n",
    "                \"url\": \"URL link to the second dataset you find. If not available, this should be an empty string.\",\n",
    "            }\n",
    "        ]\n",
    "        datasets_json = json.dumps({\"datasets\": datasets}, indent=4)\n",
    "        \n",
    "        return f\"\"\"\n",
    "        You are tasked with identifying and extracting datasets from the cybersecurity paper titled \"{title}\".\n",
    "        \n",
    "        Guidelines and Rules:\n",
    "        **STRICTLY FOLLOW ALL GUIDELINES**\n",
    "        \\t1.**Definition of a Dataset**:\n",
    "        \\t- A dataset is a named collection of data used for experiments, evaluation, training, testing, or comparison.\n",
    "        \\t- Examples: CICIDS2017, UNSW-NB15, CAIDA, Alexa top 1 million, HDFS, etc. In one paper they can use as much dataset they want, \n",
    "        for instance; if they have used 10 dataset so return all 10 dataset in output.\n",
    "        \\t- Custom-created datasets by the authors also count if explicitly mentioned as such.\n",
    "        \\t- Datasets can be mentioned explicitly by name (e.g., \"We use UNSW-NB15\") or implicitly (e.g., \"we use the dataset from [25]\" \n",
    "        if reference [25] clearly points to a dataset).\n",
    "\n",
    "        \\t2. **Be Comprehensive & Systematic:**\n",
    "        \\t- Carefully read the entire paper content (including references and methodology sections).\n",
    "        \\t- Identify every dataset mentioned, not just the first few. If you find 10 datasets, list all 10.\n",
    "        \\t- If the same dataset is mentioned multiple times under slightly different names (e.g., \"HDFS dataset\", \"HDFS logs\"), consider them \n",
    "        as referring to the same dataset.\n",
    "        \n",
    "        \\t3. **Real Example from a Paper**:\n",
    "        Consider the ACM CCS paper \"Recompose Event Sequences vs. Predict Next Events: A Novel Anomaly Detection Approach for Discrete Event Logs\"\n",
    "        as an example:\n",
    "        \\t- Introduction: \"DabLog achieves 97.18% and 80.25% F1 scores in evaluation upon HDFS system logs and UNSW-NB15 traffic logs...\"\n",
    "        \\t- Motivation section: \"Both methods were evaluated upon the same HDFS dataset [38, 39]...\"\n",
    "        \\t- Evaluation section: \"We evaluate DabLog with two datasets: UNSW-NB15 traffic logs [29] and HDFS console logs [39]...\"\n",
    "        \n",
    "        From these mentions, they clearly identify two datasets:\n",
    "        \\t- \"HDFS\"\n",
    "        \\t- \"UNSW-NB15\"\n",
    "        \n",
    "        In such a scenario, both \"HDFS\" and \"UNSW-NB15\" must be returned.\n",
    "        \n",
    "        \\t4. **Consider Reference-Based Mentions**:\n",
    "        \\t- If the paper references a dataset indirectly, for example, \"the same HDFS dataset [38, 39],\" then check references. If these references are known sources \n",
    "        for the HDFS dataset, include it.\n",
    "        \\t- For Example: in the paper \"DoubleX: Statically Detecting Vulnerable Data Flows\" author(s) have clearly mentioned \n",
    "        \"To evaluate DoubleX false negatives, we consider the dataset of vulnerable extensions released by Somé with EmPoWeb. His paper [72] provides a list of extension \n",
    "        IDs and corresponding vulnerabilities. Of the 171 Chrome extensions he reported as vulnerable in 2019, 82 still existed on March 16, 2021.\" So which mean they \n",
    "        used this **Chrome extensions dataset** for DoubleX evaluation.\n",
    "        \\t5. **No Guessing or Inferring**:\n",
    "        \n",
    "        \\t- Do not guess or infer a dataset if it's not explicitly mentioned.\n",
    "        \\t- Attacks, vulnerabilities, software tools, protocols, or platforms are not datasets.\n",
    "        \\t- If after thoroughly reviewing the paper and references you find no dataset mentioned, return 'null':\n",
    "        \n",
    "         {{\n",
    "           \"datasets\": null\n",
    "         }}\n",
    "\n",
    "        \\t6.**Do not confuse datasets with other elements**:\n",
    "        \\t- Vulnerability Codes: These are vulnerability codes, so be vigilant. Examples include \"CVE-2019-14815\", \"CVE-2016-4997\", and \"CVE-2017-9074\". Be vigilant with this information, For Example: In the paper \"Automated Bug Hunting With Data-Driven Symbolic RootCause Analysis\" authors haven't used any dataset, instead CVE (Common Vulnerabilities and Exposures) is used as part of the analysis, particularly focusing on specific vulnerabilities and their contexts. However, CVE is not treated as a \"dataset\" in the conventional sense, as it serves more as a standardized catalog for identifying known security vulnerabilities. So return this paper's output as 'null' .\n",
    "        \\t\\t- Again CVE are not 'datasets', A CVE (Common Vulnerabilities and Exposures) is a publicly disclosed cybersecurity vulnerability or exposure in a software or hardware system. Each CVE is assigned a unique identifier (CVE ID) and is documented in a centralized database to help organizations track, assess, and address security flaws.\n",
    "        \\t- Attacks: These are attack techniques, not datasets. Examples include \"SQL injection\", \"DDoS\", and \"Phishing\".\n",
    "        \\t- Bugs: These represent software flaws or defects. Examples include \"software flaws\" and \"defects\".\n",
    "        \\t- Kernel Modules: These are components of the OS, not datasets. Examples include \"ipv6.ko\" and \"nf_tables.ko\".\n",
    "        \\t- Network Protocols: These are communication protocols. Examples include \"TCP\", \"UDP\", \"IPv4\", and \"IPv6\".\n",
    "        \\t- Software Libraries or Packages: These are tools or resources, not datasets. Examples include \"libc.so\" and \"openssl\".\n",
    "        \\t- Standalone Applications and Benchmark Suites: SPEC CPU2006, NGINX, and PostgreSQL are not software libraries or packages.\n",
    "        \\t\\t- SPEC CPU2006 is a benchmark suite used to evaluate CPU and memory performance across standardized tasks, primarily for research and testing purposes. Fo example in the paper \"VIP: Safeguard Value Invariant Property for Thwarting Critical Memory Corruption Attacks\", no dataset is used , which mean you will return \"null\" output, don't consider **SPEC CPU2006** as a dataset.\n",
    "        \\t\\t- NGINX is a web server application commonly used to handle HTTP requests, serve static content, and balance load across servers.\n",
    "        \\t\\t- PostgreSQL is a standalone database management system (DBMS) that manages data storage, retrieval, and complex querying.\n",
    "        \\t\\t- ObliviSync is a secure file synchronization and backup system based on write-only ORAM techniques. It evaluates performance using realistic file size distributions without relying on traditional datasets.\n",
    "        \\t\\t\\t- Example:The paper \"ObliviSync: Practical Oblivious File Backup and Synchronization\" evaluates a system for secure file synchronization and backup but does not rely on traditional datasets. So technically, they haven't used any dataset, so it should return 'null'.\n",
    "        \\t- Permissioned Distributed Ledger Platform: Corda is a distributed ledger platform developed by R3 for businesses, focusing on privacy, efficiency, and regulatory compliance. Unlike public blockchains, Corda uses a permissioned network, ensuring that only authorized parties can participate and view transactions. Corda achieves privacy through point-to-point communication and a unique notary system that prevents double-spending without broadcasting transactions. Its modular design supports smart contracts and can be tailored to different industries, making it suitable for applications in finance, healthcare, supply chain, and more. \n",
    "        \\t- Artifact: sometimes authors release their own artifact and shared it, don't confuse it with dataset.\n",
    "        \\t- Raspberry Pi: Raspberry Pi is a small, affordable computer, often used for educational purposes, DIY projects, and experiments in computing, robotics, and IoT, not a dataset. For example in the paper \"Indistinguishability Prevents Scheduler Side Channels in Real-Time Systems\" no dataset is used , which mean you will return \"null\" output, don't consider **Raspberry Pi** as a dataset.\n",
    "        \n",
    "        \\t7. For each dataset, identify:\n",
    "        \\t- **Name** of the dataset.\n",
    "        \\t- **Contributors** (authors or creators).\n",
    "        \\t- **DOI** The DOI of the dataset (if available)\n",
    "        \\t- **URL** The URL link of the dataset (if available)\n",
    "        \\t\\t- Look for DOIs and URLs in the reference section, especially for **custom-created but public** datasets. In these cases, the contributors are usually the authors of the paper, and they often explicitly mention sharing links to platforms like GitHub or other repositories. Be sure to check for these, but do not include any random GitHub or other links—only include links where the authors explicitly state that they have shared their datasets. Remain vigilant in confirming this information. \n",
    "        \\t\\t- If no dataset is found, return:\n",
    "         \n",
    "         {{\n",
    "           \"datasets\": null\n",
    "         }}\n",
    "\n",
    "        \n",
    "        - **Null Cases** Examples (**Check these example thoroughly before returning the ouput for same paper(s) mentioned below**):\n",
    "\n",
    "        Example 1: The paper \"Indistinguishability Prevents Scheduler Side Channels in Real-Time Systems\" is not a \n",
    "        dataset-related paper, which mean the haven't used any dataset, so return the output as 'null'. And other tasks like \n",
    "        **dataset_analysis_combined** , **dataset_categories** and **dataset_usage** will be 'null' too.\n",
    "        Json Output:\n",
    "        \n",
    "        {{\n",
    "           \"datasets\": null\n",
    "         }}\n",
    "    \n",
    "        Example 2: In the paper \"ZKCPlus: Optimized Fair-exchange Protocol Supporting Practical and Flexible Data Exchange\", which mean the haven't used any dataset, so return the output as 'null'. And other tasks like **dataset_analysis_combined** , **dataset_categories** and **dataset_usage** will be 'null' too.\n",
    "        \n",
    "        Json Output:\n",
    "          \n",
    "        {{\n",
    "           \"datasets\": null\n",
    "         }}\n",
    "\n",
    "         Example 3: In the paper \"DPGen: Automated Program Synthesis for Differential Privacy\" , which mean the haven't used any dataset, so return the output as 'null'. And other tasks like **dataset_analysis_combined** , **dataset_categories** and **dataset_usage** will be 'null' too.\n",
    "         \n",
    "         Json Output:\n",
    "           \n",
    "         {{\n",
    "           \"datasets\": null\n",
    "         }}\n",
    "         \n",
    "         Example 4: In the paper \"A Security Framework for Distributed Ledgers\" , which mean the haven't used any dataset, so return the output as 'null'. And other tasks like **dataset_analysis_combined** , **dataset_categories** and **dataset_usage** will be 'null' too.\n",
    "         \n",
    "         Json Output:\n",
    "           \n",
    "         {{\n",
    "           \"datasets\": null\n",
    "         }}\n",
    "\n",
    "         \n",
    "         - Other Examples:\n",
    "         \n",
    "         Example 1: In the paper \"WristPrint: Characterizing User Re-identification Risks from Wrist-worn Accelerometry Data\", the author used two **public** datasets like \"mORAL\" and \"WISDM\".\n",
    " {{\n",
    "        \"datasets\": [\n",
    "            {{\n",
    "                \"unique_id\": \"null\",\n",
    "                \"dataset_name\": \"mORAL\",\n",
    "                \"contributors\": \"Sayma Akther, Nazir Saleheen, Shahin Alan Samiei, Vivek Shetty, Emre Ertin, Santosh Kumar\",\n",
    "                \"doi\": \"\",\n",
    "                \"url\": \"\"\n",
    "            }},\n",
    "            {{\n",
    "                \"unique_id\": \"null\",\n",
    "                \"dataset_name\": \"WISDM\",\n",
    "                \"contributors\": \"Gary M Weiss\",\n",
    "                \"doi\": \"\",\n",
    "                \"url\": \"https://www.uci.edu/ml/datasets/wisdm+smartphone+and+smartwatch+activity+and+biometrics+dataset\"\n",
    "            }}\n",
    "        ]\n",
    "    }}\n",
    "\n",
    "           Example 4: In the \"A Lightweight IoT Cryptojacking Detection Mechanism in Heterogeneous Smart Home Networks\"  a **public **dataset like \"Network traffic for machine learning classification\" or \"Benign dataset\" and **Custom-created datasets but public** dataset are used.\n",
    "    {{\n",
    "        \"datasets\": [\n",
    "            {{\n",
    "                \"unique_id\": \"null\",\n",
    "                \"dataset_name\": \"Iot cryptojacking\",\n",
    "                \"contributors\": \"Ege Tekiner, Abbas Acar, A. Selcuk Uluagac,\n",
    "                \"doi\": \"\",\n",
    "                \"url\": \"https://github.com/cslfiu/IoTCryptojacking\"\n",
    "            }},\n",
    "            {{\n",
    "                \"unique_id\": \"null\",\n",
    "                \"dataset_name\": \"Benign Dataset\",\n",
    "                \"contributors\": \"Víctor Labayen Guembe, Eduardo Magaña, Daniel Morató, Mikel Izal\",\n",
    "                \"doi\": \"10.17632/5pmnkshffm.1\",\n",
    "                \"url\": \"https://data.mendeley.com/datasets/5pmnkshffm/1\"\n",
    "            }}\n",
    "        ]\n",
    "    }}\n",
    "        \n",
    "        Here is the list of existing datasets:\n",
    "        <Existing dataset(s) start>\n",
    "        {existing_datasets}\n",
    "        <Existing dataset(s) stop>\n",
    "\n",
    "        Here is the paper:\n",
    "        <Start of Paper Content>\n",
    "        {content}\n",
    "        <End of Paper Content>\n",
    "\n",
    "        Your output must be returned in valid JSON format:\n",
    "\n",
    "        {datasets_json}\n",
    "    \n",
    "        Your response: \"\"\"\n",
    "\n",
    "    \n",
    "    elif task == \"dataset_analysis_combined\":\n",
    "        \n",
    "        return f\"\"\"\n",
    "\n",
    "        You are tasked with identifying the **availability**, **labeling_type**, and **dataset_type** for each dataset extracted in the **dataset_name** task for the cybersecurity paper titled \"{title}\".\n",
    "\n",
    "        ### **Guidelines and Rules:**\n",
    "\n",
    "        \\t1. For each dataset, you will identify the following:\n",
    "        \\t- **availability**: Whether the dataset is 'public', 'proprietary', 'restricted', 'Custom-created datasets, not shared', or 'Custom-created datasets but public'.\n",
    "        \\t\\t- **Public** are freely available for download (e.g., datasets hosted on websites like Kaggle, GitHub, or institutional repositories. \n",
    "        These datasets existed before the research and were not curated specifically by the authors.\n",
    "        - Example: in the paper \"Black-box Adversarial Attacks on Commercial Speech Platforms \n",
    "        with Minimal Information\", they used are publicaly available datasets. The output should look like this:\n",
    "        Json Output:\n",
    "\n",
    "         {{\n",
    "        \"dataset_analysis_combined\": [\n",
    "            {{\n",
    "                \"dataset_name\": \"Common Voice\",\n",
    "                \"availability\": \"public\",\n",
    "                \"labeling_type\": \"labeled\",\n",
    "                \"dataset_type\": \"real-world\"\n",
    "            }},\n",
    "                \"dataset_name\": \"Song\",\n",
    "                \"availability\": \"public\",\n",
    "                \"labeling_type\": \"labeled\",\n",
    "                \"dataset_type\": \"real-world\"\n",
    "            }},\n",
    "                \"dataset_name\": \"LibriSpeech\",\n",
    "                \"availability\": \"public\",\n",
    "                \"labeling_type\": \"labeled\",\n",
    "                \"dataset_type\": \"real-world\"\n",
    "            }},\n",
    "            }},\n",
    "                \"dataset_name\": \"Voxceleb dataset\",\n",
    "                \"availability\": \"public\",\n",
    "                \"labeling_type\": \"labeled\",\n",
    "                \"dataset_type\": \" real-world\"\n",
    "            }}\n",
    "        ]\n",
    "    }} \n",
    "\n",
    "         - Example: in the paper \"AHEAD: Adaptive Hierarchical Decomposition for Range Query under Local Differential Privacy\", they used are usually publicaly available datasets. The output should look like this:\n",
    "\n",
    "         Json Output:\n",
    "\n",
    "         {{\n",
    "        \"dataset_analysis_combined\": [\n",
    "            {{\n",
    "                \"dataset_name\": \"Salaries\",\n",
    "                \"availability\": \"public\",\n",
    "                \"labeling_type\": \"labeled\",\n",
    "                \"dataset_type\": \"real-world\"\n",
    "            }},\n",
    "                \"dataset_name\": \"blackfriday\",\n",
    "                \"availability\": \"public\",\n",
    "                \"labeling_type\": \"labeled\",\n",
    "                \"dataset_type\": \"real-world\"\n",
    "            }},\n",
    "                \"dataset_name\": \"Loan\",\n",
    "                \"availability\": \"public\",\n",
    "                \"labeling_type\": \"labeled\",\n",
    "                \"dataset_type\": \"real-world\"\n",
    "            }},\n",
    "            }},\n",
    "                \"dataset_name\": \"Financial\",\n",
    "                \"availability\": \"public\",\n",
    "                \"labeling_type\": \"labeled\",\n",
    "                \"dataset_type\": \"synthetic\"\n",
    "            }}\n",
    "        ]\n",
    "    }} \n",
    "         \n",
    "        \n",
    "        \\t\\t- **Proprietary** are those owned outright by an organization and are typically not accessible to the public, there is no route given to access them. \n",
    "        For instance, many internal company records, commercial databases, or market research datasets are considered proprietary because the owning entity.\n",
    "        \\t\\t- **Restricted** are accessible only under specific conditions (e.g., requiring permission, collaboration, DUA, or  licensing). \n",
    "        - Example 1: The HCUP dataset available at [https://hcup-us.ahrq.gov/tech_assist/centdist.jsp] is classified as a restricted dataset. \n",
    "        Although the data is derived from real-world healthcare information, access is granted only under specific conditions, such as requiring permission\n",
    "        through a data use agreement, ensuring that the sensitive nature of the data is properly managed.\n",
    "        \\t\\t- **Custom-created datasets, not shared** are generated specifically for the research project and are not shared publicly. For example the custom created dataset in the paper **(Un)informed Consent: Studying GDPR Consent Notices in the Field** is not shared so return it as **Custom-created datasets, not shared**.\n",
    "        \\t\\t- **Custom-created datasets but public** are custom datasets created by the authors but shared publicly.\n",
    "        \\t\\t- **Custom-created datasets, but restricted** are Custom-Restricted datasets created by authors but shared with access restrictions.\n",
    "         -Note: Sometimes, authors who create custom datasets may explicitly mention in their paper or dataset documentation \n",
    "        (URL/DOI or citation details) that, due to the large size of the dataset, they are unable to share it online but can provide access upon request\n",
    "        (don't make random guess). For example, in the paper \"Towards Precise Reporting of Cryptographic Misuses\", the authors mentioned in their GitHub link: \n",
    "        'Our original datasets consist of a data set of **3,489 open-source Android apps obtained from F-Droid**, and a data set of **1,437 firmwares** \n",
    "        collected from 6 vendors. Due to the large size of the two datasets (APK dataset: 49 GB, firmware dataset: 21 GB), it is difficult to share them online. \n",
    "        If you are interested in obtaining the original **F-Droid**dataset and **firmware** dataset, please contact us.'. \n",
    "        \n",
    "        -Json output:\n",
    "\n",
    "        {{\n",
    "        \"dataset_analysis_combined\": [\n",
    "            {{\n",
    "                \"dataset_name\": \"3489 open-source Android apps\",\n",
    "                \"availability\": \"Custom-created datasets, but restricted\",\n",
    "                \"labeling_type\": \"labeled\",\n",
    "                \"dataset_type\": \"realistic\"\n",
    "            }},\n",
    "                \"dataset_name\": \"1,437 firmware dataset\",\n",
    "                \"availability\": \"Custom-created datasets, but restricted\",\n",
    "                \"labeling_type\": not mentioned\",\n",
    "                \"dataset_type\": \"realistic\"\n",
    "            }},\n",
    "        ]\n",
    "    }}\n",
    "        \n",
    "        - Example 1: if authors download data (e.g., APK files or malware samples) from platforms like VirusTotal, then apply filtering, labeling, or feature extraction to create a tailored dataset, the resulting dataset is custom-created. While the original source (e.g. VirusTotal) can be cited, the curated dataset is distinct from the original collection and should be classified as 'Custom-created datasets, not shared' or 'Custom-created datasets but public', depending on whether the authors shared it publicly. Like in the paper **EIGER: Automated IOC Generation for Accurate and Interpretable Endpoint Malware Detection** they have collected 162K Malware Samples from VirusTotal and Benign public sources of free Windows software but didnt shared their dataset so return it as **Custom-created datasets, not shared**. Identify this correctly, the output should look like this:\n",
    "        -Json Output:\n",
    "\n",
    "        {{\n",
    "        \"dataset_analysis_combined\": [\n",
    "            {{\n",
    "                \"dataset_name\": \"162K Malware Samples from VirusTotal\",\n",
    "                \"availability\": \"Custom-created datasets, not shared\",\n",
    "                \"labeling_type\": \"labeled\",\n",
    "                \"dataset_type\": \"realistic\"\n",
    "            }},\n",
    "            {{\n",
    "                \"dataset_name\": \"Benign public sources of free Windows software\",\n",
    "                \"availability\": \"Custom-created datasets, not shared\",\n",
    "                \"labeling_type\": \"labeled\",\n",
    "                \"dataset_type\": \"realistic\"\n",
    "            }},\n",
    "            {{\n",
    "                \"dataset_name\": \"Hybrid Analysis Dataset\",\n",
    "                \"availability\": \"Custom-created datasets, not shared\",\n",
    "                \"labeling_type\": \"labeled\",\n",
    "                \"dataset_type\": \"realistic\"\n",
    "            }}\n",
    "        ]\n",
    "    }}\n",
    "        \n",
    "        - Example 2: in the paper \"C3PO: Large-Scale Study of Covert Monitoring of C&C Servers via Over-Permissioned Protocol Infiltration\" where they collected **200,000 malware samples** over 15 year, identify this dataset as **Custom-created datasets, not shared**, since author(s) didnt mentioned sharing this dataset with the community. Identify it correctly, the output should look like this:\n",
    "        -Json Output:\n",
    "\n",
    "        {{\n",
    "        \"dataset_analysis_combined\": [\n",
    "            {{\n",
    "                \"dataset_name\": \"200k Malware Dataset\",\n",
    "                \"availability\": \"Custom-created datasets, not shared\",\n",
    "                \"labeling_type\": \"labeled\",\n",
    "                \"dataset_type\": \"realistic\"\n",
    "            }}\n",
    "        ]\n",
    "    }}\n",
    "        \n",
    "        - Example 3: in the paper \"Deterrence of Intelligent DDoS via Multi-Hop Traffic Divergence\", the author's collected **49.8 TB real dataset from a department at Tsinghua campus network**, identify this and return it as **Custom-created datasets, not shared**. Identify it correctly, The output should look like this:.\n",
    "        \n",
    "        -Json Output:\n",
    "\n",
    "        {{\n",
    "        \"dataset_analysis_combined\": [\n",
    "            {{\n",
    "                \"dataset_name\": \"Tsinghua Network Traffic Dataset\",\n",
    "                \"availability\": \"Custom-created datasets, not shared\",\n",
    "                \"labeling_type\": \"Not Mentioned\",\n",
    "                \"dataset_type\": \"realistic\n",
    "                \n",
    "                \"\n",
    "            }}\n",
    "        ]\n",
    "    }}\n",
    "        - Example 4: In the paper \"High Fidelity Data Reduction for Big Data Security Dependency Analyses\", the dataset was collected from a real enterprise environment for one month, which makes it a custom-created dataset. However, the authors didn't mention sharing it, so return it as **Custom-created datasets, not shared**, \n",
    "        and the **labeling_type** wasn't mentioned either, so return it as **Not Mentioned**. Identify it correctly, The output should look like this:\n",
    "        \n",
    "        -Json Output:\n",
    "\n",
    "        {{\n",
    "        \"dataset_analysis_combined\": [\n",
    "            {{\n",
    "                \"dataset_name\": \"Enterprise Security Dependency Dataset\",\n",
    "                \"availability\": \"Custom-created datasets, not shared\",\n",
    "                \"labeling_type\": \"Not Mentioned\",\n",
    "                \"dataset_type\": \"real-world\"\n",
    "            }},\n",
    "        ]\n",
    "    }}\n",
    "        \n",
    "        - Example 5: in the paper \"This Sneaky Piggy Went to the Android Ad Market: Misusing Mobile Sensors for Stealthy Data Exfiltration\" the datasets used are collected from **4.5K of the most popular apps**, **Two typing datasets** and **one typing datasets** all are **Custom-created datasets, not shared**..\n",
    "         -Json Output:\n",
    "\n",
    "        {{\n",
    "        \"dataset_analysis_combined\": [\n",
    "            {{\n",
    "                \"dataset_name\": \"4.5K Popular Apps Dataset\",\n",
    "                \"availability\": \"Custom-created datasets, not shared\",\n",
    "                \"labeling_type\": \"labeled\",\n",
    "                \"dataset_type\": \"real-world\"\n",
    "            }},\n",
    "                \"dataset_name\": \"Two Typing Datasets\",\n",
    "                \"availability\": \"Custom-created datasets, not shared\",\n",
    "                \"labeling_type\": \"labeled\",\n",
    "                \"dataset_type\": \"synthetic\"\n",
    "            }},\n",
    "                \"dataset_name\": \"One Typing Dataset,\n",
    "                \"availability\": \"Custom-created datasets, not shared\",\n",
    "                \"labeling_type\": \"labeled\",\n",
    "                \"dataset_type\": \"synthetic\"\n",
    "            }},\n",
    "        ]\n",
    "    }}\n",
    "        -  Example 6: in the paper \"BAPM: Block Attention Profiling Model for Multi-tab Website Fingerprinting Attacks on Tor\" has created and used following datasets, The output should look like this:\n",
    "        - Json Ouput:\n",
    "        {{\n",
    "        \"dataset_analysis_combined\": [\n",
    "            {{\n",
    "                \"dataset_name\": \"Close World Multi-Tab Dataset\",\n",
    "                \"availability\": \"Custom-created datasets, not shared\",\n",
    "                \"labeling_type\": \"labeled\",\n",
    "                \"dataset_type\": \"synthetic\"\n",
    "            }},\n",
    "                \"dataset_name\": \"Open World Multi-Tab Dataset\",\n",
    "                \"availability\": \"Custom-created datasets, not shared\",\n",
    "                \"labeling_type\": \"labeled\",\n",
    "                \"dataset_type\": \"synthetic\"\n",
    "            }},\n",
    "                \"dataset_name\": \"Three-Tab Dataset\",\n",
    "                \"availability\": \"Custom-created datasets, not shared\",\n",
    "                \"labeling_type\": \"labeled\",\n",
    "                \"dataset_type\": \"synthetic\"\n",
    "            }},\n",
    "            }},\n",
    "                \"dataset_name\": \"real world dataset\",\n",
    "                \"availability\": \"Custom-created datasets but public\",\n",
    "                \"labeling_type\": \"labeled\",\n",
    "                \"dataset_type\": \"real-world\"\n",
    "            }}\n",
    "        ]\n",
    "    }}\n",
    "    \n",
    "         - Example 7: in the paper \"PDiff: Semantic-based Patch Presence Testing for Downstream Kernels\" the datasets used are both customer-created but one is **Custom-created datasets, not shared** and another is **Custom-created datasets but public**. The output should look like this:\n",
    "         \n",
    "         -Json Output:\n",
    "\n",
    "        {{\n",
    "        \"dataset_analysis_combined\": [\n",
    "            {{\n",
    "                \"dataset_name\": \"CVE dataset\",\n",
    "                \"availability\": \"Custom-created datasets, not shared\",\n",
    "                \"labeling_type\": \"Not mentioned\",\n",
    "                \"dataset_type\": \"real-world\"\n",
    "            }},\n",
    "                \"dataset_name\": \"Kernel Image dataset,\n",
    "                \"availability\": \"Custom-created datasets but public\",\n",
    "                \"labeling_type\": \"labeled\",\n",
    "                \"dataset_type\": \"real-world\"\n",
    "            }},\n",
    "        ]\n",
    "    }}\n",
    "      \n",
    "        \\t-.**labeling_type**: Determine the labeling status of the dataset.\n",
    "        \\t\\t- **labeled**:  A dataset is considered labeled if the paper or the dataset’s official documentation (accessed via DOI, URL, or citation details) explicitly \n",
    "        states that data points have labels or categories.  \n",
    "        \\t\\t\\t- Example conditions:\n",
    "        - The paper says “We manually labeled the dataset.”\n",
    "        - The dataset’s website or documentation includes label files or describes classes/categories for each data point.\n",
    "        - If labeling is confirmed from external sources (DOI/URL/citation), specify how it was identified, e.g., \"labeled (via citation details)\" \n",
    "        or \"labeled (found via URL)\".\n",
    "        - For image datasets, if classes, annotations, or bounding boxes are mentioned, consider them as labeled.\n",
    "        \\t\\t- **unlabeled**: A dataset is considered unlabeled only if the paper or dataset documentation **explicitly states** that it has no labels or is unlabeled.  \n",
    "        \\t\\t\\t- For example, if the paper says, “The dataset is completely unlabeled,” or “We have no ground-truth labels,” then mark it as **unlabeled**.\n",
    "        \\t\\t\\t- If discovered via an external source (DOI/URL/citation) that explicitly says it’s unlabeled, note that as \"unlabeled (found via URL)\" or similar.\n",
    "        \\t\\t-**hybrid**: A dataset is considered hybrid if it explicitly contains both labeled and unlabeled data.  \n",
    "        \\t\\t\\t- For instance, if the paper says, “The dataset includes 10,000 labeled samples and 100,000 unlabeled samples,” return **hybrid**.\n",
    "        \\t\\t\\t- If the dataset’s documentation (DOI/URL/citation) mentions both labeled and unlabeled subsets, also mark it as **hybrid**.\n",
    "        \\t\\t- **re-labeled**: If the paper explicitly states that they took an existing dataset and re-annotated or re-labeled it for their study, return **re-labeled**.\n",
    "        \\t\\t\\t- For example, if it says, “We took the UNSW-NB15 dataset and re-labeled the events according to our criteria,” return **re-labeled**.\n",
    "        \\t\\t- **not mentioned**: If after thoroughly checking the paper, its references, and any accessible DOI/URL information, you cannot find any mention of labeling \n",
    "        status (no explicit mention of labeling, unlabeled status, hybrid, or re-labeling), return **not mentioned**.  \n",
    "        \\t\\t\\t- Use **not mentioned** if:\n",
    "        - The paper never states anything about labeling.\n",
    "        - The dataset’s official sources (DOI/URL) do not mention labeling.\n",
    "        - No external citation details clarify the labeling status.\n",
    "\n",
    "        -**Consistent Example Using HDFS**:\n",
    "        Suppose the paper mentions the HDFS dataset and references [38,39] for its original introduction:\n",
    "        \\t\\t\\t- The paper itself doesn’t state whether HDFS is labeled or unlabeled.  \n",
    "        \\t\\t\\t- The instructions say you can use citation details (i.e., papers [38,39]) to learn about the dataset’s labeling. \n",
    "        \\t\\t\\t- After checking the referenced papers (assuming you have \"web access\" through the citation details-i.e., you can infer what the source papers are known for):\n",
    "        - If the HDFS dataset source paper (Xu et al., SOSP ’09) mentions that the dataset consists of system logs classified by event types or that it is \n",
    "        commonly known that HDFS data is often annotated with specific event types, you can conclude it is **labeled** (e.g., \"labeled (via citation details)\"\n",
    "        if found in the referenced paper).\n",
    "        - If the source says explicitly it’s unlabeled logs (just raw logs without event types) and you confirm it from citation details, return \n",
    "        **unlabeled (via citation details)**.\n",
    "        - If you find both labeled and unlabeled samples mentioned in the original dataset source, return **hybrid**.\n",
    "        - If the paper or the reference does not clarify labeling at all and the dataset’s official documentation (if available) is not accessible, return **not mentioned**.\n",
    "        - If the paper says “We re-labeled the HDFS dataset to fit our classification scheme,” return **re-labeled**.\n",
    "        \\t- **dataset_type**: Determine the type of dataset.\n",
    "        \\t\\t- **Real-world**: The dataset is directly collected from a real-world system or environment without significant preprocessing. Examples include raw network traffic logs or unaltered user interaction data or a complete packet capture (PCAP) file from a corporate network during a normal workday.\n",
    "        \\t\\t- **realistic**: Data simulating real-world scenarios, but collected in controlled or lab environments to mimic actual conditions. This may involve preprocessing or specific configurations. Example: A network traffic dataset collected from real systems but heavily anonymized or filtered for privacy or anonymized DNS logs or cleaned financial transaction data.\n",
    "        \\t\\t- **synthetic**: The dataset is completely generated using simulations, models, or algorithms without \n",
    "        any direct data from real-world systems. Examples include simulated attack traffic or algorithmically generated \n",
    "        synthetic images, such as the **SYMTCP**.\n",
    "        \\t\\t\\t- Example: Datasets generated through symbolic execution (e.g., in the SYMTCP project) \n",
    "        are considered **synthetic** used in the paper \"SYMTCP: Eluding Stateful Deep Packet Inspection \n",
    "        with Automated Discrepancy Discovery\", The output should look like this:\n",
    "        - Json Output:\n",
    "        {{\n",
    "        \"dataset_analysis_combined\": [\n",
    "            {{\n",
    "                \"dataset_name\": \"SYMTCP dataset\",\n",
    "                \"availability\": \"Custom-created datasets but public\",\n",
    "                \"labeling_type\":\"labeled\",\n",
    "                \"dataset_type\": \"synthetic\"\n",
    "            }},\n",
    "        ]\n",
    "    }}\n",
    "        \\t\\t\\t- In the paper \"Preparing Network Intrusion Detection Deep Learning Models with Minimal Data Using Adversarial Domain Adaptation\", they have used two benchmark datasets; one is **hybrid** and another is **synthetic**.\n",
    "        - Another json Example:\n",
    "\n",
    "        {{\n",
    "        \"dataset_analysis_combined\": [\n",
    "            {{\n",
    "                \"dataset_name\": \"UNSW-NB15\",\n",
    "                \"availability\": \"public\",\n",
    "                \"labeling_type\":\"labeled\",\n",
    "                \"dataset_type\": \"hybrid\"\n",
    "            }},\n",
    "            {{\n",
    "                \"dataset_name\": \"NSL-KDD\",\n",
    "                \"availability\": \"public\",\n",
    "                \"labeling_type\": \"labeled\",\n",
    "                \"dataset_type\": \"synthetic\"\n",
    "            }},\n",
    "        ]\n",
    "    }}\n",
    "            \n",
    "        \n",
    "        \n",
    "        \\t- **hybrid**:  The dataset combines both real-world and synthetic elements. For example, \"UNSW-NB15\" dataset which is considered a \"hybrid dataset\", encompassing both real-world and synthetic elements.\n",
    "        \\t\\t- For example: in the paper \"Filtering DDoS Attacks from Unlabeled Network Traffic Data Using Online Deep Learning\", they have used two datasets \"CICIDS2017\" and \"CAIDA2007\". Be vigilant with \"CICIDS2017\" dataset whenever you found it in any paper make sure to return it's **dataset_type** as **realistic**. The output should look like this:\n",
    "        -Json Output:\n",
    "\n",
    "        {{\n",
    "        \"dataset_analysis_combined\": [\n",
    "            {{\n",
    "                \"dataset_name\": \"CICIDS2017\",\n",
    "                \"availability\": \"public\",\n",
    "                \"labeling_type\":\"labeled\",\n",
    "                \"dataset_type\": \"realistic\"\n",
    "            }},\n",
    "            {{\n",
    "                \"dataset_name\": \"CAIDA UCSD DDoS Attack 2007\",\n",
    "                \"availability\": \"public\",\n",
    "                \"labeling_type\": \"unlabeled\",\n",
    "                \"dataset_type\": \"realistic\"\n",
    "            }},\n",
    "        ]\n",
    "    }}\n",
    "        \n",
    "        \\t2. Ensure that the dataset names match the ones extracted from the **dataset_name** task.\n",
    "        \\t3. If no dataset is found in the **dataset_name** task, leave **dataset_analysis_combined** task 'null'. Be vigilant.\n",
    "        \n",
    "        **Null cases Examples**\n",
    "        Example 1 (using the same example used in **dataset_name** task): In the paper \"ZKCPlus: Optimized Fair-exchange Protocol Supporting Practical and Flexible Data Exchange\", which mean the haven't used any dataset, so return the output as 'null'. And other tasks like **dataset_analysis_combined** , **dataset_categories** and **dataset_usage** will be 'null' too.\n",
    "        \n",
    "        - Json Output:\n",
    "\n",
    "        {{\n",
    "\n",
    "        \"dataset_analysis_combined\": null\n",
    "    }}\n",
    "\n",
    "        Example 2 (using the same example used in **dataset_name** task): The paper \"Indistinguishability Prevents Scheduler Side Channels in Real-Time Systems\" is not a dataset-related paper, which mean the haven't used any dataset, so return the output as 'null'. And other tasks like **dataset_analysis_combined** , **dataset_categories** and **dataset_usage** will be 'null' too.\n",
    "\n",
    "        - Json Output:\n",
    "\n",
    "        {{\n",
    "\n",
    "        \"dataset_analysis_combined\": null\n",
    "    }}\n",
    "    \n",
    "        Here are the datasets extracted earlier:\n",
    "        {dataset_names}\n",
    "\n",
    "        <Start of Paper Content>\n",
    "        {content}\n",
    "        <End of Paper Content>\n",
    "\n",
    "        Your output must be returned only in valid JSON format using the following structure:\n",
    "         \n",
    "         {{\n",
    "        \"dataset_analysis_combined\": [\n",
    "            {{\n",
    "                \"dataset_name\": \"Exact dataset name from the **dataset_name** task\",\n",
    "                \"availability\": \"Extracted availability status (**public**, **proprietary**, **restricted**, **Custom-created datasets, not shared**, **Custom-created datasets but public**, or **Custom-created datasets, but restricted**)\",\n",
    "                \"labeling_type\": \"Extracted labeling type (**labeled**, **unlabeled**, **hybrid**, **Re-labeled**, or **not-mentioned**)\",\n",
    "                \"dataset_type\": \"Extracted dataset type (**real-world**,**realistic**, **synthetic**, **hybrid**)\"\n",
    "            }}\n",
    "        ]\n",
    "    }}\n",
    "\n",
    "    Output Examples:\n",
    "\n",
    "    **CAREFULLY CHECK THESE PAPER BEFORE RETURNING BACK THE OUTPUT**\n",
    "    \n",
    "\n",
    "        - Example 1: For the paper \"Differentially Private Publishing of High-dimensional Data\".\n",
    "        -Json Output:\n",
    "\n",
    "         {{\n",
    "        \"dataset_analysis_combined\": [\n",
    "            {{\n",
    "                \"dataset_name\": \"Netflix\",\n",
    "                \"availability\": \"public\",\n",
    "                \"labeling_type\":\"labeled\",\n",
    "                \"dataset_type\": \"real-world\"\n",
    "            }},\n",
    "            {{\n",
    "                \"dataset_name\": \"Transaction\",\n",
    "                \"availability\": \"public\",\n",
    "                \"labeling_type\":\"labeled\",\n",
    "                \"dataset_type\": \"real-world\"\n",
    "            }},\n",
    "            {{\n",
    "                \"dataset_name\": \"Movielens\",\n",
    "                \"availability\": \"public\",\n",
    "                \"labeling_type\":\"labeled\",\n",
    "                \"dataset_type\": \"real-world\"\n",
    "            }},\n",
    "            {{\n",
    "                \"dataset_name\": \"Document\",\n",
    "                \"availability\": \"public\",\n",
    "                \"labeling_type\":\"unlabeled (found via URL)\",\n",
    "                \"dataset_type\": \"real-world\"\n",
    "            }},\n",
    "            {{\n",
    "                \"dataset_name\": \"AOL\",\n",
    "                \"availability\": \"public\",\n",
    "                \"labeling_type\":\"unlabeled (found via URL)\",\n",
    "                \"dataset_type\": \"real-world\"\n",
    "            }},\n",
    "            {{\n",
    "                \"dataset_name\": \"Kosarak\",\n",
    "                \"availability\": \"public\",\n",
    "                \"labeling_type\": \"unlabeled (found via URL)\",\n",
    "                \"dataset_type\": \"real-world\"\n",
    "            }},\n",
    "          ]\n",
    "        }}\n",
    "        - Example 2: For the paper \"Recompose Event Sequences vs. Predict Next Events: \n",
    "        A Novel Anomaly Detection Approach for Discrete Event Logs\", be aware with **HDFS dataset**.\n",
    "        -Json Output:\n",
    "\n",
    "             {{\n",
    "        \"dataset_analysis_combined\": [\n",
    "            {{\n",
    "                \"dataset_name\": \"UNSW-NB15\",\n",
    "                \"availability\": \"public\",\n",
    "                \"labeling_type\":\"labeled\",\n",
    "                \"dataset_type\": \"hybrid\"\n",
    "            }},\n",
    "            {{\n",
    "                \"dataset_name\": \"HDFS dataset\",\n",
    "                \"availability\": \"public\",\n",
    "                \"labeling_type\": \"labeled (via citation details)\",\n",
    "                \"dataset_type\": \"realistic\"\n",
    "            }},\n",
    "           ]\n",
    "         }}\n",
    "            - Example 3: \"Model Extraction Attacks on Graph Neural Networks: Taxonomy and Realisation\".\n",
    "            - Json Output:\n",
    "\n",
    "             {{\n",
    "        \"dataset_analysis_combined\": [\n",
    "            {{\n",
    "                \"dataset_name\": \"Cora\",\n",
    "                \"availability\": \"public\",\n",
    "                \"labeling_type\":\"labeled\",\n",
    "                \"dataset_type\": \"real-world\"\n",
    "            }},\n",
    "            {{\n",
    "                \"dataset_name\": \"Pubmed\",\n",
    "                \"availability\": \"public\",\n",
    "                \"labeling_type\": \"labeled\",\n",
    "                \"dataset_type\": \"real-world\"\n",
    "            }},\n",
    "            {{\n",
    "                \"dataset_name\": \"Citeseer\",\n",
    "                \"availability\": \"public\",\n",
    "                \"labeling_type\": \"labeled\",\n",
    "                \"dataset_type\": \"real-world\"\n",
    "            }}\n",
    "           ]\n",
    "        }}\n",
    "        - Example 4: For the paper \"Continuous Release of Data Streams under both Centralized and Local Differential Privacy\", the output should look like:\n",
    "        -Json Output:\n",
    "             {{\n",
    "        \"dataset_analysis_combined\": [\n",
    "            {{\n",
    "                \"dataset_name\": \"DNS\",\n",
    "                \"availability\": \"public\",\n",
    "                \"labeling_type\":\"unlabeled\",\n",
    "                \"dataset_type\": \"real-world\"\n",
    "            }},\n",
    "            {{\n",
    "                \"dataset_name\": \"Fare\",\n",
    "                \"availability\": \"restricted\",\n",
    "                \"labeling_type\":\"unlabeled\",\n",
    "                \"dataset_type\": \"real-world\"\n",
    "            }},\n",
    "            {{\n",
    "                \"dataset_name\": \"Kosarak\",\n",
    "                \"availability\": \"public\",\n",
    "                \"labeling_type\":\"unlabeled\",\n",
    "                \"dataset_type\": \"real-world\"\n",
    "            }},\n",
    "            {{\n",
    "                \"dataset_name\": \"POS\",\n",
    "                \"availability\": \"public\",\n",
    "                \"labeling_type\": \"unlabeled\",\n",
    "                \"dataset_type\": \"real-world\"\n",
    "            }},\n",
    "           ]\n",
    "        }}\n",
    "\n",
    "        - Example 5: As an AI assitant, you have web access so if a dataset is cited is from another work, note its title, contributors, and source publication details for other tasks like **labeling_type**, **availability** or **dataset_type** to extract details by web searching from these citations. For Example:\n",
    "        - \"Vassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur. 2015. Librispeech: an ASR corpus based on public domain audiobooks. In Proc. of ICASSP.\"\n",
    "        - Example: in the paper \"MineSweeper: An In-depth Look into Drive-by Cryptocurrency Mining and Its Defense\" the author custom-created only one dataset and the stated \"We ran 50 Docker containers in parallel for one week mid-March 2018 to collect data from Alexa’s Top 1 Million websites (as of February 28, 2018)\". \n",
    "        Also the shared this dataset with the community **https://github.com/vusec/minesweeper**. The output should look like this:\n",
    "        \n",
    "        Json Output:\n",
    "         {{\n",
    "        \"dataset_analysis_combined\": [\n",
    "            {{\n",
    "                \"dataset_name\": \"collected dataset(minesweeper)\",\n",
    "                \"availability\": \"Custom-created datasets but public\",\n",
    "                \"labeling_type\": \"labeled(via URL)\",\n",
    "                \"dataset_type\": \"real-world\"\n",
    "            }},\n",
    "        ]\n",
    "    }}\n",
    "        - Example (c) in the paper \"Secure Multi-party Computation of Differentially Private Heavy Hitters\", two datasets are used, the output should look like this:\n",
    "        Json Output:\n",
    "        \n",
    "        {{\n",
    "        \"dataset_analysis_combined\": [\n",
    "            {{\n",
    "                \"dataset_name\": \"Zipf distribution\",\n",
    "                \"availability\": \"public\",\n",
    "                \"labeling_type\": \"labeled\",\n",
    "                \"dataset_type\": \"synthetic\"\n",
    "        \n",
    "            }},\n",
    "                \"dataset_name\": \"Online retail dataset\",\n",
    "                \"availability\": \"public\",\n",
    "                \"labeling_type\": \"unlabeled\",\n",
    "                \"dataset_type\": \"real-world\"\n",
    "            }}\n",
    "           ]\n",
    "        }} \n",
    "\n",
    "         Your response: \"\"\"\n",
    "\n",
    "    \n",
    "    elif task == \"dataset_categories\":\n",
    "        \n",
    "        return f\"\"\"\n",
    "\n",
    "        You are tasked with identifying the specific categories and subcategories of datasets extracted from the **dataset_name** task used in the cybersecurity paper titled \"{title}\".\n",
    "\n",
    "        Clarification:\n",
    "        The **dataset_categories** refers specifically to **what the dataset consists of** or contains, not how it is used in the research. \n",
    "        Focus on the dataset's inherent characteristics and contents.\n",
    "        Note: These categories are derived from the taxonomy outlined in the USENIX paper \"Cybersecurity Research Datasets: Taxonomy and Empirical Analysis\" by \n",
    "        Zheng et al., which provides a structured framework for categorizing cybersecurity datasets. Additionally, a new category for multimedia \n",
    "        data has been added based on evolving research needs.\n",
    "\n",
    "        Guidelines and Rules:\n",
    "\n",
    "        \\t1. By **dataset_categories**, we mean identifying whether a dataset belongs to the following major categories and their subcategories:\n",
    "\n",
    "        **Major Categories and Subcategories**:\n",
    "\n",
    "        \\t\\t- **Attacker-Related**:\n",
    "        \\t\\t  1. **Attacks**: Datasets containing information about malicious actions performed to harm systems (e.g., CICIDS2017, Kitsune etc).\n",
    "        \\t\\t  2. **Vulnerabilities**: Datasets capturing weaknesses in systems or software that attackers can exploit (e.g., CVE databases or Open Source \n",
    "        Vulnerability Database as a dataset).\n",
    "        \\t\\t  3. **Exploits**: Technical methods or tools used to execute attacks, such as exploit scripts or frameworks.\n",
    "        \\t\\t  4. **Cybercrime Infrastructures**: Datasets capturing illegal operations and tools, such as botnets, marketplaces, or malware delivery.\n",
    "        \\t\\t  5. **Malware**: is a curated collection of data samples that contain malicious software (malware) or artifacts derived from it.\n",
    "        Raw binaries or executables (e.g., .exe, .apk, .elf files), or Network traffic generated by malware (PCAP files, DNS queries, C2 communications) or etc.\n",
    "\n",
    "        \\t\\t- **Defender Artifacts**:\n",
    "        \\t\\t  1. **Alerts**: Logs or outputs from defensive systems like intrusion detection systems or firewalls.\n",
    "        \\t\\t  2. **Configurations**: Information on setup and configurations of defense systems (e.g., SSL certificate settings).\n",
    "\n",
    "        \\t\\t- **User & Organizational Characteristics**:\n",
    "        \\t\\t  1. **User Activities**: Data on the behavior of users or organizations (e.g., social media activity, browsing logs).\n",
    "        \\t\\t  2. **User Attitudes**: Survey data capturing opinions or sentiments on cybersecurity topics.\n",
    "        \\t\\t  3. **User Attributes**: Characteristics of users or organizations (e.g., demographic profiles or organizational metadata).\n",
    "\n",
    "        \\t\\t- **Macro-Level Internet Characteristics**:\n",
    "        \\t\\t  1. **Applications**: Data on Internet services or products (e.g., website rankings, mobile apps).\n",
    "        \\t\\t  2. **Network Traces**: Packet-level traffic data or network activity logs.\n",
    "        \\t\\t  3. **Topology**: Information on the structure of the Internet, such as AS relationships or routing paths.\n",
    "        \\t\\t  4. **Benchmarks**: contain information about measurements of Internet performance, such as upload/download speed or end-to-end \n",
    "        network reliability. For example in the paper \"Tackling bufferbloat in 3G/4G networks\"  Jiang and Wang constructed a dataset that measured \n",
    "        3G/4G network performance in the US and Korea.\n",
    "        \\t\\t  5. **Adverse Events**: Data on disruptions or outages, like failures caused by misconfigurations.\n",
    "\n",
    "        \\t\\t- **Visual and Multimedia Data** (New Category):\n",
    "        \\t\\t  1. **Image Datasets**: Datasets containing static visual data for tasks like classification, recognition, or detection (e.g., CIFAR-10, MNIST).\n",
    "        \\t\\t  2. **Video Datasets**: Datasets containing dynamic visual data for tasks like motion tracking or behavior analysis (e.g., UCF101, Kinetics).\n",
    "        \\t\\t  3. **Audio Datasets**: Datasets containing audio data (e.g., SpeechCommand, LibriSpeech)..\n",
    "        \\t\\t  4. **Multimodal Datasets**:  Datasets combining different types of data (e.g., images and text or audio and visual) for tasks like cross-modal \n",
    "        retrieval (e.g., Voxceleb).\n",
    "        \\t\\t  5. **Synthetic Media Datasets**: Artificially generated, any form of media datasets.\n",
    "        \\t\\t\\t - For Example: **Typing Motion Dataset (Two-Handed & One-Handed Typing)** used in the paper 'This Sneaky Piggy Went to the Android Ad Market:\n",
    "        Misusing Mobile Sensors for Stealthy Data Exfiltration'  which is artificially generated and falls under this sub-category.\n",
    "\n",
    "        \\t\\t- **Others-catchall** (New Category):\n",
    "        \\t\\t 1.**others**: If no **dataset** fall under above given category return **others**, that's are new catch-all\n",
    "        category.\n",
    "    \n",
    "\n",
    "        \\t2. **Examples for Clarity**:\n",
    "        \\t\\t- A dataset like **Netflix Ratings** used in privacy studies should be categorized under **User & Organizational Characteristics** -> **User Activities**.\n",
    "        \\t\\t- A dataset like **CAIDA AS Relationships**, which captures Internet topology data, should be categorized under **Macro-Level Internet Characteristics** -> **Topology**.\n",
    "        \\t\\t- A dataset like **CIFAR-10**, used for image classification, should be categorized under **Visual and Multimedia Data** -> **Image Datasets**.\n",
    "\n",
    "        \\t3. **Do Not Confuse with Domain**:\n",
    "        \\t\\t- **Domain** refers to the high-level research area (e.g., IoT, malware analysis).\n",
    "        \\t\\t- **Dataset Categories** focus exclusively on the dataset's inherent characteristics (e.g., attacks, vulnerabilities, defender artifacts).\n",
    "\n",
    "        \\t5. **Null Cases**:\n",
    "        \\t\\t. If no dataset found in **dataset_name** task, leave **dataset_categories** as null.\n",
    "\n",
    "        Here are the datasets extracted earlier:\n",
    "        \n",
    "        {dataset_names}\n",
    "\n",
    "        ### Output Structure:\n",
    "\n",
    "        The output must strictly follow this JSON structure:\n",
    "              \n",
    "      {{\n",
    "      \n",
    "      \"dataset_categories\": [\n",
    "            {{\n",
    "                \"dataset_name\": \"Kitsune\",\n",
    "                \"category\": \"attacker_related\",\n",
    "                \"subcategory\": \"attacks\",\n",
    "                \"attacker_related_items\":[\n",
    "                    {{\"name\": \"Fuzzing\"}},\n",
    "                    {{\"name\": \"ARP MitM\"}},\n",
    "                    {{\"name\": \"SSDP Flood\"}},\n",
    "                    {{\"name\": \"SYN DoS\"}},\n",
    "                    {{\"name\": \"Mirai Botnet\"}}\n",
    "                    ]\n",
    "                }},\n",
    "                \n",
    "                {{\n",
    "                \"dataset_name\": \"CIFAR-10\",\n",
    "                \"category\": \"visual_and_multimedia_data\",\n",
    "                \"subcategory\": \"image_datasets\",\n",
    "                \"visual_data_items\": [\n",
    "                    {{\"name\": \"Image Classification\"}}\n",
    "                    ]\n",
    "                }}\n",
    "              ]\n",
    "           }}\n",
    "\n",
    "    ### Output Example:\n",
    "    - For the **Kitsune** dataset, which contains nine attacks such as:\n",
    "        1. OS Scan\n",
    "        2. Fuzzing\n",
    "        3. Video Injection\n",
    "        4. ARP MitM\n",
    "        5. Active Wiretap\n",
    "        6. SSDP Flood\n",
    "        7. SYN DoS\n",
    "        8. SSL Renegotiation\n",
    "        9. Mirai Botnet\n",
    "\n",
    "      If the paper only utilizes attacks 2, 4, 6, 7, and 9, the output should list only those attacks.\n",
    "      \n",
    "      <Start of Paper Content>\n",
    "      {content}\n",
    "      <End of Paper Content>\n",
    "      \n",
    "      Your response: \"\"\"\n",
    "\n",
    "        \n",
    "    \n",
    "    \n",
    "    else:\n",
    "        raise ValueError(\"Invalid task\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f346efb3-740c-440a-869f-d757d8f9db68",
   "metadata": {},
   "source": [
    "Runner(Processing) Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7220e63-e84e-4b2f-a640-50af10809bf5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import openai\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=os.environ.get(\"OPENAI_API_KEY\")\n",
    ")\n",
    "\n",
    "# Incremental saving\n",
    "def save_incremental_results(results, output_file=\"results_incremental.jsonl\"):\n",
    "    with open(output_file, \"a\", encoding=\"utf-8\") as file:\n",
    "        for paper_title, result in results.items():\n",
    "            file.write(json.dumps({paper_title: result}) + \"\\n\")\n",
    "\n",
    "# Load previously saved results\n",
    "def load_saved_results(output_file=\"results_incremental.jsonl\"):\n",
    "    saved_titles = set()\n",
    "    saved_results = {}\n",
    "    try:\n",
    "        with open(output_file, \"r\", encoding=\"utf-8\") as file:\n",
    "            for line in file:\n",
    "                result = json.loads(line.strip())\n",
    "                for title, data in result.items():\n",
    "                    saved_titles.add(title)\n",
    "                    saved_results[title] = data\n",
    "    except FileNotFoundError:\n",
    "        print(f\"No saved results found in {output_file}. Starting fresh.\")\n",
    "    return saved_titles, saved_results\n",
    "\n",
    "# Function to process papers for multiple tasks\n",
    "def process_papers_for_tasks(papers, tasks, start_index=0, output_file=\"results_incremental.jsonl\"):\n",
    "    task_results = {}\n",
    "    for i, paper in enumerate(papers, start=start_index):\n",
    "        paper_title = paper['title']\n",
    "        print(f\"Processing paper {i + 1}/{len(papers) + start_index}: {paper_title}\")\n",
    "\n",
    "        # Skip already processed papers\n",
    "        if paper_title in processed_titles:\n",
    "            print(f\"Skipping already processed paper: {paper_title}\")\n",
    "            continue\n",
    "\n",
    "        task_results[paper_title] = {}\n",
    "\n",
    "        # Process each task for the paper\n",
    "        for task in tasks:\n",
    "            user_prompt = generate_system_prompt(paper, task, task_results[paper_title].get(\"dataset_name\", None))\n",
    "            try:\n",
    "                response = client.chat.completions.create(\n",
    "                    model=\"gpt-4o-mini\",\n",
    "                    messages=[\n",
    "                        {\"role\": \"user\", \"content\": user_prompt}\n",
    "                    ],\n",
    "                    temperature=0.2,\n",
    "                    max_tokens=5000\n",
    "                )\n",
    "                response_text = response.choices[0].message.content\n",
    "                print(f\"Result for {task}: {response_text}\")\n",
    "                task_results[paper_title][task] = response_text\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {task} for paper {i + 1}: {e}\")\n",
    "                task_results[paper_title][task] = f\"error: {str(e)}\"\n",
    "\n",
    "        # Save incremental results\n",
    "        save_incremental_results({paper_title: task_results[paper_title]}, output_file)\n",
    "\n",
    "    return task_results\n",
    "\n",
    "\n",
    "# Load saved results\n",
    "output_file = \"results_incremental.jsonl\"\n",
    "processed_titles, processed_results = load_saved_results(output_file)\n",
    "\n",
    "# Remaining papers\n",
    "remaining_papers = [paper for paper in test_papers if paper['title'] not in processed_titles]\n",
    "\n",
    "# Define the tasks you are going to process\n",
    "tasks = [\n",
    "    \"title\", \n",
    "    \"authors_name\", \n",
    "    \"conference_name\", \n",
    "    \"published_year\", \n",
    "    \"school_institution\", \n",
    "    \"domain\", \n",
    "    \"dataset_name\", \n",
    "    \"dataset_analysis_combined\", \n",
    "    \"dataset_categories\"\n",
    "]\n",
    "\n",
    "# Resume processing\n",
    "all_results = process_papers_for_tasks(remaining_papers, tasks, start_index=len(processed_titles), output_file=output_file)\n",
    "\n",
    "# Combine with saved results\n",
    "processed_results.update(all_results)\n",
    "\n",
    "# Save final results to a new file\n",
    "output_csv = \"results_merged_full_final.csv\"\n",
    "output_jsonl = \"results_merged_full_final.jsonl\"\n",
    "\n",
    "# Save to CSV\n",
    "import csv\n",
    "with open(output_csv, \"w\", newline=\"\", encoding=\"utf-8\") as csv_file:\n",
    "    writer = csv.writer(csv_file)\n",
    "    writer.writerow(tasks)  # Header row\n",
    "    for title, results in processed_results.items():\n",
    "        row = []\n",
    "        for task in tasks:\n",
    "            task_result = results.get(task, \"No result\")\n",
    "            if isinstance(task_result, (dict, list)):\n",
    "                task_result = json.dumps(task_result)  # Convert to string\n",
    "            row.append(task_result)\n",
    "        writer.writerow(row)\n",
    "\n",
    "# Save to JSONL\n",
    "with open(output_jsonl, \"w\", encoding=\"utf-8\") as jsonl_file:\n",
    "    for title, results in processed_results.items():\n",
    "        jsonl_file.write(json.dumps({title: results}) + \"\\n\")\n",
    "\n",
    "print(f\"Final results saved to {output_csv} and {output_jsonl}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f39c5b4-3b14-461f-bbf8-8aa54aabda58",
   "metadata": {},
   "source": [
    "DATA ANALYSIS AND VISUALIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23be4c89-0d16-40d7-800e-34fedd198525",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "import ast\n",
    "import re\n",
    "import chardet\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1) Detect CSV encoding\n",
    "file_path = \"results_merged_full_final.csv\"\n",
    "with open(file_path, \"rb\") as f:\n",
    "    enc = chardet.detect(f.read(100000))[\"encoding\"]\n",
    "print(f\"Detected Encoding: {enc}\")\n",
    "\n",
    "# 2) Prepare a small parser that strips ```json``` fences then tries json.loads or ast.literal_eval\n",
    "_fence_re = re.compile(r\"```(?:json)?\\s*|\\s*```\")\n",
    "\n",
    "def parse_record(raw: str) -> dict:\n",
    "    \"\"\"Strip out any ```json``` fences, then parse into a dict (JSON or Python literal).\"\"\"\n",
    "    if not raw:\n",
    "        return {}\n",
    "    s = _fence_re.sub(\"\", raw).strip()\n",
    "    # try real JSON\n",
    "    try:\n",
    "        obj = json.loads(s)\n",
    "        if isinstance(obj, dict):\n",
    "            return obj\n",
    "    except json.JSONDecodeError:\n",
    "        pass\n",
    "    # fallback to Python literal\n",
    "    try:\n",
    "        obj = ast.literal_eval(s)\n",
    "        if isinstance(obj, dict):\n",
    "            return obj\n",
    "    except Exception:\n",
    "        pass\n",
    "    return {}\n",
    "\n",
    "# 3) Count how many have a non‐empty `datasets` list\n",
    "dataset_related_count = 0\n",
    "non_dataset_related_count = 0\n",
    "\n",
    "with open(file_path, \"r\", encoding=enc, errors=\"replace\") as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    for row in reader:\n",
    "        raw_ds = row.get(\"dataset_name\", \"\")\n",
    "        rec = parse_record(raw_ds)\n",
    "        ds_list = rec.get(\"datasets\")\n",
    "        if isinstance(ds_list, list) and len(ds_list) > 0:\n",
    "            dataset_related_count += 1\n",
    "        else:\n",
    "            non_dataset_related_count += 1\n",
    "\n",
    "# 4) Calculate and print percentages\n",
    "total = dataset_related_count + non_dataset_related_count\n",
    "print(f\"Dataset-Related Papers: {dataset_related_count} ({dataset_related_count/total*100:.1f}%)\")\n",
    "print(f\"Non-Dataset-Related Papers: {non_dataset_related_count} ({non_dataset_related_count/total*100:.1f}%)\")\n",
    "\n",
    "# 5) Plot pie chart\n",
    "labels = [\"Dataset-Related\", \"Non-Dataset-Related\"]\n",
    "sizes  = [dataset_related_count, non_dataset_related_count]\n",
    "colors = [\"#4CAF50\", \"#FF5733\"]\n",
    "\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.pie(sizes, labels=labels, autopct=\"%1.1f%%\", startangle=90, colors=colors)\n",
    "plt.title(\"Distribution of Dataset-Related vs Non-Dataset-Related Papers\")\n",
    "plt.axis(\"equal\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4568db03-3a5c-402b-8ad5-954f5200f9dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install chardet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1696c71-9a37-4ba3-a730-1f365f65f336",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "import ast\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import chardet\n",
    "\n",
    "file_path = \"results_merged_full_final.csv\"\n",
    "with open(file_path, \"rb\") as raw:\n",
    "    detected = chardet.detect(raw.read(100000))\n",
    "encoding_used = detected[\"encoding\"]\n",
    "print(f\"Detected Encoding: {encoding_used}\")\n",
    "\n",
    "#  2) helper to strip fences and backticks\n",
    "def strip_fences(s: str) -> str:\n",
    "    \"\"\"Remove ```json ...``` or ``` ... ``` fences if present.\"\"\"\n",
    "    if not s:\n",
    "        return s\n",
    "    lines = s.splitlines()\n",
    "    # drop any lines that start or end with backticks\n",
    "    inner = [l for l in lines \n",
    "             if not (l.strip().startswith(\"```\") and l.strip().endswith(\"```\")) \n",
    "             and not l.strip().startswith(\"```json\")]\n",
    "    return \"\\n\".join(inner).strip()\n",
    "\n",
    "#  3) helper to parse the field \n",
    "def parse_conference_field(raw: str) -> str:\n",
    "    \"\"\"\n",
    "    Attempt json.loads, then ast.literal_eval, then fallback to raw.\n",
    "    Returns the conference code string.\n",
    "    \"\"\"\n",
    "    if raw is None:\n",
    "        return \"\"\n",
    "    s = raw.strip()\n",
    "    s = strip_fences(s)\n",
    "\n",
    "    # 1) try JSON\n",
    "    try:\n",
    "        obj = json.loads(s)\n",
    "        if isinstance(obj, dict) and \"conference\" in obj:\n",
    "            return str(obj[\"conference\"]).strip()\n",
    "    except json.JSONDecodeError:\n",
    "        pass\n",
    "\n",
    "    # 2) try Python literal eval (single-quoted dict)\n",
    "    try:\n",
    "        obj = ast.literal_eval(s)\n",
    "        if isinstance(obj, dict) and \"conference\" in obj:\n",
    "            return str(obj[\"conference\"]).strip()\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # 3) bare value\n",
    "    return s.strip(\"'\\\" \")\n",
    "\n",
    "# 4) read CSV and count \n",
    "conference_counter = Counter()\n",
    "total_rows = 0\n",
    "\n",
    "with open(file_path, \"r\", encoding=encoding_used, errors=\"replace\") as cf:\n",
    "    reader = csv.DictReader(cf)\n",
    "    for row in reader:\n",
    "        total_rows += 1\n",
    "        raw = row.get(\"conference_name\", \"\")\n",
    "        conf = parse_conference_field(raw)\n",
    "        if conf:\n",
    "            # your mapping\n",
    "            mapping = {\n",
    "                \"USS\": \"USENIX\", \"USENIX ATC\": \"USENIX\", \"ATC\": \"USENIX\",\n",
    "                \"FAST\":\"USENIX\", \"NSDI\":\"USENIX\", \"OSDI\":\"USENIX\",\n",
    "                \"NDSS\":\"NDSS\", \"MADWeb\":\"NDSS\",\n",
    "                \"ACSAC\":\"ACSAC\",\"AISCC\":\"ACSAC\",\n",
    "                \"TPDS\":\"SP\",\"TASLP\":\"SP\", \n",
    "                \"CCS\":\"CCS\",\"ASIA CCS\":\"CCS\"\n",
    "            }\n",
    "            conf_group = mapping.get(conf, conf)\n",
    "            conference_counter[conf_group] += 1\n",
    "        else:\n",
    "            conference_counter[\"SP\"] += 1\n",
    "\n",
    "# 5) report & plot \n",
    "print(f\"\\nProcessed rows: {total_rows}\")\n",
    "print(\"Papers per consolidated conference:\")\n",
    "for conf, cnt in conference_counter.items():\n",
    "    print(f\"  {conf:10s}: {cnt}\")\n",
    "\n",
    "# visualize\n",
    "labels = list(conference_counter.keys())\n",
    "counts = list(conference_counter.values())\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.bar(labels, counts)\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.title(\"Total Papers per Consolidated Conference\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53706e37-6b81-4037-a813-8ecfb16b36f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "import ast\n",
    "import chardet\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "file_path = \"results_merged_full_final.csv\"\n",
    "\n",
    "# 1) Detect encoding\n",
    "with open(file_path, \"rb\") as f:\n",
    "    enc = chardet.detect(f.read(100000))[\"encoding\"]\n",
    "print(\"Detected Encoding:\", enc)\n",
    "\n",
    "# 2) Helper to strip any ```json``` fences and backticks, then parse\n",
    "def clean_and_parse(raw):\n",
    "    \"\"\"\n",
    "    Strip out any ```json``` or ``` backticks, then try json.loads.\n",
    "    If that fails, try ast.literal_eval. Return dict or empty dict.\n",
    "    \"\"\"\n",
    "    if not raw:\n",
    "        return {}\n",
    "    # remove any backtick fences\n",
    "    s = raw.replace(\"```json\", \"\").replace(\"```\", \"\").strip()\n",
    "    # try JSON first\n",
    "    try:\n",
    "        obj = json.loads(s)\n",
    "        if isinstance(obj, dict):\n",
    "            return obj\n",
    "    except json.JSONDecodeError:\n",
    "        pass\n",
    "    # fallback to Python literal\n",
    "    try:\n",
    "        obj = ast.literal_eval(s)\n",
    "        if isinstance(obj, dict):\n",
    "            return obj\n",
    "    except Exception:\n",
    "        pass\n",
    "    return {}\n",
    "\n",
    "# 3) Count dataset‐related papers per year\n",
    "yearly_dataset_papers = Counter()\n",
    "\n",
    "with open(file_path, \"r\", encoding=enc, errors=\"replace\") as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    for row in reader:\n",
    "        # parse year\n",
    "        ydict = clean_and_parse(row.get(\"published_year\", \"\"))\n",
    "        year = ydict.get(\"year\")\n",
    "        # normalize to int\n",
    "        if isinstance(year, str) and year.isdigit():\n",
    "            year = int(year)\n",
    "        elif not isinstance(year, int):\n",
    "            continue  # skip if we can't get a valid year\n",
    "\n",
    "        # parse datasets\n",
    "        ddict = clean_and_parse(row.get(\"dataset_name\", \"\"))\n",
    "        datasets = ddict.get(\"datasets\")\n",
    "        if isinstance(datasets, list) and datasets:\n",
    "            yearly_dataset_papers[year] += 1\n",
    "\n",
    "# 4) Prepare & plot\n",
    "years  = sorted(yearly_dataset_papers)\n",
    "counts = [yearly_dataset_papers[y] for y in years]\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(years, counts, marker=\"o\", linestyle=\"-\")\n",
    "plt.xlabel(\"Year\")\n",
    "plt.ylabel(\"Number of Dataset-Related Papers\")\n",
    "plt.title(\"Temporal Trends in Dataset Usage (2015-2024)\")\n",
    "plt.xticks(range(min(years), max(years)+1), rotation=45)\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 5) Print the final counts\n",
    "print(\"\\nYearly Dataset Paper Counts:\")\n",
    "for y in years:\n",
    "    print(f\"{y}: {yearly_dataset_papers[y]} papers\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6514f2d-4fa7-4648-b7bf-337144bd9474",
   "metadata": {},
   "source": [
    "Dataset Summary File (paper-dataset instances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bcc8e28-0985-45ca-b3dd-404c1be06499",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import csv, json, re, ast\n",
    "\n",
    "INPUT = \"results_merged_full_final.csv\"\n",
    "OUTPUT = \"datasets_summary_updated_final_new.csv\"\n",
    "\n",
    "#  helpers\n",
    "def between_backticks(s: str) -> str:\n",
    "    \"\"\"Return content between triple backticks if present; else original.\"\"\"\n",
    "    if not isinstance(s, str):\n",
    "        return \"\"\n",
    "    m = re.search(r\"```(?:json)?\\s*(.+?)\\s*```\", s, flags=re.S)\n",
    "    return m.group(1) if m else s\n",
    "\n",
    "def parse_any(cell):\n",
    "    \"\"\"Parse JSON or Python-literal after stripping backticks.\"\"\"\n",
    "    if cell is None:\n",
    "        return None\n",
    "    s = between_backticks(str(cell)).strip()\n",
    "    # try strict JSON\n",
    "    try:\n",
    "        return json.loads(s)\n",
    "    except Exception:\n",
    "        pass\n",
    "    # try Python literal (single quotes, etc.)\n",
    "    try:\n",
    "        return ast.literal_eval(s)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def get_title(cell):\n",
    "    obj = parse_any(cell)\n",
    "    if isinstance(obj, dict) and \"title\" in obj:\n",
    "        return str(obj[\"title\"]).strip()\n",
    "    # fallback: return plain string without backticks\n",
    "    raw = between_backticks(str(cell or \"\")).strip()\n",
    "    return raw or \"N/A\"\n",
    "\n",
    "def get_year(cell):\n",
    "    obj = parse_any(cell)\n",
    "    if isinstance(obj, dict) and \"year\" in obj:\n",
    "        return str(obj[\"year\"]).strip()\n",
    "    return \"N/A\"\n",
    "\n",
    "def get_conference(cell):\n",
    "    obj = parse_any(cell)\n",
    "    if isinstance(obj, dict) and \"conference\" in obj:\n",
    "        return str(obj[\"conference\"]).strip()\n",
    "    return \"N/A\"\n",
    "\n",
    "def norm_name(s: str) -> str:\n",
    "    \"\"\"Aggressive normalization for joining dataset names across columns.\"\"\"\n",
    "    return re.sub(r\"[^a-z0-9]+\", \"\", (s or \"\").lower())\n",
    "\n",
    "def ensure_list(x):\n",
    "    \"\"\"Coerce None/non-list to empty list.\"\"\"\n",
    "    return x if isinstance(x, list) else []\n",
    "\n",
    "# main\n",
    "def process_csv_to_summary(input_csv, output_csv):\n",
    "    out_rows = []\n",
    "\n",
    "    with open(input_csv, \"r\", encoding=\"utf-8\", errors=\"replace\", newline=\"\") as f:\n",
    "        reader = csv.DictReader(f)\n",
    "\n",
    "        for row in reader:\n",
    "            title = get_title(row.get(\"title\"))\n",
    "            year  = get_year(row.get(\"published_year\"))          # correct column\n",
    "            conf  = get_conference(row.get(\"conference_name\"))   # correct column\n",
    "\n",
    "            # dataset_name -> {\"datasets\":[...]} or null\n",
    "            dn_obj   = parse_any(row.get(\"dataset_name\"))\n",
    "            datasets = ensure_list(dn_obj.get(\"datasets\") if isinstance(dn_obj, dict) else None)\n",
    "\n",
    "            # dataset_analysis_combined -> {\"dataset_analysis_combined\":[...]} or null\n",
    "            dac_obj  = parse_any(row.get(\"dataset_analysis_combined\"))\n",
    "            dac_list = ensure_list(dac_obj.get(\"dataset_analysis_combined\") if isinstance(dac_obj, dict) else None)\n",
    "\n",
    "            # dataset_categories -> {\"dataset_categories\":[...]} or null\n",
    "            cat_obj  = parse_any(row.get(\"dataset_categories\"))\n",
    "            cat_list = ensure_list(cat_obj.get(\"dataset_categories\") if isinstance(cat_obj, dict) else None)\n",
    "\n",
    "            # build lookups with normalized names\n",
    "            dac_by = {norm_name(x.get(\"dataset_name\")): x for x in dac_list if isinstance(x, dict)}\n",
    "            cat_by = {}\n",
    "            for x in cat_list:\n",
    "                if not isinstance(x, dict):\n",
    "                    continue\n",
    "                cat_by[norm_name(x.get(\"dataset_name\"))] = x\n",
    "\n",
    "            for d in datasets:\n",
    "                if not isinstance(d, dict):\n",
    "                    continue\n",
    "                raw_name = d.get(\"dataset_name\", \"N/A\")\n",
    "                key = norm_name(raw_name)\n",
    "\n",
    "                availability = labeling_type = dataset_type = \"N/A\"\n",
    "                category = subcategory = \"N/A\"\n",
    "\n",
    "                if key in dac_by:\n",
    "                    a = dac_by[key]\n",
    "                    availability  = a.get(\"availability\", \"N/A\")\n",
    "                    labeling_type = a.get(\"labeling_type\", \"N/A\")\n",
    "                    dataset_type  = a.get(\"dataset_type\", \"N/A\")\n",
    "\n",
    "                if key in cat_by:\n",
    "                    c = cat_by[key]\n",
    "                    category    = c.get(\"category\", \"N/A\")\n",
    "                    subcategory = c.get(\"subcategory\", \"N/A\")\n",
    "\n",
    "                out_rows.append({\n",
    "                    \"title\": title,\n",
    "                    \"dataset_name\": raw_name,\n",
    "                    \"contributors\": d.get(\"contributors\", \"N/A\"),\n",
    "                    \"doi\": d.get(\"doi\", \"N/A\"),\n",
    "                    \"url\": d.get(\"url\", \"N/A\"),\n",
    "                    \"availability\": availability,\n",
    "                    \"labeling_type\": labeling_type,\n",
    "                    \"dataset_type\": dataset_type,\n",
    "                    \"category\": category,\n",
    "                    \"subcategory\": subcategory,\n",
    "                    \"conference_name\": conf,\n",
    "                    \"published_year\": year,\n",
    "                })\n",
    "\n",
    "    fieldnames = [\n",
    "        \"title\", \"dataset_name\", \"contributors\", \"doi\", \"url\",\n",
    "        \"availability\", \"labeling_type\", \"dataset_type\",\n",
    "        \"category\", \"subcategory\", \"conference_name\", \"published_year\"\n",
    "    ]\n",
    "    with open(output_csv, \"w\", encoding=\"utf-8\", newline=\"\") as out:\n",
    "        w = csv.DictWriter(out, fieldnames=fieldnames)\n",
    "        w.writeheader()\n",
    "        w.writerows(out_rows)\n",
    "\n",
    "    print(f\"Processed data saved to {output_csv}\")\n",
    "\n",
    "# Run\n",
    "if __name__ == \"__main__\":\n",
    "    process_csv_to_summary(INPUT, OUTPUT)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7840b2f-3e07-4743-83be-2d276b1b002a",
   "metadata": {},
   "source": [
    "Frequently Used Datasets List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2cfcdcd-31ac-498b-8279-829d45348f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Function to normalize dataset names\n",
    "def normalize_dataset_name(name):\n",
    "    if not name or name == \"N/A\":\n",
    "        return None\n",
    "    name = name.strip().lower().replace(\"-\", \"\").replace(\"_\", \"\")\n",
    "    if \"cifar10\" in name or \"cifar100\" in name:\n",
    "        return \"cifar10 and cifar100\"  # Group CIFAR-10 and CIFAR-100 as one\n",
    "    if \"enron\" in name or \"enron email dataset\" in name:\n",
    "        return \"enron email dataset\"\n",
    "    return name\n",
    "\n",
    "# Function to extract public datasets from CSV\n",
    "def extract_public_datasets(file_path, output_csv):\n",
    "    public_datasets = Counter()\n",
    "\n",
    "    # Read the CSV file\n",
    "    with open(file_path, 'r', encoding='utf-8', errors='replace') as file:\n",
    "        reader = csv.DictReader(file)\n",
    "        \n",
    "        for row in reader:\n",
    "            dataset_name = row.get(\"dataset_name\", \"\").strip()\n",
    "            availability = row.get(\"availability\", \"\").strip().lower()\n",
    "\n",
    "            if dataset_name and availability == \"public\":\n",
    "                normalized_name = normalize_dataset_name(dataset_name)\n",
    "                if normalized_name:\n",
    "                    public_datasets[normalized_name] += 1\n",
    "\n",
    "    # Write to CSV\n",
    "    with open(output_csv, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow([\"dataset_name\", \"frequency\"])\n",
    "        for dataset_name, frequency in public_datasets.most_common(15):\n",
    "            writer.writerow([dataset_name, frequency])\n",
    "\n",
    "    print(f\"Top public datasets saved to {output_csv}\\n\")\n",
    "    \n",
    "    # Print the top public datasets and their frequencies\n",
    "    print(\"Top Public Datasets by Frequency:\")\n",
    "    for dataset_name, frequency in public_datasets.most_common(30):\n",
    "        print(f\"{dataset_name}: {frequency}\")\n",
    "\n",
    "    return public_datasets\n",
    "\n",
    "# Function to plot the datasets safely\n",
    "def plot_top_datasets(dataset_counts, output_png):\n",
    "    # Get the top 30 datasets\n",
    "    sorted_datasets = dataset_counts.most_common(35)\n",
    "\n",
    "    # Prevent plotting empty data\n",
    "    if not sorted_datasets:\n",
    "        print(\"No public datasets found. Skipping plot generation.\")\n",
    "        return\n",
    "\n",
    "    dataset_names = [item[0] for item in sorted_datasets]\n",
    "    frequencies = [item[1] for item in sorted_datasets]\n",
    "\n",
    "    # Set Seaborn style for better visuals\n",
    "    sns.set_theme(style=\"whitegrid\")\n",
    "    plt.figure(figsize=(14, 8))\n",
    "\n",
    "    # Create horizontal bar plot\n",
    "    bars = sns.barplot(\n",
    "        x=frequencies,\n",
    "        y=dataset_names,\n",
    "        palette=\"Blues_d\",\n",
    "        orient=\"h\",\n",
    "    )\n",
    "\n",
    "    # Annotate frequencies on the bars\n",
    "    for index, bar in enumerate(bars.patches):\n",
    "        plt.text(\n",
    "            bar.get_width() + 1,  # Position to the right of the bar\n",
    "            bar.get_y() + bar.get_height() / 2,  # Centered vertically\n",
    "            f\"{frequencies[index]}\",\n",
    "            fontsize=10,\n",
    "            va=\"center\",\n",
    "            ha=\"left\",\n",
    "        )\n",
    "\n",
    "    # Add titles and labels\n",
    "    plt.xlabel(\"Frequency\", fontsize=12)\n",
    "    plt.ylabel(\"Dataset Name\", fontsize=12)\n",
    "    plt.title(\"Top 30 Public Datasets by Frequency\", fontsize=16, fontweight=\"bold\")\n",
    "    plt.xticks(fontsize=10)\n",
    "    plt.yticks(fontsize=10)\n",
    "\n",
    "    # Adjust layout for better spacing\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_png, dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "# Main script\n",
    "input_csv = \"datasets_summary_updated_final_new.csv\"\n",
    "output_csv = \"frequently_used_dataset_list.csv\"\n",
    "output_png = \"top_public_datasets.png\"\n",
    "\n",
    "# Extract and plot datasets\n",
    "public_datasets = extract_public_datasets(input_csv, output_csv)\n",
    "plot_top_datasets(public_datasets, output_png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a899dabe-d880-4ef1-9c4a-b56476e6ca28",
   "metadata": {},
   "source": [
    "Research Domains Trends-Descending Order(Top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1041a0a3-4d3e-4a92-8142-cec738317bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Load the CSV file\n",
    "file_path = \"results_merged_full_final.csv\"\n",
    "\n",
    "# Try utf-8 first, fallback to latin1 if needed\n",
    "try:\n",
    "    df = pd.read_csv(file_path, encoding=\"utf-8\")\n",
    "except UnicodeDecodeError:\n",
    "    df = pd.read_csv(file_path, encoding=\"latin1\")\n",
    "\n",
    "# Parse list of domain JSONs from each cell\n",
    "def parse_domain_list(cell):\n",
    "    if pd.isna(cell) or not isinstance(cell, str):\n",
    "        return []\n",
    "    \n",
    "    try:\n",
    "        cleaned = cell.strip(\"```json\").strip(\"```\").strip()\n",
    "        domain_list = json.loads(cleaned)\n",
    "        \n",
    "        # Ensure it's a list of dicts\n",
    "        if isinstance(domain_list, list):\n",
    "            return [\n",
    "                {\n",
    "                    \"high_level_domain\": d.get(\"high_level_domain\", \"\").strip(),\n",
    "                    \"subdomain\": d.get(\"subdomain\", \"\").strip()\n",
    "                }\n",
    "                for d in domain_list\n",
    "                if d.get(\"high_level_domain\") and d.get(\"subdomain\")\n",
    "            ]\n",
    "        else:\n",
    "            return []\n",
    "    except Exception:\n",
    "        return []\n",
    "\n",
    "# Apply to the domain column\n",
    "df[\"domain_parsed\"] = df[\"domain\"].apply(parse_domain_list)\n",
    "\n",
    "# Explode the parsed domain list into separate rows\n",
    "domain_df = df.explode(\"domain_parsed\").dropna(subset=[\"domain_parsed\"])\n",
    "\n",
    "# Extract high_level_domain and subdomain into separate columns\n",
    "domain_df[\"high_level_domain\"] = domain_df[\"domain_parsed\"].apply(lambda x: x[\"high_level_domain\"])\n",
    "domain_df[\"sub_domain\"] = domain_df[\"domain_parsed\"].apply(lambda x: x[\"subdomain\"])\n",
    "\n",
    "# Drop the helper column\n",
    "domain_df = domain_df.drop(columns=[\"domain_parsed\"])\n",
    "\n",
    "# Count total occurrences of each high-level domain\n",
    "high_level_counts = domain_df[\"high_level_domain\"].value_counts().reset_index()\n",
    "high_level_counts.columns = [\"high_level_domain\", \"total_count\"]\n",
    "\n",
    "# Calculate percentage of each high-level domain\n",
    "total_entries = high_level_counts[\"total_count\"].sum()\n",
    "high_level_counts[\"percentage\"] = (high_level_counts[\"total_count\"] / total_entries) * 100\n",
    "\n",
    "# Count subdomain frequencies under each high-level domain\n",
    "subdomain_counts = domain_df.groupby([\"high_level_domain\", \"sub_domain\"]).size().reset_index(name=\"count\")\n",
    "\n",
    "# Merge subdomain count with high-level totals\n",
    "subdomain_counts = subdomain_counts.merge(high_level_counts[[\"high_level_domain\", \"total_count\"]], on=\"high_level_domain\")\n",
    "\n",
    "# Calculate subdomain percentage under each domain\n",
    "subdomain_counts[\"percentage\"] = (subdomain_counts[\"count\"] / subdomain_counts[\"total_count\"]) * 100\n",
    "\n",
    "# Save output\n",
    "output_file = \"research_domain_trend.csv\"\n",
    "subdomain_counts.to_csv(output_file, index=False)\n",
    "\n",
    "# Display summary\n",
    "print(f\" Results saved to {output_file}\\n\")\n",
    "\n",
    "print(\" Summary of High-Level Domains:\")\n",
    "print(high_level_counts)\n",
    "\n",
    "print(\"\\n Top 20 Subdomain Breakdown:\")\n",
    "print(subdomain_counts.head(20))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ba6e48-8107-4292-a48c-e2585dfd1a8c",
   "metadata": {},
   "source": [
    "Research subdomain and Subcategory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eda716b-12a3-4d18-a3c7-1d3d0af23c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import json\n",
    "\n",
    "def load_csv(file_path):\n",
    "    \"\"\"Load data from a CSV file into a pandas DataFrame.\"\"\"\n",
    "    return pd.read_csv(file_path, encoding=\"latin1\")\n",
    "\n",
    "def parse_json_field(field):\n",
    "    \"\"\"Parse JSON-like fields, handling inconsistencies.\"\"\"\n",
    "    if pd.isna(field) or not isinstance(field, str):\n",
    "        return None\n",
    "    try:\n",
    "        field_cleaned = field.strip(\"```json\").strip(\"```\").strip()\n",
    "        return json.loads(field_cleaned)\n",
    "    except json.JSONDecodeError:\n",
    "        return None\n",
    "\n",
    "def extract_subdomain_subcategory_counts(df):\n",
    "    \"\"\"Extract and count subcategories for each research subdomain along with the published year.\"\"\"\n",
    "    subdomain_subcategory_counts = defaultdict(lambda: defaultdict(lambda: defaultdict(int)))\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        # Extract and clean year\n",
    "        year_data = parse_json_field(row.get(\"published_year\", \"\"))\n",
    "        year = year_data.get(\"year\") if isinstance(year_data, dict) else None\n",
    "        if isinstance(year, str) and year.isdigit():\n",
    "            year = int(year)\n",
    "        if not isinstance(year, int):\n",
    "            continue\n",
    "\n",
    "        # Extract subdomains from the domain column (which is a list of dicts)\n",
    "        domain_data = parse_json_field(row.get(\"domain\", \"\"))\n",
    "        subdomains = []\n",
    "        if isinstance(domain_data, list):\n",
    "            for item in domain_data:\n",
    "                if isinstance(item, dict):\n",
    "                    sub = item.get(\"subdomain\")\n",
    "                    if sub and isinstance(sub, str):\n",
    "                        subdomains.append(sub.strip().lower())\n",
    "\n",
    "        # Extract dataset subcategories\n",
    "        dataset_data = parse_json_field(row.get(\"dataset_categories\", \"\"))\n",
    "        dataset_entries = dataset_data.get(\"dataset_categories\", []) if isinstance(dataset_data, dict) else []\n",
    "        if not isinstance(dataset_entries, list):\n",
    "            dataset_entries = []\n",
    "\n",
    "        for subdomain in subdomains:\n",
    "            for entry in dataset_entries:\n",
    "                if not isinstance(entry, dict):\n",
    "                    continue\n",
    "                subcat = entry.get(\"subcategory\", \"Unknown\")\n",
    "                if not subcat or subcat in [\"null\", \"nan\"]:\n",
    "                    continue\n",
    "                subcat = str(subcat).strip().lower()\n",
    "                subdomain_subcategory_counts[subdomain][year][subcat] += 1\n",
    "\n",
    "    return subdomain_subcategory_counts\n",
    "\n",
    "def generate_dataframe(subdomain_subcategory_counts):\n",
    "    \"\"\"Convert the nested dictionary to a pandas DataFrame.\"\"\"\n",
    "    rows = []\n",
    "    for subdomain, years in subdomain_subcategory_counts.items():\n",
    "        for year, subcategories in years.items():\n",
    "            for subcat, count in subcategories.items():\n",
    "                rows.append({\n",
    "                    \"Research_Subdomain\": subdomain,\n",
    "                    \"Year\": year,\n",
    "                    \"Subcategory\": subcat,\n",
    "                    \"Count\": count\n",
    "                })\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "\n",
    "    # Get top 10 subdomains by total dataset usage\n",
    "    top_subdomains = df.groupby(\"Research_Subdomain\")[\"Count\"].sum().nlargest(10).index\n",
    "    df_filtered = df[df[\"Research_Subdomain\"].isin(top_subdomains)]\n",
    "\n",
    "    print(\"\\n Top 10 Research Subdomain Trends Preview:\")\n",
    "    print(df_filtered.head())\n",
    "\n",
    "    return df_filtered\n",
    "\n",
    "# 🔹 Main Execution\n",
    "file_path = \"results_merged_full_final.csv\"  #  Corrected input file\n",
    "df = load_csv(file_path)\n",
    "\n",
    "counts = extract_subdomain_subcategory_counts(df)\n",
    "df_final = generate_dataframe(counts)\n",
    "\n",
    "# Save results\n",
    "output_csv = \"top_10_subdomain_subcategory_trends_new.csv\"\n",
    "df_final.to_csv(output_csv, index=False)\n",
    "print(f\"\\n Results saved to: {output_csv}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38ff6967-9cd0-40fa-9c6e-aa3f29853425",
   "metadata": {},
   "source": [
    "Most Used Dataset Subcategory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff308678-1525-4b45-b4c7-f681d22c0fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import json\n",
    "\n",
    "def load_csv(file_path):\n",
    "    \"\"\"Load data from a CSV file into a pandas DataFrame.\"\"\"\n",
    "    return pd.read_csv(file_path, encoding=\"latin1\")\n",
    "\n",
    "def parse_json_field(field):\n",
    "    \"\"\"Parse JSON-like fields, handling inconsistencies.\"\"\"\n",
    "    if pd.isna(field) or not isinstance(field, str):\n",
    "        return None\n",
    "    try:\n",
    "        field_cleaned = field.strip(\"```json\").strip(\"```\").strip()\n",
    "        return json.loads(field_cleaned)\n",
    "    except json.JSONDecodeError:\n",
    "        return None\n",
    "\n",
    "def extract_subdomain_subcategory_counts(df):\n",
    "    \"\"\"Extract and count subcategories for each research subdomain along with the published year.\"\"\"\n",
    "    subdomain_subcategory_counts = defaultdict(lambda: defaultdict(lambda: defaultdict(int)))\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        # Extract and clean year\n",
    "        year_data = parse_json_field(row.get(\"published_year\", \"\"))\n",
    "        year = year_data.get(\"year\") if isinstance(year_data, dict) else None\n",
    "        if isinstance(year, str) and year.isdigit():\n",
    "            year = int(year)\n",
    "        if not isinstance(year, int):\n",
    "            continue\n",
    "\n",
    "        # Extract subdomains from the domain column (which is a list of dicts)\n",
    "        domain_data = parse_json_field(row.get(\"domain\", \"\"))\n",
    "        subdomains = []\n",
    "        if isinstance(domain_data, list):\n",
    "            for item in domain_data:\n",
    "                if isinstance(item, dict):\n",
    "                    sub = item.get(\"subdomain\")\n",
    "                    if sub and isinstance(sub, str):\n",
    "                        subdomains.append(sub.strip().lower())\n",
    "\n",
    "        # Extract dataset subcategories\n",
    "        dataset_data = parse_json_field(row.get(\"dataset_categories\", \"\"))\n",
    "        dataset_entries = dataset_data.get(\"dataset_categories\", []) if isinstance(dataset_data, dict) else []\n",
    "        if not isinstance(dataset_entries, list):\n",
    "            dataset_entries = []\n",
    "\n",
    "        for subdomain in subdomains:\n",
    "            for entry in dataset_entries:\n",
    "                if not isinstance(entry, dict):\n",
    "                    continue\n",
    "                subcat = entry.get(\"subcategory\", \"Unknown\")\n",
    "                if not subcat or subcat in [\"null\", \"nan\"]:\n",
    "                    continue\n",
    "                subcat = str(subcat).strip().lower()\n",
    "                subdomain_subcategory_counts[subdomain][year][subcat] += 1\n",
    "\n",
    "    return subdomain_subcategory_counts\n",
    "\n",
    "def generate_dataframe(subdomain_subcategory_counts):\n",
    "    \"\"\"Convert the nested dictionary to a pandas DataFrame.\"\"\"\n",
    "    rows = []\n",
    "    for subdomain, years in subdomain_subcategory_counts.items():\n",
    "        for year, subcategories in years.items():\n",
    "            for subcat, count in subcategories.items():\n",
    "                rows.append({\n",
    "                    \"Research_Subdomain\": subdomain,\n",
    "                    \"Year\": year,\n",
    "                    \"Subcategory\": subcat,\n",
    "                    \"Count\": count\n",
    "                })\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "\n",
    "    # Get top 10 subdomains by total dataset usage\n",
    "    top_subdomains = df.groupby(\"Research_Subdomain\")[\"Count\"].sum().nlargest(10).index\n",
    "    df_filtered = df[df[\"Research_Subdomain\"].isin(top_subdomains)]\n",
    "\n",
    "    print(\"\\n Top 10 Research Subdomain Trends Preview:\")\n",
    "    print(df_filtered.head())\n",
    "\n",
    "    return df_filtered\n",
    "\n",
    "#  Main Execution\n",
    "file_path = \"results_merged_full_final.csv\"  #  Correct input file\n",
    "df = load_csv(file_path)\n",
    "\n",
    "counts = extract_subdomain_subcategory_counts(df)\n",
    "df_final = generate_dataframe(counts)\n",
    "\n",
    "# 🔹 Save the final results\n",
    "output_csv = \"top_10_subdomain_subcategory_trends.csv\"  # Final output file\n",
    "df_final.to_csv(output_csv, index=False)\n",
    "print(f\"\\n Results saved to: {output_csv}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d6704bc-295f-490a-91ce-6194b5d9881c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Normalize and map similar subcategory names\n",
    "def normalize_subcategory(subcategory):\n",
    "    mapping = {\n",
    "        \"user activities\": \"user_activities\",\n",
    "        \"user_activities\": \"user_activities\",\n",
    "        \"audit_reports\": \"audio_datasets\",\n",
    "        \"audio_visual_datasets\": \"audio_datasets\",\n",
    "        \"logs\": \"alerts\",\n",
    "        \"audio-visual_datasets\": \"audio_datasets\",\n",
    "        \"cybercrime_infrastructure\": \"cybercrime_infrastructures\"\n",
    "    }\n",
    "    return mapping.get(subcategory.strip().lower(), subcategory.strip().lower())\n",
    "\n",
    "# Normalize and map similar subdomain names\n",
    "def normalize_subdomain(subdomain):\n",
    "    mapping = {\n",
    "        \"software and application security\": \"SAS\",\n",
    "        \"human and societal aspects of security and privacy\": \"HSASP\",\n",
    "        \"intrusion/anomaly detection and malware mitigation\": \"IDMM\",\n",
    "        \"network security\": \"NS\",\n",
    "        \"machine learning\": \"ML\",\n",
    "        \"formal methods and theory of security\": \"FMTOS\",\n",
    "        \"systems security\": \"SS\",\n",
    "        \"privacy-preserving protocols\": \"PPP\",\n",
    "        \"data management systems\": \"DMS\",\n",
    "        \"cryptography\": \"CR\"    \n",
    "    }\n",
    "    return mapping.get(subdomain.strip().lower(), subdomain.strip().lower())\n",
    "\n",
    "# Load and prepare the dataset\n",
    "def load_csv(file_path):\n",
    "    return pd.read_csv(file_path, encoding=\"latin1\")\n",
    "\n",
    "def clean_and_normalize(df):\n",
    "    df[\"Subcategory\"] = df[\"Subcategory\"].astype(str).apply(normalize_subcategory)\n",
    "    df[\"Research Subdomain\"] = df[\"Research_Subdomain\"].astype(str).apply(normalize_subdomain)\n",
    "\n",
    "    df[\"Year\"] = pd.to_numeric(df[\"Year\"], errors=\"coerce\")\n",
    "    df = df.dropna(subset=[\"Year\"])\n",
    "    df[\"Year\"] = df[\"Year\"].astype(int)\n",
    "\n",
    "    top_subdomains = df.groupby(\"Research Subdomain\")[\"Count\"].sum().nlargest(10).index\n",
    "    df = df[df[\"Research Subdomain\"].isin(top_subdomains)]\n",
    "\n",
    "    df.to_csv(\"cleaned_top_10_research_subdomains.csv\", index=False)\n",
    "    print(\" Saved cleaned data: cleaned_top_10_research_subdomains.csv\")\n",
    "\n",
    "    # Convert Year to integer, remove invalid and 2024 entries\n",
    "    df[\"Year\"] = pd.to_numeric(df[\"Year\"], errors=\"coerce\")\n",
    "    df = df[df[\"Year\"] != 2024]\n",
    "    df = df.dropna(subset=[\"Year\"])  # Drop rows with invalid years\n",
    "    df[\"Year\"] = df[\"Year\"].astype(int)\n",
    "\n",
    "    return df\n",
    "\n",
    "# Top 10 Subcategories Per Year\n",
    "def analyze_top_10_subcategories_per_year(df):\n",
    "    top_subcategories_per_year = (\n",
    "        df.groupby([\"Year\", \"Subcategory\"])[\"Count\"]\n",
    "        .sum().reset_index()\n",
    "        .sort_values([\"Year\", \"Count\"], ascending=[True, False])\n",
    "    )\n",
    "\n",
    "    top_10_per_year = top_subcategories_per_year.groupby(\"Year\").head(10).reset_index(drop=True)\n",
    "    top_10_per_year.to_csv(\"top_10_subcategories_per_year.csv\", index=False)\n",
    "    print(\" Saved: top_10_subcategories_per_year.csv\")\n",
    "\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    sns.barplot(\n",
    "        data=top_10_per_year,\n",
    "        x=\"Year\", y=\"Count\", hue=\"Subcategory\",\n",
    "        dodge=True, palette=\"tab10\"\n",
    "    )\n",
    "    plt.title(\"Top 10 Subcategories Used in Each Year\")\n",
    "    plt.xlabel(\"Year\")\n",
    "    plt.ylabel(\"Dataset Count\")\n",
    "    plt.legend(title=\"Subcategory\", bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n",
    "    plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.6)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"top_10_subcategories_per_year.png\", dpi=300, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "\n",
    "# Emerging & Declining Subcategories\n",
    "def analyze_emerging_declining_subcategories(df):\n",
    "    subcategory_trends = df.groupby([\"Year\", \"Subcategory\"])[\"Count\"].sum().reset_index()\n",
    "    subcategory_trends.to_csv(\"subcategory_trends_over_time.csv\", index=False)\n",
    "    print(\" Saved: subcategory_trends_over_time.csv\")\n",
    "\n",
    "    pivot_table = subcategory_trends.pivot(index=\"Year\", columns=\"Subcategory\", values=\"Count\").fillna(0)\n",
    "\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    pivot_table.plot(kind=\"line\", figsize=(14, 8), marker='o')\n",
    "    plt.title(\"Emerging & Declining Dataset Subcategories Over Time\")\n",
    "    plt.xlabel(\"Year\")\n",
    "    plt.ylabel(\"Dataset Count\")\n",
    "    plt.legend(title=\"Subcategory\", bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n",
    "    plt.grid()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"emerging_declining_subcategories.png\", dpi=300, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "\n",
    "# Subdomain vs Subcategory Correlation\n",
    "def analyze_subcategory_subdomain_correlation(df):\n",
    "    correlation_df = df.groupby([\"Research Subdomain\", \"Subcategory\"])[\"Count\"].sum().reset_index()\n",
    "    correlation_df.to_csv(\"subcategory_subdomain_correlation.csv\", index=False)\n",
    "    print(\" Saved: subcategory_subdomain_correlation.csv\")\n",
    "\n",
    "    pivot = correlation_df.pivot(index=\"Research Subdomain\", columns=\"Subcategory\", values=\"Count\").fillna(0)\n",
    "\n",
    "    plt.figure(figsize=(14, 10))\n",
    "    sns.heatmap(pivot, annot=False, cmap=\"coolwarm\", linewidths=0.5, linecolor=\"gray\")\n",
    "    plt.title(\"Subdomain vs Dataset Subcategory Correlation\")\n",
    "    plt.xlabel(\"Subcategory\")\n",
    "    plt.ylabel(\"Research Subdomain\")\n",
    "    plt.xticks(rotation=45, ha=\"right\")\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"subcategory_subdomain_correlation.png\", dpi=300, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "\n",
    "# Main Execution\n",
    "file_path = \"top_10_subdomain_subcategory_trends.csv\"\n",
    "df = load_csv(file_path)\n",
    "df = clean_and_normalize(df)\n",
    "\n",
    "analyze_top_10_subcategories_per_year(df)\n",
    "analyze_emerging_declining_subcategories(df)\n",
    "analyze_subcategory_subdomain_correlation(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e28bcbb6-b402-4dfa-800f-f6762c517dce",
   "metadata": {},
   "source": [
    "UNIQUE DATASETs- REMOVING REPEATATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae3fc5a-9bc7-42f1-a785-25325ff5ffdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "\n",
    "def normalize_dataset_name(name):\n",
    "    \"\"\"Normalize dataset names to standardize variations (case-insensitive, no spaces, no special characters).\"\"\"\n",
    "    name = str(name).strip().lower()\n",
    "    name = name.replace(\"-\", \"\").replace(\"_\", \"\").replace(\" \", \"\")\n",
    "\n",
    "    # Common dataset name variations \n",
    "    replacements = {\n",
    "        \"imdbdataset\": \"imdb\",\n",
    "        \"imdbmoviereviews\": \"imdb\"\n",
    "    }\n",
    "\n",
    "    return replacements.get(name, name)\n",
    "\n",
    "\n",
    "def extract_unique_datasets(file_path, output_csv, output_json):\n",
    "    \"\"\"Extract unique datasets while allowing multiple entries if they have different category/subcategory.\"\"\"\n",
    "    \n",
    "    # Load CSV file\n",
    "    df = pd.read_csv(file_path, encoding=\"utf-8\")\n",
    "\n",
    "    # Normalize dataset names\n",
    "    df[\"dataset_name_normalized\"] = df[\"dataset_name\"].apply(lambda x: normalize_dataset_name(str(x)))\n",
    "\n",
    "    # Group by normalized dataset name while keeping category/subcategory distinct\n",
    "    unique_datasets = df.groupby([\"dataset_name_normalized\", \"category\", \"subcategory\"]).agg({\n",
    "        \"dataset_name\": \"first\",  # Keep the first original dataset name\n",
    "        \"availability\": \"first\",\n",
    "        \"labeling_type\": \"first\",\n",
    "        \"dataset_type\": \"first\"\n",
    "    }).reset_index()\n",
    "\n",
    "    # Drop the extra column used for normalization\n",
    "    unique_datasets.drop(columns=[\"dataset_name_normalized\"], inplace=True)\n",
    "\n",
    "    # Save to CSV\n",
    "    unique_datasets.to_csv(output_csv, index=False, encoding=\"utf-8\")\n",
    "\n",
    "    # Save to JSON\n",
    "    unique_datasets.to_json(output_json, orient=\"records\", indent=4)\n",
    "\n",
    "    print(f\" Unique dataset list saved to: {output_csv} and {output_json}\")\n",
    "    return unique_datasets\n",
    "\n",
    "\n",
    "# Run the Extraction\n",
    "file_path = \"datasets_summary_updated_final_new.csv\"  # Your updated CSV input file\n",
    "output_csv = \"unique_datasets_updated_new_last_final.csv\"  # Output file for unique datasets (CSV)\n",
    "output_json = \"unique_datasets_updated_new_last_final.json\"  # Output file for unique datasets (JSON)\n",
    "\n",
    "# Extract unique datasets\n",
    "unique_datasets_df = extract_unique_datasets(file_path, output_csv, output_json)\n",
    "\n",
    "# Display a preview of the extracted datasets\n",
    "print(unique_datasets_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b684b201-06aa-4875-a1b7-45d64f6009d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define category name variations to standardize them\n",
    "category_mapping = {\n",
    "    \"attacker_related\": \"attacker_related\",\n",
    "    \"user_and_organizational_characteristics\": \"user_and_organizational_characteristics\",\n",
    "    \"macro_level_internet_characteristics\": \"macro_level_internet_characteristics\",\n",
    "    \"visual_and_multimedia_data\": \"visual_and_multimedia_data\",\n",
    "    \"defender_artifacts\": \"defender_artifacts\",\n",
    "    \"user_organizational_characteristics\": \"user_and_organizational_characteristics\",\n",
    "    \"macro_internet_characteristics\": \"macro_level_internet_characteristics\",\n",
    "    \"user & organizational characteristics\": \"user_and_organizational_characteristics\",\n",
    "    \"user_&_organizational_characteristics\": \"user_and_organizational_characteristics\",\n",
    "    \"macro-level internet characteristics\": \"macro_level_internet_characteristics\",\n",
    "}\n",
    "\n",
    "# Load the dataset\n",
    "file_path = \"datasets_summary_updated_final_new.csv\"\n",
    "df = pd.read_csv(file_path, encoding=\"latin1\")\n",
    "\n",
    "# Standardize the category column\n",
    "df['category'] = df['category'].replace(category_mapping)\n",
    "\n",
    "# Filter for only \"Custom-created datasets, not shared\", \"Custom-created datasets but public\", and \"Custom-created datasets, but restricted\"\n",
    "filtered_df = df[df['availability'].isin([\"Custom-created datasets, not shared\", \"Custom-created datasets but public\", \"Custom-created datasets, but restricted\"])]\n",
    "\n",
    "# Select only relevant columns\n",
    "filtered_df = filtered_df[['title', 'dataset_name', 'category', 'subcategory', 'availability']]\n",
    "\n",
    "# Save to CSV\n",
    "output_csv = \"custom_created_datasets_with_categories.csv\"\n",
    "filtered_df.to_csv(output_csv, index=False)\n",
    "\n",
    "# Print output confirmation\n",
    "print(f\"\\nExtracted {len(filtered_df)} rows of custom-created datasets with their categories and saved to {output_csv}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a50a41ac-79f9-4257-b1c7-5e90b117756d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "# Define category name variations to standardize them\n",
    "category_mapping = {\n",
    "    \"attacker_related\": \"attacker_related\",\n",
    "    \"user_and_organizational_characteristics\": \"user_and_organizational_characteristics\",\n",
    "    \"macro_level_internet_characteristics\": \"macro_level_internet_characteristics\",\n",
    "    \"visual_and_multimedia_data\": \"visual_and_multimedia_data\",\n",
    "    \"defender_artifacts\": \"defender_artifacts\",\n",
    "    \"user_organizational_characteristics\": \"user_and_organizational_characteristics\",\n",
    "    \"macro_internet_characteristics\": \"macro_level_internet_characteristics\",\n",
    "    \"user & organizational characteristics\": \"user_and_organizational_characteristics\",\n",
    "    \"user_&_organizational_characteristics\": \"user_and_organizational_characteristics\",\n",
    "    \"macro-level internet characteristics\": \"macro_level_internet_characteristics\",\n",
    "    \"Alerts\":\"alerts\",\n",
    "}\n",
    "\n",
    "# Load the dataset\n",
    "file_path = \"custom_created_datasets_with_categories.csv\"\n",
    "df = pd.read_csv(file_path, encoding=\"latin1\")\n",
    "\n",
    "# Standardize the category column\n",
    "df['category'] = df['category'].replace(category_mapping)\n",
    "\n",
    "# Filter for only \"Custom-created datasets, not shared\" and \"Custom-created datasets but public\"\n",
    "filtered_df = df[df['availability'].isin([\"Custom-created datasets, not shared\", \"Custom-created datasets but public\", \"Custom-created datasets, but restricted\"])]\n",
    "\n",
    "# Count occurrences of subcategories\n",
    "subcategory_counts = Counter(filtered_df['subcategory'].dropna())\n",
    "\n",
    "# Convert to DataFrame for saving\n",
    "subcategory_df = pd.DataFrame(subcategory_counts.items(), columns=['Subcategory', 'Count'])\n",
    "\n",
    "# Sort by count in descending order\n",
    "subcategory_df = subcategory_df.sort_values(by=\"Count\", ascending=False)\n",
    "\n",
    "# Save results to CSV\n",
    "output_csv = \"most_common_subcategories_custom_datasets.csv\"\n",
    "subcategory_df.to_csv(output_csv, index=False)\n",
    "\n",
    "# Display top subcategories\n",
    "print(\"\\nMost Common Subcategories for Custom-Created Datasets:\")\n",
    "print(subcategory_df.head(20))  # Show top 10 subcategories\n",
    "print(f\"\\nResults saved to {output_csv}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be7e8e13-7436-42f4-bbbc-4a4716077ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define category name variations to standardize them\n",
    "category_mapping = {\n",
    "    \"attacker_related\": \"attacker_related\",\n",
    "    \"user_and_organizational_characteristics\": \"user_and_organizational_characteristics\",\n",
    "    \"macro_level_internet_characteristics\": \"macro_level_internet_characteristics\",\n",
    "    \"visual_and_multimedia_data\": \"visual_and_multimedia_data\",\n",
    "    \"defender_artifacts\": \"defender_artifacts\",\n",
    "    \"user_organizational_characteristics\": \"user_and_organizational_characteristics\",\n",
    "    \"macro_internet_characteristics\": \"macro_level_internet_characteristics\",\n",
    "    \"Macro-Level Internet Characteristics\": \"macro_level_internet_characteristics\",\n",
    "    \"user & organizational characteristics\": \"user_and_organizational_characteristics\",\n",
    "    \"user_&_organizational_characteristics\": \"user_and_organizational_characteristics\",\n",
    "    \"macro-level internet characteristics\": \"macro_level_internet_characteristics\",\n",
    "}\n",
    "\n",
    "# Load the dataset\n",
    "file_path = \"datasets_summary_updated_final_new.csv\"\n",
    "df = pd.read_csv(file_path, encoding=\"latin1\")\n",
    "\n",
    "# Standardize the category colum\n",
    "df['category'] = df['category'].replace(category_mapping)\n",
    "\n",
    "# Standardize the labeling_type column (treat all variations of 'Not Mentioned' as one)\n",
    "df['labeling_type'] = df['labeling_type'].replace({\n",
    "    \"Not mentioned\": \"not mentioned\",\n",
    "    \"Not Mentioned\": \"not mentioned\"\n",
    "})\n",
    "\n",
    "# Filter for only \"Custom-created datasets, not shared\" and \"Custom-created datasets but public\"\n",
    "filtered_df = df[df['availability'].isin([\"Custom-created datasets, not shared\", \"Custom-created datasets but public\",\"Custom-created datasets, but restricted\"])]\n",
    "\n",
    "# Select relevant columns\n",
    "filtered_df = filtered_df[['title', 'dataset_name', 'category', 'subcategory', 'availability', 'labeling_type', 'dataset_type']]\n",
    "\n",
    "# Save to CSV\n",
    "output_csv = \"custom_created_datasets_with_categories.csv\"\n",
    "filtered_df.to_csv(output_csv, index=False)\n",
    "\n",
    "# Count breakdown\n",
    "availability_counts = filtered_df['availability'].value_counts()\n",
    "\n",
    "# Labeling type breakdown\n",
    "labeling_counts = filtered_df.groupby('availability')['labeling_type'].value_counts()\n",
    "\n",
    "# Dataset type breakdown\n",
    "dataset_type_counts = filtered_df.groupby('availability')['dataset_type'].value_counts()\n",
    "\n",
    "# Category and subcategory breakdown\n",
    "category_counts = filtered_df.groupby('availability')['category'].value_counts()\n",
    "subcategory_counts = filtered_df.groupby('availability')['subcategory'].value_counts()\n",
    "\n",
    "# Print results\n",
    "print(f\"\\nExtracted {len(filtered_df)} rows of custom-created datasets with their categories and saved to {output_csv}.\\n\")\n",
    "\n",
    "# Print availability counts\n",
    "print(\"Custom-created dataset counts:\")\n",
    "for key, value in availability_counts.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "# Print labeling type breakdown\n",
    "print(\"\\nLabeling type breakdown for custom-created datasets (Standardized):\")\n",
    "print(labeling_counts)\n",
    "\n",
    "# Print dataset type breakdown\n",
    "print(\"\\nDataset type breakdown for custom-created datasets:\")\n",
    "print(dataset_type_counts)\n",
    "\n",
    "# Print category breakdown\n",
    "print(\"\\nCategory breakdown for custom-created datasets:\")\n",
    "print(category_counts)\n",
    "\n",
    "# Print subcategory breakdown\n",
    "print(\"\\nSubcategory breakdown for custom-created datasets:\")\n",
    "print(subcategory_counts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0904c530-56ba-4ae6-945f-ac96816ad884",
   "metadata": {},
   "source": [
    "CDFs for All paper and HLDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30674462-40e3-4228-b198-b325a6f3e66d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import ast\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import chardet\n",
    "\n",
    "# File paths\n",
    "INPUT_CSV = \"results_merged_full_final.csv\"\n",
    "OUT_CSV = \"cdf_and_raw_counts.csv\"\n",
    "OUT_PDF = \"fractional_cdf_single_panel.pdf\"\n",
    "\n",
    "# Helper utilities\n",
    "_fence_re = re.compile(r\"```(?:json)?\\s*|\\s*```\")\n",
    "\n",
    "def strip_fences(s: str) -> str:\n",
    "    if not isinstance(s, str):\n",
    "        return \"\"\n",
    "    lines = s.splitlines()\n",
    "    inner = []\n",
    "    for ln in lines:\n",
    "        stripped = ln.strip()\n",
    "        if (stripped.startswith(\"```\") and stripped.endswith(\"```\")) or stripped.startswith(\"```json\"):\n",
    "            continue\n",
    "        inner.append(ln)\n",
    "    return \"\\n\".join(inner).strip()\n",
    "\n",
    "def parse_published_year(cell: str):\n",
    "    \"\"\"Try JSON, literal_eval, then first 4-digit sequence.\"\"\"\n",
    "    if cell is None or (isinstance(cell, float) and pd.isna(cell)):\n",
    "        return None\n",
    "    s = strip_fences(str(cell).strip())\n",
    "    # try JSON\n",
    "    try:\n",
    "        obj = json.loads(s)\n",
    "        if isinstance(obj, dict) and \"year\" in obj:\n",
    "            return int(obj[\"year\"])\n",
    "    except Exception:\n",
    "        pass\n",
    "    # try python literal\n",
    "    try:\n",
    "        obj = ast.literal_eval(s)\n",
    "        if isinstance(obj, dict) and \"year\" in obj:\n",
    "            return int(obj[\"year\"])\n",
    "    except Exception:\n",
    "        pass\n",
    "    # fallback: first 4-digit\n",
    "    m = re.search(r\"\\d{4}\", s)\n",
    "    if m:\n",
    "        return int(m.group(0))\n",
    "    return None\n",
    "\n",
    "def parse_domain_list(cell: str):\n",
    "    \"\"\"Parse the domain cell (json list of dicts) and return list of high_level_domain strings.\"\"\"\n",
    "    if cell is None or (isinstance(cell, float) and pd.isna(cell)):\n",
    "        return []\n",
    "    s = strip_fences(str(cell).strip())\n",
    "    try:\n",
    "        arr = json.loads(s)\n",
    "        if not isinstance(arr, list):\n",
    "            return []\n",
    "        out = []\n",
    "        for d in arr:\n",
    "            if not isinstance(d, dict):\n",
    "                continue\n",
    "            high = d.get(\"high_level_domain\", \"\").strip()\n",
    "            # normalize known mislabel\n",
    "            if high == \"Software and application security\":\n",
    "                high = \"Security and privacy\"\n",
    "            if high:\n",
    "                out.append(high)\n",
    "        return out\n",
    "    except Exception:\n",
    "        return []\n",
    "\n",
    "# 1) Read CSV (detect encoding)\n",
    "with open(INPUT_CSV, \"rb\") as f:\n",
    "    enc = chardet.detect(f.read(100000))[\"encoding\"]\n",
    "print(f\"Detected encoding: {enc}\")\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(INPUT_CSV, encoding=enc)\n",
    "except Exception:\n",
    "    print(f\"Failed to read with {enc}, retrying latin1\")\n",
    "    df = pd.read_csv(INPUT_CSV, encoding=\"latin1\")\n",
    "\n",
    "print(f\"Loaded {len(df)} rows; columns: {list(df.columns)}\")\n",
    "\n",
    "# 2) Parse year and domains\n",
    "possible_year_cols = [c for c in df.columns if \"year\" in c.lower()]\n",
    "if not possible_year_cols:\n",
    "    raise KeyError(\"No column containing 'year' found in CSV.\")\n",
    "year_col = possible_year_cols[0]\n",
    "print(f\"Using '{year_col}' as year column\")\n",
    "\n",
    "df[\"year\"] = df[year_col].apply(parse_published_year)\n",
    "missing = df[\"year\"].isna().sum()\n",
    "if missing:\n",
    "    print(f\"Warning: {missing} rows have no parsed year and will be dropped\")\n",
    "df = df.dropna(subset=[\"year\"])\n",
    "df[\"year\"] = df[\"year\"].astype(int)\n",
    "\n",
    "df[\"HLDs\"] = df[\"domain\"].apply(parse_domain_list)\n",
    "\n",
    "# 3) Top-5 HLDs overall\n",
    "all_hlds = [h for sub in df[\"HLDs\"] for h in sub]\n",
    "hld_counts = Counter(all_hlds)\n",
    "top5_hlds = [h for h, _ in hld_counts.most_common(5)]\n",
    "print(\"Top-5 HLDs:\", top5_hlds)\n",
    "\n",
    "# 4) Cumulative raw counts per year\n",
    "years = sorted(df[\"year\"].unique())\n",
    "cum_total_raw = []\n",
    "cum_counts_hlds_raw = {h: [] for h in top5_hlds}\n",
    "\n",
    "for yr in years:\n",
    "    subset = df[df[\"year\"] <= yr]\n",
    "    cum_total_raw.append(subset.shape[0])\n",
    "    for h in top5_hlds:\n",
    "        cnt = subset[subset[\"HLDs\"].apply(lambda L: h in L)].shape[0]\n",
    "        cum_counts_hlds_raw[h].append(cnt)\n",
    "\n",
    "# 5) Percentages relative to final value\n",
    "def to_percentages(raw_list):\n",
    "    final = raw_list[-1] if raw_list else 0\n",
    "    if final == 0:\n",
    "        return [0.0] * len(raw_list)\n",
    "    return [(x / final) * 100.0 for x in raw_list]\n",
    "\n",
    "cum_total_pct = to_percentages(cum_total_raw)\n",
    "cum_counts_hlds_pct = {h: to_percentages(cum_counts_hlds_raw[h]) for h in top5_hlds}\n",
    "\n",
    "# 6) Save CSV\n",
    "data = {\"Year\": years, \"Total_Raw\": cum_total_raw, \"Total_Pct\": cum_total_pct}\n",
    "for h in top5_hlds:\n",
    "    data[h.replace(\" \", \"_\") + \"_Raw\"] = cum_counts_hlds_raw[h]\n",
    "    data[h.replace(\" \", \"_\") + \"_Pct\"] = cum_counts_hlds_pct[h]\n",
    "\n",
    "cdf_df = pd.DataFrame(data)\n",
    "cdf_df.to_csv(OUT_CSV, index=False)\n",
    "print(f\"Saved CSV -> {OUT_CSV}\")\n",
    "\n",
    "# 7) Fractional CDF plot\n",
    "\n",
    "total_final = cum_total_raw[-1] if cum_total_raw else 1\n",
    "frac_total = [x / total_final for x in cum_total_raw]\n",
    "\n",
    "frac_counts_hlds = {}\n",
    "for h in top5_hlds:\n",
    "    raw = cum_counts_hlds_raw[h]\n",
    "    final = raw[-1] if raw and raw[-1] > 0 else 1\n",
    "    frac_counts_hlds[h] = [x / final for x in raw]\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(years, frac_total, marker=\"s\", linestyle=\"-\", label=\"All Papers\", linewidth=2, color=\"orange\")\n",
    "\n",
    "colors = [\"black\", \"forestgreen\", \"royalblue\", \"crimson\", \"teal\"]\n",
    "line_styles = [\"-\", \":\", \"-.\", \"--\", (0, (3,1,1,1))]\n",
    "for idx, h in enumerate(top5_hlds):\n",
    "    plt.plot(years, frac_counts_hlds[h], marker=\"o\", linestyle=line_styles[idx],\n",
    "             color=colors[idx], label=h, linewidth=1.6)\n",
    "\n",
    "plt.xlabel(\"Year\", fontsize=11)\n",
    "plt.ylabel(\"Cumulative Fraction (0–1)\", fontsize=11)\n",
    "plt.title(\"Fractional CDF (2015–2024*)\")\n",
    "plt.ylim(0, 1.02)\n",
    "plt.xlim(min(years)-0.5, max(years)+0.5)\n",
    "plt.xticks(years, rotation=45)\n",
    "plt.grid(True, linestyle=\"--\", alpha=0.4)\n",
    "plt.legend(loc=\"upper left\", fontsize=\"small\", framealpha=0.85)\n",
    "plt.text(0.5, -0.12, \"* 2024 is partial (through Sept. 2024)\", transform=plt.gca().transAxes, ha=\"center\", fontsize=8)\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUT_PDF, bbox_inches=\"tight\")\n",
    "plt.close()\n",
    "print(f\"Saved PDF -> {OUT_PDF}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f0bcda5-cb4a-45ba-8cea-65a0b0cc55aa",
   "metadata": {},
   "source": [
    "Domain Coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521f2f08-23bb-4703-a170-b035db898283",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import ast\n",
    "\n",
    "# Config / mapping\n",
    "conference_mapping = {\n",
    "    \"USS\": \"USENIX\",       \"USENIX ATC\": \"USENIX\",   \"ATC\": \"USENIX\",\n",
    "    \"FAST\": \"USENIX\",      \"NSDI\": \"USENIX\",         \"OSDI\": \"USENIX\",\n",
    "    \"NDSS\": \"NDSS\",        \"MADWeb\": \"NDSS\",\n",
    "    \"ACSAC\": \"ACSAC\",      \"AISCC\": \"ACSAC\",\n",
    "    \"SP\": \"SP\",            \"TPDS\": \"SP\",             \"TASLP\": \"SP\",\n",
    "    \"CCS\": \"CCS\",          \"ASIA CCS\": \"CCS\",        \"CCSNONE\": \"CCS\",\n",
    "}\n",
    "\n",
    "\n",
    "input_file = \"results_merged_full_final.csv\"   # adjust path if needed\n",
    "output_file1 = \"domain_coverage_by_conference_percentiles_final.csv\"\n",
    "output_file2 = \"non_sp_counts_per_conference.csv\"\n",
    "\n",
    "# Helpers\n",
    "def strip_fences(s: str) -> str:\n",
    "    if not isinstance(s, str):\n",
    "        return \"\"\n",
    "    lines = s.splitlines()\n",
    "    inner = []\n",
    "    for ln in lines:\n",
    "        stripped = ln.strip()\n",
    "        if (stripped.startswith(\"```\") and stripped.endswith(\"```\")) or stripped.startswith(\"```json\"):\n",
    "            continue\n",
    "        if stripped == \"```\":\n",
    "            continue\n",
    "        inner.append(ln)\n",
    "    return \"\\n\".join(inner).strip()\n",
    "\n",
    "def parse_conference_field(raw):\n",
    "    if raw is None or (isinstance(raw, float) and pd.isna(raw)):\n",
    "        return None\n",
    "    s = str(raw).strip()\n",
    "    if not s:\n",
    "        return None\n",
    "    s = strip_fences(s)\n",
    "    # try JSON\n",
    "    try:\n",
    "        obj = json.loads(s)\n",
    "        if isinstance(obj, dict) and \"conference\" in obj:\n",
    "            raw_conf = str(obj[\"conference\"]).strip()\n",
    "            return conference_mapping.get(raw_conf, raw_conf) if raw_conf else None\n",
    "    except Exception:\n",
    "        pass\n",
    "    # try literal eval\n",
    "    try:\n",
    "        obj = ast.literal_eval(s)\n",
    "        if isinstance(obj, dict) and \"conference\" in obj:\n",
    "            raw_conf = str(obj[\"conference\"]).strip()\n",
    "            return conference_mapping.get(raw_conf, raw_conf) if raw_conf else None\n",
    "    except Exception:\n",
    "        pass\n",
    "    bare = s.strip(\"\\\"' \")\n",
    "    return conference_mapping.get(bare, bare) if bare else None\n",
    "\n",
    "def parse_domain_list(cell):\n",
    "    if cell is None or (isinstance(cell, float) and pd.isna(cell)):\n",
    "        return []\n",
    "    s = str(cell).strip()\n",
    "    if not s:\n",
    "        return []\n",
    "    s = strip_fences(s)\n",
    "    try:\n",
    "        arr = json.loads(s)\n",
    "        if not isinstance(arr, list):\n",
    "            return []\n",
    "        out = []\n",
    "        for d in arr:\n",
    "            if not isinstance(d, dict):\n",
    "                continue\n",
    "            high = d.get(\"high_level_domain\", \"\")\n",
    "            sub  = d.get(\"subdomain\", \"\")\n",
    "            if not isinstance(high, str):\n",
    "                high = str(high) if high is not None else \"\"\n",
    "            if not isinstance(sub, str):\n",
    "                sub = str(sub) if sub is not None else \"\"\n",
    "            high = high.strip()\n",
    "            sub  = sub.strip().lower()\n",
    "            # fix known mislabel\n",
    "            if high == \"Software and application security\" or sub == \"software and application security\":\n",
    "                high = \"Security and privacy\"\n",
    "            if high:\n",
    "                out.append(high)\n",
    "        return out\n",
    "    except Exception:\n",
    "        try:\n",
    "            arr = ast.literal_eval(s)\n",
    "            if isinstance(arr, list):\n",
    "                out = []\n",
    "                for d in arr:\n",
    "                    if isinstance(d, dict):\n",
    "                        high = d.get(\"high_level_domain\", \"\")\n",
    "                        if isinstance(high, str) and high.strip():\n",
    "                            out.append(high.strip())\n",
    "                return out\n",
    "        except Exception:\n",
    "            return []\n",
    "    return []\n",
    "\n",
    "# Main\n",
    "# 1) load\n",
    "try:\n",
    "    df = pd.read_csv(input_file, encoding=\"utf-8\")\n",
    "except UnicodeDecodeError:\n",
    "    df = pd.read_csv(input_file, encoding=\"latin1\")\n",
    "\n",
    "print(f\"Initially loaded {len(df)} rows from {input_file}\")\n",
    "\n",
    "# 2) parse conference (conservative, do not drop rows)\n",
    "df[\"ConferenceRaw\"] = df[\"conference_name\"].apply(parse_conference_field)\n",
    "print(f\"After parsing Conference, # non-null = {df['ConferenceRaw'].notna().sum()} (should be {len(df)})\")\n",
    "\n",
    "# 3) parse domains\n",
    "df[\"All_HLDs\"] = df[\"domain\"].apply(parse_domain_list)\n",
    "\n",
    "# 4) deduplicate per paper (keep insertion order -> unique)\n",
    "df[\"Unique_HLDs\"] = df[\"All_HLDs\"].apply(lambda L: list(dict.fromkeys(L)) if isinstance(L, list) else [])\n",
    "\n",
    "# 5) num distinct HLDs per paper\n",
    "df[\"num_hlds\"] = df[\"Unique_HLDs\"].apply(len)\n",
    "print(f\"After parsing domains, total rows = {len(df)} (should be {len(df)})\")\n",
    "\n",
    "# 6) explode to (paper, one HLD) rows\n",
    "exploded = df.explode(\"Unique_HLDs\").rename(columns={\"Unique_HLDs\": \"HLD\"})\n",
    "\n",
    "# 7) pivot: # papers with that HLD (unique HLD counts)\n",
    "domain_counts = (\n",
    "    exploded\n",
    "    .dropna(subset=[\"HLD\"])\n",
    "    .groupby([\"ConferenceRaw\", \"HLD\"])\n",
    "    .size()\n",
    "    .unstack(fill_value=0)\n",
    ")\n",
    "\n",
    "# ensure conferences that exist in parsed ConferenceRaw but not in domain_counts are present\n",
    "paper_counts_series = df[\"ConferenceRaw\"].value_counts().sort_index()\n",
    "for conf in paper_counts_series.index:\n",
    "    if conf not in domain_counts.index:\n",
    "        domain_counts.loc[conf] = 0\n",
    "domain_counts = domain_counts.reindex(sorted(domain_counts.index))\n",
    "\n",
    "# 8) total number of (paper->HLD) tag assignments per conference\n",
    "hld_tag_counts = exploded.dropna(subset=[\"HLD\"]).groupby(\"ConferenceRaw\").size()\n",
    "\n",
    "# 9) Papers column (exact paper counts from parsed ConferenceRaw), AvgDom column (Avg #distinct HLDs per paper)\n",
    "paper_counts_aligned = paper_counts_series.reindex(domain_counts.index).fillna(0).astype(int)\n",
    "domain_counts[\"Papers\"] = paper_counts_aligned\n",
    "avg_domains = (hld_tag_counts.reindex(domain_counts.index).fillna(0) / paper_counts_aligned.replace(0, np.nan)).round(2)\n",
    "avg_domains = avg_domains.fillna(0.0)\n",
    "domain_counts[\"AvgDom\"] = avg_domains\n",
    "\n",
    "# 10) compute percentiles P50 and P90 per conference (on num_hlds distribution)\n",
    "percentiles = []\n",
    "for conf in sorted(paper_counts_series.index.tolist()):\n",
    "    arr = df.loc[df[\"ConferenceRaw\"] == conf, \"num_hlds\"].to_numpy()\n",
    "    if arr.size == 0:\n",
    "        p50, p90 = np.nan, np.nan\n",
    "    else:\n",
    "        p50 = float(np.percentile(arr, 50))\n",
    "        p90 = float(np.percentile(arr, 90))\n",
    "    percentiles.append({\"Conference\": conf, \"P50\": round(p50,2) if not np.isnan(p50) else np.nan, \"P90\": round(p90,2) if not np.isnan(p90) else np.nan})\n",
    "\n",
    "pct_df = pd.DataFrame(percentiles).set_index(\"Conference\")\n",
    "pct_df = pct_df.reindex(domain_counts.index)\n",
    "\n",
    "# 11) merge percentiles into domain_counts\n",
    "out = domain_counts.copy()\n",
    "out[\"P50\"] = pct_df[\"P50\"]\n",
    "out[\"P90\"] = pct_df[\"P90\"]\n",
    "out[\"Papers\"] = out[\"Papers\"].astype(int)\n",
    "\n",
    "# 12) Prepare final ordering\n",
    "out_reset = out.reset_index().rename(columns={\"ConferenceRaw\": \"Conference\", \"index\": \"Conference\"})\n",
    "extras = {\"Conference\", \"Papers\", \"AvgDom\", \"P50\", \"P90\"}\n",
    "hld_cols = [c for c in out_reset.columns if c not in extras]\n",
    "hld_cols = sorted(hld_cols)\n",
    "final_cols = [\"Conference\", \"Papers\"] + hld_cols + [\"AvgDom\", \"P50\", \"P90\"]\n",
    "final_cols = [c for c in final_cols if c in out_reset.columns]\n",
    "out1 = out_reset.loc[:, final_cols]\n",
    "\n",
    "\n",
    "# 14) Append only the Total row (no \"Avg per domain\" row)\n",
    "numeric_cols = [c for c in out1.columns if c != \"Conference\" and pd.api.types.is_numeric_dtype(out1[c])]\n",
    "total_vals = out1[numeric_cols].sum(numeric_only=True)\n",
    "total_row = {c: (int(total_vals[c]) if pd.api.types.is_integer_dtype(out1[c]) else float(round(total_vals[c],2))) for c in numeric_cols}\n",
    "total_row[\"Conference\"] = \"Total\"\n",
    "# ensure any missing final columns exist in total_row\n",
    "for col in out1.columns:\n",
    "    if col not in total_row:\n",
    "        total_row[col] = \"--\"\n",
    "\n",
    "append_df = pd.DataFrame([total_row], columns=out1.columns)\n",
    "out_final = pd.concat([out1, append_df], ignore_index=True, sort=False)\n",
    "\n",
    "# 15) save\n",
    "out_final.to_csv(output_file1, index=False, float_format=\"%.2f\")\n",
    "nonsp_df.to_csv(output_file2, index=False)\n",
    "print(f\"Wrote files:\\n - {output_file1}\\n - {output_file2}\")\n",
    "\n",
    "print(\"\\n=== Preview (last 6 rows) ===\")\n",
    "print(out_final.tail(6).to_string(index=False))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2b881f-7c9e-425d-a988-0a1f5208e6aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
